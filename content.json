{"meta":{"title":"Rao Weibo的博客","subtitle":"","description":"记录一些工作、学习相关的笔记","author":"Rao Weibo","url":"https://weiborao.link","root":"/"},"pages":[],"posts":[{"title":"使用ChatGPT生成cloud-init制作AppDynamics在AWS上的安装镜像","slug":"ChatGPT-generate-code-to-build-AMI-for-AppDynamics","date":"2023-03-02T01:00:00.000Z","updated":"2023-03-02T01:55:28.270Z","comments":true,"path":"ChatGPT-generate-code-to-build-AMI-for-AppDynamics.html","link":"","permalink":"https://weiborao.link/ChatGPT-generate-code-to-build-AMI-for-AppDynamics.html","excerpt":"","text":"作者: 饶维波 目录 本文记录通过ChatGPT生成cloud-init制作AppDynamics在AWS上 AMI安装镜像的过程，形成操作文档作为参考指南。 任务简介和总体思路任务简介AppDynamics是针对混合和现代云原生应用重新设计的可观测性平台，可提供Cisco全栈可观测性所需的业务洞察。AppDynamics 解决方案集中了技术堆栈的实时可见性，并将分布式环境的复杂性转化为相关和 AI 辅助的见解。AppDynamics 解决方案将技术运行状况、应用程序性能和安全性与用户体验和业务影响联系起来，为您的团队提供快速解决问题和优化数字服务所需的数据，以与最重要的内容保持一致。 为了方便用户在AWS云端安装部署AppDynamics，我们需要制作一个AWS的安装镜像AMI。用户使用安装镜像来启动EC2实例并自动完成Enterprise Console的部署，再通过Enterprise Console的Web界面来安装部署AppDynamics的Controller 和 Event Server，简化安装部署。 本次任务的目标是需要完成AppDynamics AMI的制作，且为了便于后续维护，维护的工作主要包括操作系统层面的安全漏洞修复、AppDynamics软件版本的升级，尽量使用自动化，节省人的时间精力的同时，避免人为错误。 任务关键点是需要编写一个自动化脚本，考虑为Shell脚本或者cloud-init脚本。 总体思路首先需要考虑安装镜像的目标状态是什么？我们设想用户通过镜像创建EC2虚拟机实例，可以直接进入Enterprise Console的界面，并已预设了用户名密码。 那我们可否启动一个EC2实例，并且在上面完成Enterprise Console的安装，然后生成快照做成AMI镜像？咨询了一下安装过AppDynamics的同事，他表示他宁愿选择重新安装，因为每个用户的需求是不同的。因此，我们只需要把需要安装的软件包下载到EC2实例，并设置好安装参数，等用户对AMI镜像进行实例化（基于AMI创建虚拟机）时，再进行Enterprise Console的安装。 AWS对于AMI镜像有要求，即不允许使用静态设定的密码，建议使用EC2的instance-id作为密码。 那么，我们的任务就基本清晰了，以下是任务清单 启动EC2实例，安装所需要的依赖包，以确保AppDynamics软件包能正常安装 设置用户权限 将AppDynamics的软件包拷贝到EC2实例 生成默认的安装参数，并设置系统启动脚本，该脚本只执行一次，在AMI实例化时执行安装，并启动Enterprise Console 按照AWS的安全合规要求进行处理，包括设置SSH参数，清除SSH Key，不留后门等要求。 关机后，基于EC2实例的快照制作镜像 最后是对镜像进行验证 上面最关键的点是自动化脚本的编写，在这里，笔者使用ChatGPT来辅助生成代码。 通过与ChatGPT对话，让其生成代码与ChatGPT的对话经多次对话后，整理内容如下： 请你帮助生成一个Cloud-init代码，自动执行以下内容。 在AWS Console启动AWS Linux 2 AMI，并在启动时执行以下动作：1、安装libaio, numactl, tzdata, ncurses-libs-5.x2、在/etc/security/limits.conf 中添加以下配置 12345root hard nofile 65535root soft nofile 65535root hard nproc 8192root soft nproc 8192 3、你已经会安装Cisco AppDynamics Enterprise Console 21.6.1版本，现在我将安装文件拷贝到了s3://ciscoappd/platform-setup-x64-linux-21.6.1.26487.sh。请创建/opt/appdynamics目录，将Cisco AppDynamics Enterprise Console的安装文件s3://ciscoappd/platform-setup-x64-linux-21.6.1.26487.sh拷贝至该目录，并将其设置为可执行；4、在/opt/appdynamics目录，生成初始文件response.varfile.bak，内容如下： 123456789101112serverHostName=HOST_NAMEsys.languageId=endisableEULA=trueplatformAdmin.port=9191platformAdmin.databasePort=3377platformAdmin.dataDir=/opt/appdynamics/platform/mysql/dataplatformAdmin.databasePassword=ENTER_PASSWORDplatformAdmin.databaseRootPassword=ENTER_PASSWORDplatformAdmin.adminPassword=ENTER_PASSWORDplatformAdmin.useHttps$Boolean=falsesys.installationDir=/opt/appdynamics/platform 上面的工作完成之后，再执行以下工作。 5、请生成一个开机启动脚本，要求如下：5.1 该脚本不在本EC2实例中执行，将其设置为开机后延迟30秒之后再执行，脚本仅需执行一次；5.2 将/opt/appdynamics/response.varfile.bak 复制为 opt/appdynamics/response.varfile5.2 将/opt/appdynamics/response.varfile中的ENTER_PASSWORD替换为EC2 meta-data中的instance-id5.3 将/opt/appdynamics/response.varfile中的HOST_NAME设置为EC2 meta-data中的hostname5.4 静默安装Enterprise Console /opt/appdynamics/platform-setup-x64-linux-21.6.1.26487.sh -q -varfile /opt/appdynamics/response.varfile 一些对话经验： 在经过多次与ChatGPT对话后，它告诉我他能安装的AppDynamics最新版本是21.6.1，如果我请他直接安装23.1.1.18，它在理解上会出错误，于是我请它按照21.6.1版本来安装。 如果一次提问的内容过于复杂，ChatGPT在生成代码时，会因为时间过久而中断，因此，要注意控制一次对话的长度。 如果它理解不对，可以直接指正它，并缩小目标，使得需求更具体，比如请使用‘write_files’和‘runcmd’生成代码。如果不加限制，它可能会给出整段代码全部都用 echo 语句来实现，比结构化的代码可读性要差一些。 生成的cloud-init代码通过继续追问，让其帮忙生成Systemd的服务配置文件，要求只运行一次，再经过多次调试和修改，最终形成以下cloud-init代码。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071#cloud-configpackages: - libaio - numactl - tzdata - ncurses-libs-5.xwrite_files: - path: /etc/security/limits.conf content: | root hard nofile 65535 root soft nofile 65535 root hard nproc 8192 root soft nproc 8192 - path: /opt/appdynamics/response.varfile.bak content: | serverHostName=HOST_NAME sys.languageId=en disableEULA=true platformAdmin.port=9191 platformAdmin.databasePort=3377 platformAdmin.dataDir=/opt/appdynamics/platform/mysql/data platformAdmin.databasePassword=ENTER_PASSWORD platformAdmin.databaseRootPassword=ENTER_PASSWORD platformAdmin.adminPassword=ENTER_PASSWORD platformAdmin.useHttps$Boolean=false sys.installationDir=/opt/appdynamics/platform - path: /etc/systemd/system/appd.console.service permissions: &#x27;0644&#x27; content: | [Unit] Description=AppDynamics Enterprise Console After=network.target [Service] Type=forking ExecStart=/opt/appdynamics/platform/platform-admin/bin/platform-admin.sh start-platform-admin ExecStop=/opt/appdynamics/platform/platform-admin/bin/platform-admin.sh stop-platform-admin User=root Restart=always [Install] WantedBy=multi-user.target - path: /etc/systemd/system/appd.console.install.service permissions: &#x27;0644&#x27; content: | [Unit] Description=AppDynamics Enterprise Console Installation After=network.target [Service] Type=oneshot RemainAfterExit=no ExecStart=/bin/sh -c &#x27;sleep 5 &amp;&amp; cp /opt/appdynamics/response.varfile.bak /opt/appdynamics/response.varfile &amp;&amp; sed -i \\&quot;s/ENTER_PASSWORD/`curl http://169.254.169.254/latest/meta-data/instance-id`/g\\&quot; /opt/appdynamics/response.varfile &amp;&amp; sed -i \\&quot;s/HOST_NAME/`curl http://169.254.169.254/latest/meta-data/hostname`/g\\&quot; /opt/appdynamics/response.varfile &amp;&amp; /opt/appdynamics/platform-setup-x64-linux-23.1.1.18.sh -q -varfile /opt/appdynamics/response.varfile &amp;&amp; systemctl daemon-reload &amp;&amp; systemctl enable appd.console.service &amp;&amp; systemctl start appd.console.service&#x27; [Install] WantedBy=multi-user.targetruncmd: # Create directory and copy Cisco AppDynamics Enterprise Console setup file - aws s3 cp s3://ciscoappdnx/platform-setup-x64-linux-23.1.1.18.sh /opt/appdynamics/ --region cn-northwest-1 - chmod +x /opt/appdynamics/platform-setup-x64-linux-23.1.1.18.sh - systemctl daemon-reload - systemctl enable appd.console.install.service - sed -i &#x27;s/#PermitRootLogin yes/PermitRootLogin no/g&#x27; /etc/ssh/sshd_config - rm -rf /root/.ssh/authorized_keys - rm -rf /home/ec2-user/.ssh/authorized_keys - shred -u /etc/ssh/*_key /etc/ssh/*_key.pub 将上述代码复制粘贴到创建EC2实例的 user-data 这一栏中，即可完成软件的拷贝和开机脚本的生成。 上述的代码是经过多次调测才形成的，其中有两处要注意： aws s3 cp 命令，在最后要加上 ‘region’ 参数，否则会报fatal error: An error occurred (400) when calling the HeadObject operation: Bad Request。这一点ChatGPT没有注意到，需要提醒它，当然它学的很快。 ExecStart=/bin/sh -c ‘sleep 5 &amp;&amp; cp /opt/appdynamics/response.varfile.bak /opt/appdynamics/response.varfile &amp;&amp; sed -i &quot;s/ENTER_PASSWORD/curl http://169.254.169.254/latest/meta-data/instance-id/g&quot; /opt/appdynamics/response.varfile &amp;&amp; sed -i &quot;s/HOST_NAME/curl http://169.254.169.254/latest/meta-data/hostname/g&quot; /opt/appdynamics/response.varfile &amp;&amp; /opt/appdynamics/platform-setup-x64-linux-23.1.1.18.sh -q -varfile /opt/appdynamics/response.varfile &amp;&amp; systemctl daemon-reload &amp;&amp; systemctl enable appd.console.service &amp;&amp; systemctl start appd.console.service’这条命令是很复杂的，一个命令一个命令串行执行，其中还包括了转义字符，如果不是ChatGPT生成，我个人很难完成这个命令。该命令顺序执行以下内容，只有前面的命令执行成功，才会继续执行下一条命令： 等待5秒钟再启动，主要是让EC2的网络和其他关联服务充分就绪； 将response.varfile.bak拷贝至response.varfile 使用sed命令将ENTER_PASSWORD替换为instance-id，instance-id由curl命令获取，更新response.varfile 使用sed命令将HOST_NAME替换为hostname，hostname由curl命令获取，更新response.varfile 执行静默安装脚本，经测算安装需要大约10分钟 重新加载systemd，并创建appd.console.service 最后是执行appd.console.service启动Console Service 通过SSM连接到EC2实例（注意设置EC2的IAM Role，具备SSM和S3的访问权限），通过以下命令进行调测。 12345678910111213141516sudo sucd /opt/appdynamicsls -al# 查看platform-setup-x64-linux-23.1.1.18.sh 和 response.varfile.bak是否存在，# 且安装包是否被设置为可执行cat /etc/security/limits.confcat /etc/systemd/system/appd.console.servicecat /etc/systemd/system/appd.console.install.servicecat /etc/ssh/sshd_config | grep PermitRootLoginls -al /etc/sshls -al /root/.sshls -al /home/ec2-user/.sshcat /var/log/cloud-init-output.log# 检查是否有错误发生 在AWS上制作安装镜像AMI文件创建一个EC2实例前提条件：VPC、公有子网均已设置好。 以下是操作步骤截图及相关说明 1、在EC2服务界面中点击Launch instance 2、填写EC2实例的Name，选择Amazon Linux 3、选择实例类型c5.xlarge，不需要Key pair，选择公有子网，并自动分配公网IP地址。 4、设置Security Group ，设置磁盘大小500G Security Group的设置如下 5、分配IAM 角色 IAM角色具体设置如下，设置SSM权限和S3的访问权限 6、将上述cloud-init 代码输入user-data中，并点击 Launch instance 检查EC2实例的设置1、通过SSM连接EC2实例，如果未设置SSM的权限，则不能使用该方式管理EC2 2、执行命令检查EC2的设置是否符合预期 以下是文本格式的输出： 123456789101112131415161718192021222324252627282930313233[root@ip-172-31-13-10 appdynamics]# cat /etc/security/limits.confroot hard nofile 65535root soft nofile 65535root hard nproc 8192root soft nproc 8192[root@ip-172-31-13-10 appdynamics]# cat /etc/systemd/system/appd.console.service[Unit]Description=AppDynamics Enterprise ConsoleAfter=network.target[Service]Type=forkingExecStart=/opt/appdynamics/platform/platform-admin/bin/platform-admin.sh start-platform-adminExecStop=/opt/appdynamics/platform/platform-admin/bin/platform-admin.sh stop-platform-adminUser=rootRestart=always[Install]WantedBy=multi-user.target[root@ip-172-31-13-10 appdynamics]# cat /etc/systemd/system/appd.console.install.service[Unit]Description=AppDynamics Enterprise Console InstallationAfter=network.target[Service]Type=oneshotRemainAfterExit=noExecStart=/bin/sh -c &#x27;sleep 5 &amp;&amp; cp /opt/appdynamics/response.varfile.bak /opt/appdynamics/response.varfile &amp;&amp; sed -i \\&quot;s/ENTER_PASSWORD/`curl http://169.254.169.254/latest/meta-data/instance-id`/g\\&quot; /opt/appdynamics/response.varfile &amp;&amp; sed -i \\&quot;s/HOST_NAME/`curl http://169.254.169.254/latest/meta-data/hostname`/g\\&quot; /opt/appdynamics/response.varfile &amp;&amp; /opt/appdynamics/platform-setup-x64-linux-23.1.1.18.sh -q -varfile/opt/appdynamics/response.varfile &amp;&amp; systemctl daemon-reload &amp;&amp; systemctl enable appd.console.service &amp;&amp; systemctl start appd.console.service&#x27;[Install]WantedBy=multi-user.target 可以看出是符合预期的。 执行 history -c 清除历史记录。 生成AMI镜像1、将该EC2关机 2、确认关机后，点击制作Image 3、填入Image的参数，点击Create Image 验证AMI使用AMI启动EC2实例1、等待AMI状态变Available后，点击Launch instance from AMI 2、进入EC2启动的设置页面，进行必要的设置 除了user-data维持空白，其他设置可于创建AMI的EC2一致。 3、EC2创建后，转到EC2的信息页面 4、测试Enterprise Console的Web界面 大概10分钟后，可以通过以下地址访问 1234http://[ec2-69-230-211-253.cn-northwest-1.compute.amazonaws.com.cn](http://ec2-69-230-211-253.cn-northwest-1.compute.amazonaws.com.cn/):9191username: adminpassword: 从信息页面中拷贝instance-id，如上图为i-06b75d367808d02af 5、使用SSM连接EC2，检查相关服务是否正常 总结本文整理了在AWS上制作AppDynamics的AMI镜像的过程，笔者使用了ChatGPT帮助生成自动化脚本，极大的提高了效率。本文中的cloud-init代码已发布到Github，如下： appd-console-install-cloud-init/appd-install-cloud-init-CN.yaml at main · weiborao/appd-console-install-cloud-init 非常感谢各位耐心的阅读，谢谢。","categories":[],"tags":[{"name":"chatGPT","slug":"chatGPT","permalink":"https://weiborao.link/tags/chatGPT/"},{"name":"AMI","slug":"AMI","permalink":"https://weiborao.link/tags/AMI/"},{"name":"AppDynamics","slug":"AppDynamics","permalink":"https://weiborao.link/tags/AppDynamics/"},{"name":"cloud-init","slug":"cloud-init","permalink":"https://weiborao.link/tags/cloud-init/"}]},{"title":"Traceroute 漏洞？","slug":"traceroute-vulnerability","date":"2021-12-16T13:25:41.000Z","updated":"2021-12-17T00:26:29.206Z","comments":true,"path":"traceroute-vulnerability.html","link":"","permalink":"https://weiborao.link/traceroute-vulnerability.html","excerpt":"","text":"本文将分析一种名为允许Traceroute探测的漏洞，对其判定方法进行追查，并提出建议。 背景最近，笔者在做一个网络设备方面的安全合规性的测试，其中有一项是需要使用漏洞扫描设备对被测试的设备进行全面的安全漏洞扫描。在笔者对被测设备进行了针对性的安全加固后，漏洞扫描报告仍然有一个低危漏洞：允许Traceroute探测。该低危漏洞的详细信息如下： 需要说明的一点是，扫描器如需成功扫描到该设备，这个设备的IP地址要求能被扫描器探测到，否则扫描器会认为设备不在线。通常的做法就是允许Ping，具体而言，被测设备至少要允许ICMP Echo Request请求，并应答ICMP Echo Reply。 笔者在被测试设备的接口上配置了ACL，入向仅允许ICMP Echo Request报文，出向仅允许ICMP Echo Reply，但是扫描结果依然判断被测设备允许Traceroute探测。 对此，笔者疑惑不已，下文将展开思考和分析。 对漏洞的解读 详细描述：使用Traceroute探测来获取扫描器与远程主机之间的路由信息。攻击者也可以利用这些信息来了解目标网络的网络拓扑。 解读：如果扫描器与被测试设备处于同一网段，那么扫描器（充当攻击者角色）与被测设备之间的网络拓扑实际是已知的，即通过二层交换网络互联；根本不需要使用Traceroute来获取路由信息、网络拓扑。按照笔者之前的文章《通过Wireshark重新认识Traceroute》的分析，如果针对同一个网段的主机进行Traceroute，那么第一个TTL=1的UDP报文发送到该地址后，立刻会收到Port-Unreachable的ICMP报文。如果是Windows系统，tracert会发送TTL=1 的ICMP Echo Request，但是同一网段的地址只是会简单的回复一个ICMP Echo Reply的报文，就宣告结束。不管发送UDP报文还是ICMP报文，整个过程中，不会出现TTL超时的情况，也就不会收到TTL Exceeded的ICMP报文。 解决办法：在防火墙中禁用Time Exceeded类型的ICMP包。 推理：根据解决办法的定义推理：如果防火墙禁用了Time Exceeded类型的ICMP报文，该漏洞就得到解决，那么扫描结果中就不会出现允许Traceroute探测的漏洞。也可以说，如果扫描器没有接收到Time Exceeded类型的ICMP包，也就没有发现Traceroute的证据，那么，扫描结果中不能出现允许Traceroute探测的漏洞。 综合上述的分析，笔者认为，依据漏洞的解决办法进行推理，如果扫描器与被测设备在同一网段，扫描过程中不会收到TTL Exceeded的ICMP包，不能得出被测设备允许Traceroute探测这个结论的。 简言之，扫描器对于该漏洞的判定原则与其提供的解决办法并不匹配。 漏洞判定方法的追查为了追查扫描器到底如何判定该漏洞，笔者请负责漏洞扫描的工程师设法进行抓包，将扫描过程中扫描器发送/接收的报文进行全量抓取。将抓取到的报文进行分析，有如下发现： 扫描器对被测设备发起随机的TCP端口扫描，因笔者进行了安全加固，被测设备没有回复任何一个TCP报文； 扫描器对被测设备发起其他常见协议的扫描，例如SNMP、SIP、NTP、TFTP等，均未得到响应； 扫描器对被测设备发起基于UDP报文的Traceroute探测，逐一发送TTL=1、2、3的UDP报文，UDP端口号为32768，未得到响应； 扫描器对被测设备发起基于ICMP的Traceroute探测，得到ICMP Echo Reply的回包，如下图： 注：扫描器地址为192.168.100.11，被测设备地址为192.168.99.101，其中隔着一个设备192.168.100.101，均为同一系列的两个不同型号的产品，两个设备的软件版本相同。 笔者设法将TTL=1和TTL=2的报文进行了过滤，但是还有TTL=3的报文继续发送过来。 按照上述的抓包分析来分析，判定原则似乎是逐一发送TTL=1、2、……N，（N&gt;=3）的ICMP Echo Request报文，期望回复TTL Exceeded的报文，实际回复ICMP Echo Reply报文，竟然也判定为允许Traceroute探测。 笔者去掉安全加固的策略，重新进行扫描，抓取扫描器的收发报文，如下： 在该图中，我们可以清晰的看到中间的设备192.168.100.101发送的TTL Exceeded报文，从而让扫描器探测到其与远程主机之间的路由信息，网络拓扑信息（发现了中间设备）。 建议通过上述的分析，我们得出结论： 该漏洞扫描程序的判定原则与其提供的解决办法不匹配，即按照解决办法整改后，依然会扫描出允许Traceroute探测的漏洞。 通过抓包分析，我们发现其判定原则存在不合理之处，即扫描过程中通过发送TTL=1、2、3……的报文，期望回复ICMP TTL Exceeded，然而在回复了ICMP Echo Reply，仍然会判定为有该漏洞。 执行该漏洞扫描时，不能将扫描器与被测设备进行同网段直连，这样被测设备不会返回ICMP TTL Exceeded报文。 建议： 修改判定原则，使其与解决办法相匹配； 执行该漏洞扫描时，需要在扫描器和被测设备之间至少增加一个路由设备（可为同款测试设备），以便真实的反映出Traceroute的功能。","categories":[],"tags":[{"name":"traceroute","slug":"traceroute","permalink":"https://weiborao.link/tags/traceroute/"},{"name":"icmp","slug":"icmp","permalink":"https://weiborao.link/tags/icmp/"}]},{"title":"有关AnyCast","slug":"anycast-brief","date":"2021-07-06T06:00:00.000Z","updated":"2021-07-06T06:10:58.348Z","comments":true,"path":"anycast-brief.html","link":"","permalink":"https://weiborao.link/anycast-brief.html","excerpt":"","text":"AnyCast，一组服务器拥有相同的IP地址，当客户端访问该组服务器时，网络会将请求发送至最近的服务器进行处理，从而极大的缩短途径的公网路径，减少延时、抖动、丢包的情况。 AnyCast通常和BGP路由协议关联在一起，其实现的主要原理是：位于不同地理位置的路由器向外发布同一段IP网段的BGP路由，路由在Internet中传播后，访问发起端所处网络的路由器会选择最短的BGP AS Path路由，从而实现最短路径的访问。如果BGP选路区分不出最近的路径，那就由IGP最短路径进行转发。 AnyCast的好处： 就近访问，减少时延、提升性能 获得高冗余性和可用性，即当任意目的节点异常时，可自动路由到就近目的节点 实现负载均衡，且对客户端是透明的（由网络路由实现） 缓解DDOS攻击，AnyCast将DDOS攻击流量引导至本地服务器，极大的减少了DDOS流量的范围以及规模 Office 365 的SharePoint使用了AnyCast，例如nslookup 解析cisco.sharepoint.com、citrix.sharepoint.com、intel.sharepoint.com 得到相同的地址 12345678910➜ ~ nslookup cisco.sharepoint.com | tail -n 2 | grep AddressAddress: 13.107.136.9➜ ~ nslookup citrix.sharepoint.com | tail -n 2 | grep AddressAddress: 13.107.136.9➜ ~ nslookup intel.sharepoint.com | tail -n 2 | grep AddressAddress: 13.107.136.9➜ ~ nslookup nike.sharepoint.com | tail -n 2 | grep AddressAddress: 13.107.136.9➜ ~ nslookup bmw.sharepoint.com | tail -n 2 | grep AddressAddress: 13.107.136.9 Telnet 至 route-views3.routeviews.org 查询13.107.136.9的路由，发现该地址命中的路由是13.107.136.0/24，从AS 8068 – AS 8075发布出来，这都是Microsoft的AS号。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194route-views3.routeviews.org&gt; show bgp ipv4 13.107.136.9BGP routing table entry for 13.107.136.0&#x2F;24Paths: (40 available, best #33, table default) Not advertised to any peer 61568 8075 8068 190.15.124.18 from 190.15.124.18 (190.15.124.18) Origin IGP, valid, external Last update: Mon Jul 5 21:13:03 2021 202365 49697 8075 8068 194.50.19.4 from 194.50.19.4 (185.1.166.50) Origin IGP, valid, external Community: 49697:3000 65101:1002 65102:1000 65103:276 65104:150 Last update: Sun Jul 4 17:31:02 2021 39120 8075 8068 89.21.210.85 from 89.21.210.85 (195.60.191.13) Origin IGP, valid, external Last update: Sat Jul 3 20:46:17 2021 40630 6939 8075 8068 208.94.118.10 from 208.94.118.10 (208.94.118.10) Origin IGP, valid, external Community: 40630:100 40630:11701 Last update: Mon Jul 5 19:14:13 2021 54574 8075 8068 154.18.7.114 from 154.18.7.114 (193.41.248.191) Origin IGP, valid, external Community: 54574:1000 Last update: Wed Jun 30 02:40:58 2021 39120 8075 8068 89.21.210.86 from 89.21.210.86 (195.60.190.28) Origin IGP, valid, external Last update: Mon Jun 28 17:54:55 2021 64116 7195 8075 8068 45.183.45.1 from 45.183.45.1 (10.7.1.1) Origin IGP, valid, external Community: 7195:1000 7195:1001 7195:1300 7195:1302 Last update: Fri Jul 2 00:44:52 2021 54574 6461 8075 8068 38.19.140.162 from 38.19.140.162 (193.41.248.193) Origin IGP, valid, external Community: 6461:5997 54574:2000 54574:6461 Last update: Mon Jun 28 07:19:51 2021 39351 8075 8068 193.138.216.164 from 193.138.216.164 (193.138.216.164) Origin IGP, valid, external Last update: Fri Jun 25 20:17:33 2021 202365 57866 8075 8068 5.255.90.109 from 5.255.90.109 (185.255.155.66) Origin IGP, metric 0, valid, external Community: 57866:12 57866:304 57866:501 Large Community: 57866:41441:41441 Last update: Sun Jun 27 19:25:03 2021 3216 12389 8075 8068 195.239.252.124 from 195.239.252.124 (195.239.252.124) Origin IGP, valid, external Community: 3216:1000 3216:1077 3216:1101 Last update: Thu Jun 24 09:16:54 2021 3216 12389 8075 8068 195.239.77.236 from 195.239.77.236 (195.239.77.236) Origin IGP, valid, external Community: 3216:1000 3216:1077 3216:1101 Last update: Thu Jun 24 09:14:17 2021 11537 8075 8068 64.57.28.241 from 64.57.28.241 (64.57.28.241) Origin IGP, metric 17, valid, external Community: 11537:254 11537:3500 11537:5000 11537:5002 11537:5014 Last update: Thu Jun 24 08:50:42 2021 19653 8075 8068 67.219.192.5 from 67.219.192.5 (67.219.192.5) Origin IGP, valid, external Last update: Wed Jun 30 11:14:51 2021 14315 6453 6453 8075 8068 104.251.122.1 from 104.251.122.1 (104.251.122.1) Origin IGP, valid, external Community: 14315:5000 Last update: Tue Jun 22 10:36:25 2021 38136 38008 8075 8068 103.152.35.22 from 103.152.35.22 (103.152.35.22) Origin IGP, valid, external Community: 38008:103 65521:10 Large Community: 38136:1000:11 Last update: Tue Jun 22 07:22:58 2021 17920 6939 8075 8068 103.149.144.251 from 103.149.144.251 (103.149.144.251) Origin IGP, valid, external Community: 17920:1000 17920:1008 Last update: Sat Jun 19 01:43:19 2021 50236 34549 8075 8068 2.56.9.2 from 2.56.9.2 (2.56.9.2) Origin IGP, valid, external Community: 34549:200 34549:10000 50236:10010 Last update: Thu Jun 17 16:09:18 2021 3280 39737 8075 8068 77.83.243.7 from 77.83.243.7 (77.83.243.7) Origin IGP, valid, external Community: 39737:80 39737:2040 Last update: Sat Jul 3 23:45:54 2021 39120 8075 8068 94.101.60.146 from 94.101.60.146 (94.161.60.146) Origin IGP, valid, external Last update: Fri Jun 18 15:06:50 2021 207740 58057 8075 8068 136.243.0.23 from 136.243.0.23 (136.243.0.23) Origin IGP, valid, external Last update: Tue Jun 22 18:11:37 2021 38136 50131 53340 174 8075 8068 172.83.155.50 from 172.83.155.50 (172.83.155.50) Origin IGP, valid, external Large Community: 38136:1000:17 Last update: Tue Jun 8 08:24:12 2021 40387 11537 8075 8068 72.36.126.8 from 72.36.126.8 (72.36.126.8) Origin IGP, valid, external Community: 40387:1400 Last update: Tue Jun 8 07:02:44 2021 38136 57695 8075 8068 170.39.224.212 from 170.39.224.212 (170.39.224.212) Origin IGP, valid, external Community: 57695:12000 57695:12002 Large Community: 38136:1000:1 Last update: Thu May 27 15:18:00 2021 3561 209 3356 8075 8068 206.24.210.80 from 206.24.210.80 (206.24.210.80) Origin IGP, valid, external Last update: Thu May 27 09:50:45 2021 29479 50304 8075 8068 109.233.62.1 from 109.233.62.1 (109.233.62.2) Origin IGP, valid, external Last update: Thu May 27 09:50:11 2021 209 3356 8075 8068 205.171.8.123 from 205.171.8.123 (205.171.8.123) Origin IGP, metric 8000051, valid, external Community: 209:88 209:888 3356:3 3356:22 3356:86 3356:575 3356:666 3356:901 3356:2057 3356:12341 Last update: Thu May 27 09:48:26 2021 14840 8075 8068 186.211.128.32 from 186.211.128.32 (201.16.22.1) Origin IGP, valid, external Community: 14840:10 14840:40 14840:6004 14840:7110 Last update: Wed Jun 23 00:56:13 2021 209 3356 8075 8068 205.171.200.245 from 205.171.200.245 (205.171.200.245) Origin IGP, metric 0, valid, external Community: 209:88 209:888 3356:3 3356:22 3356:86 3356:575 3356:666 3356:901 3356:2059 3356:10327 Last update: Thu May 27 09:48:23 2021 209 3356 8075 8068 205.171.3.54 from 205.171.3.54 (205.171.3.54) Origin IGP, metric 8000036, valid, external Community: 209:88 209:888 3356:3 3356:22 3356:86 3356:575 3356:666 3356:901 3356:2011 3356:11918 Last update: Thu May 27 09:48:17 2021 23367 8075 8068 64.250.124.251 from 64.250.124.251 (64.250.124.251) Origin IGP, valid, external Last update: Thu May 27 09:48:06 2021 5650 8075 8068 74.40.7.35 from 74.40.7.35 (74.40.0.100) Origin IGP, metric 0, valid, external Last update: Tue May 25 23:52:37 2021 38001 8075 8068 202.150.221.33 from 202.150.221.33 (10.11.33.29) Origin IGP, valid, external, best (Older Path) Community: 38001:420 38001:2406 38001:3001 38001:8009 Last update: Fri May 21 11:01:01 2021 6939 8075 8068 64.71.137.241 from 64.71.137.241 (216.218.252.164) Origin IGP, valid, external Last update: Fri May 21 10:58:51 2021 45352 8075 8068 210.5.41.225 from 210.5.41.225 (210.5.40.186) Origin IGP, valid, external Last update: Fri May 21 10:57:46 2021 3292 8075 8068 195.215.109.247 from 195.215.109.247 (83.88.49.1) Origin IGP, valid, external Community: 3292:1100 3292:24500 3292:24580 Last update: Wed Jun 9 23:59:27 2021 3257 8075 8068 89.149.178.10 from 89.149.178.10 (213.200.83.26) Origin IGP, metric 10, valid, external Community: 3257:4000 3257:8052 3257:50001 3257:50110 3257:54900 3257:54901 Last update: Fri Jul 2 03:36:10 2021 46450 8075 8068 158.106.197.135 from 158.106.197.135 (158.106.197.135) Origin IGP, valid, external Community: 46450:31111 Last update: Fri May 21 10:53:55 2021 5645 8075 8068 206.248.155.130 from 206.248.155.130 (206.248.155.130) Origin IGP, valid, external Community: no-advertise Last update: Fri May 21 10:53:54 2021 55222 8075 8068 162.211.99.255 from 162.211.99.255 (162.211.99.255) Origin IGP, valid, external Community: 55222:101 55222:200 55222:6001 55222:20020 Last update: Fri May 21 10:53:45 2021 AnyCast 典型应用场景： DNS： CDN 负载均衡 分散DDOS攻击 Reference： https://www.cnblogs.com/itzgr/p/10192799.html https://www.imperva.com/blog/how-anycast-works/","categories":[],"tags":[{"name":"anycast","slug":"anycast","permalink":"https://weiborao.link/tags/anycast/"}]},{"title":"在Docker容器中tcpdump抓包探秘Traceroute","slug":"docker-traceroute-tcpdump","date":"2021-06-29T07:20:00.000Z","updated":"2021-06-29T14:19:26.631Z","comments":true,"path":"docker-traceroute-tcpdump.html","link":"","permalink":"https://weiborao.link/docker-traceroute-tcpdump.html","excerpt":"","text":"在上一篇文章《通过Wireshark重新认识Traceroute》中，我在MAC电脑上进行抓包来对MAC上的Traceroute过程进行分析。但是MAC电脑上运行了多个应用，我在抓完报文后，使用过滤器将其他报文过滤掉了，但是也很可能把Traceroute产生的报文给删掉了。 为了打造一个纯净的Traceroute环境，我又使用容器来做一次测试。 本文记录以下内容： 创建一个包含traceroute、ping、whois等工具的容器，取名traceroute 创建一个只包含tcpdump的容器，对traceroute容器抓包 简略的分析抓包文件—-不是本文重点 创建Traceroute容器镜像Dockerfile准备如果我们执行docker run -it ubuntu，想在该容器中执行ping、traceroute命令，抱歉，找不到该命令。 1234567➜ docker pull ubuntu➜ docker run -it --rm ubunturoot@631c227046f7:/# ping 8.8.8.8bash: ping: command not foundroot@631c227046f7:/# traceroute 8.8.8.8bash: traceroute: command not foundroot@631c227046f7:/# 这时候，您可以直接在容器里执行： 1root@631c227046f7:/# apt-get update &amp;&amp; apt-get install traceroute iputils-ping --no-install-recommends 当然，您也可以创建一个容器镜像，如下： 123mkdir traceroutecd traceroutetouch Dockerfile Dockerfile 内容如下： 12345678910FROM ubuntu:latestRUN apt-get update &amp;&amp; apt-get install -y \\ traceroute \\ iputils-ping \\ whois \\ netbase \\ iproute2 \\ --no-install-recommends \\ &amp;&amp; rm -rf &#x2F;var&#x2F;lib&#x2F;apt&#x2F;lists&#x2F;* 注：上述要安装的软件包，是我经过尝试后，一个个增加的。 最开始我只安装了traceroute，当我执行traceroute -A 8.8.8.8时，提示如下错误信息： 1whois.radb.net/nicname: Servname not supported for ai_socktype 我想起traceroute可能需要调用whois来查询BGP AS号，于是我继续安装whois，仍然报错。—-经过验证，如果不安装whois软件包，traceroute -A 8.8.8.8 可正常工作。 只能在网上搜索答案了，果然有人遇到过类似问题： 123https://stackoverflow.com/questions/56430294/bash-script-that-uses-whois-command-gets-servname-not-supported-error-on-do解决办法：apt-get update &amp;&amp; apt-get install -y --no-install-recommends netbase 安装iproute2的目的是为了执行ip a, ip link 这些命令。 根据Dockerfile创建容器镜像根据上述的Dockerfile创建容器镜像，执行命令：docker image build -t traceroute:latest . 注意末尾的小点，表示当前目录。 1234567891011121314151617➜ traceroute docker image build -t traceroute:latest .[+] Building 48.7s (6/6) FINISHED =&gt; [internal] load build definition from Dockerfile 0.0s =&gt; =&gt; transferring dockerfile: 274B 0.0s =&gt; [internal] load .dockerignore 0.0s =&gt; =&gt; transferring context: 2B 0.0s =&gt; [internal] load metadata for docker.io/library/ubuntu:latest 0.0s =&gt; CACHED [1/2] FROM docker.io/library/ubuntu:latest 0.0s =&gt; [2/2] RUN apt-get update &amp;&amp; apt-get install -y traceroute iputils-ping whois netbase iproute2 --no-ins 48.5s =&gt; exporting to image 0.1s =&gt; =&gt; exporting layers 0.1s =&gt; =&gt; writing image sha256:bbf499e59136872e8d171fddcd1548497a9e76b96d154dcb340b8326252d97d5 0.0s =&gt; =&gt; naming to docker.io/library/traceroute:latest 0.0s ➜ traceroute docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEtraceroute latest bbf499e59136 5 seconds ago 77.4MB 如何抓取容器的网络报文接下来，需要考虑的问题是在MAC电脑上如何抓取容器的网络报文呢？ Wireshark on macOS如何抓包？我执行了docker run -it –name traceroute traceroute 启动了一个名为tracertoute的容器，打开Wireshark，哪一个接口对应的是这个容器的接口呢？ 上面的接口眼花缭乱的，找不出是哪一个接口，en0和lo0上有流量，但是并不是容器连接的端口，只得寻求其他方法。 基于 Container 网络共享机制来抓包通过搜索”how to tcpdump in docker”，找到一篇文章How to TCPdump effectively in Docker，获益匪浅。 该文章介绍了一种Docker的网络模式，container模式。 In the --net=container:id all traffic in/out a specific container (or group of containers) can be captured. 另一篇文章：https://www.freeaihub.com/article/container-module-in-docker-network.html Docker网络container模式是指，创建新容器的时候，通过--net container参数，指定其和已经存在的某个容器共享一个 Network Namespace。如下图所示，右方黄色新创建的container，其网卡共享左边容器。因此就不会拥有自己独立的 IP，而是共享左边容器的 IP 172.17.0.2,端口范围等网络资源，两个容器的进程通过 lo 网卡设备通信。 但这两个容器在其他的资源上，如文件系统、进程列表等还是隔离的。 专门创建一个安装了tcpdump的容器，对上文创建的traceroute容器进行抓包。 参考How to TCPdump effectively in Docker，在Terminal中执行以下命令，创建一个tcpdump的容器镜像 12345docker build -t tcpdump - &lt;&lt;EOF FROM ubuntu RUN apt-get update &amp;&amp; apt-get install -y tcpdump CMD tcpdump -i eth0 -w /var/traceroute-ubuntu.pcapngEOF 创建过程如下： 123456789101112[+] Building 0.1s (6&#x2F;6) FINISHED &#x3D;&gt; [internal] load build definition from Dockerfile 0.0s &#x3D;&gt; &#x3D;&gt; transferring dockerfile: 159B 0.0s &#x3D;&gt; [internal] load .dockerignore 0.0s &#x3D;&gt; &#x3D;&gt; transferring context: 2B 0.0s &#x3D;&gt; [internal] load metadata for docker.io&#x2F;library&#x2F;ubuntu:latest 0.0s &#x3D;&gt; [1&#x2F;2] FROM docker.io&#x2F;library&#x2F;ubuntu 0.0s &#x3D;&gt; CACHED [2&#x2F;2] RUN apt-get update &amp;&amp; apt-get install -y tcpdump 0.0s &#x3D;&gt; exporting to image 0.0s &#x3D;&gt; &#x3D;&gt; exporting layers 0.0s &#x3D;&gt; &#x3D;&gt; writing image sha256:d5d8942d4836fabbb0343a082d9aa0009c6dd5071c70a54ae57baf6728462562 0.0s &#x3D;&gt; &#x3D;&gt; naming to docker.io&#x2F;library&#x2F;tcpdump 0.0s 镜像列表： 12345➜ traceroute docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEtcpdump latest d5d8942d4836 3 minutes ago 109MBtraceroute latest bbf499e59136 35 minutes ago 77.4MBubuntu latest 9873176a8ff5 11 days ago 72.7MB 启动traceroute容器： 12➜ docker run -it --name traceroute tracerouteroot@fbe3eb98ae63:/# 启动tcpdump容器，进行抓包： 12➜ ~ docker run -it --net=container:traceroute tcpdumptcpdump: listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes 在traceroute容器发起traceroute： 1234567891011121314151617root@fbe3eb98ae63:/# traceroute -A -q 1 -N 1 -z 500 -e 8.8.8.8traceroute to 8.8.8.8 (8.8.8.8), 30 hops max, 60 byte packets 1 localhost (172.17.0.1) [*] 0.129 ms 2 localhost (10.39.101.1) [*] 3.716 ms 3 localhost (192.168.1.1) [*] 3.926 ms 4 222.129.32.1 (222.129.32.1) [AS4808] 6.837 ms 5 61.148.163.181 (61.148.163.181) [AS4808] 9.247 ms 6 219.232.11.65 (219.232.11.65) [AS17431] 7.049 ms 7 61.149.203.205 (61.149.203.205) [AS4808] 7.650 ms 8 219.158.7.22 (219.158.7.22) [AS4837] 40.815 ms 9 219.158.103.218 (219.158.103.218) [AS4837] 64.771 ms10 219.158.103.30 (219.158.103.30) [AS4837] 50.402 ms11 219.158.10.30 (219.158.10.30) [AS4837] 53.261 ms12 219.158.33.174 (219.158.33.174) [AS4837] 55.503 ms13 108.170.241.65 (108.170.241.65) [AS15169] 54.876 ms14 142.251.64.173 (142.251.64.173) [AS15169] 45.966 ms15 dns.google (8.8.8.8) [AS15169] 52.492 ms traceroute -A -q 1 -N 1 -z 500 -e 8.8.8.8 参数解释如下： -A： 向radb.net查找对应节点IP所在的AS Path信息，并将查询信息输出 -q 1： 将缺省发送3个探测包改为1个 -N 1： 将并发16个探测改为一次一个，以便于逐个分析 -z 500： 表示每次等待500毫秒再发出下一个探测 -e： 显示ICMP的扩展消息，如果有的话 按下CTRL+C，停止tcpdump容器的抓包： 12345➜ ~ docker run -it --net=container:traceroute tcpdumptcpdump: listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes^C237 packets captured237 packets received by filter0 packets dropped by kernel 该容器在抓完包后自动停止，并将抓包文件存储在容器内部的/var/traceroute-ubuntu.pcapng，通过docker cp命令将抓包文件拷贝出来。 12345➜ ~ docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESf397f50267ae tcpdump &quot;/bin/sh -c &#x27;tcpdump…&quot; 2 minutes ago Exited (130) 20 seconds ago condescending_corifbe3eb98ae63 traceroute &quot;bash&quot; 48 minutes ago Up 48 minutes traceroute➜ ~ docker cp f397f50267ae:/var/traceroute-ubuntu.pcapng ./ 简略分析抓包文件以下是本次抓取的237个报文的头36个报文，如下： 上图中，Wireshare直接分析出来Whois报文。 过程概述如下： 向8.8.8.8发起UDP报文，目的端口为33434，TTL设置为1 收到路由器的ICMP TTL超时消息，获取路由器对应的IP地址 尝试向DNS服务器查询路由器IP地址的反向域名解析，如果得到域名，就将其显示在Traceroute的输出结果中。 发起DNS查询，查询whois.radb.net域名，得到应答为198.108.0.18 发起TCP连接请求，与198.108.0.18建立TCP连接。 向whois.radb.net发起查询，询问路由器IP地址对应的AS号，并得到回应，如果是私有地址，其AS号为0。 重复上述第1、2、3、6步，每次将TTL加1，直至目标地址接收到探测报文，并返回ICMP Port Unreachable消息。 抓包文件下载 番外篇ThousandEyes的介绍可参考这篇文章《在AWS上部署TE Agent并进行测试》。 ThousandEyes可将探针部署到容器中，而且安装十分简单，从ThousandEyes的Cloud &amp; Enterprise Agents中，选择Agent Settings，再选择Enterprise Agents，再点击Add New Enterprise Agent，切换到Docker这个TAB。 将下面的文字复制粘贴到Terminal终端： 1234567891011121314151617181920docker pull thousandeyes/enterprise-agent &gt; /dev/null 2&gt;&amp;1docker stop &#x27;TEagent&#x27; &gt; /dev/null 2&gt;&amp;1docker rm &#x27;TEagent&#x27; &gt; /dev/null 2&gt;&amp;1docker run \\ --hostname=&#x27;TEagent&#x27; \\ --memory=2g \\ --memory-swap=2g \\ --detach=true \\ --tty=true \\ --shm-size=512M \\ -e TEAGENT_ACCOUNT_TOKEN=lje1co39qi 8b5oviy \\ -e TEAGENT_INET=4 \\ -v &#x27;/Users/werao/thousandeyes/TEagent/te-agent&#x27;:/var/lib/te-agent \\ -v &#x27;/Users/werao/thousandeyes/TEagent/te-browserbot&#x27;:/var/lib/te-browserbot \\ -v &#x27;/Users/werao/thousandeyes/TEagent/log/&#x27;:/var/log/agent \\ --cap-add=NET_ADMIN \\ --cap-add=SYS_ADMIN \\ --name &#x27;TEagent&#x27; \\ --restart=unless-stopped \\ thousandeyes/enterprise-agent /sbin/my_init 安装完后，ThousandEyes即可使用该Agent进行测试了。 创建一个简单的探测如下： 在ThousandEyes的界面，可观测上述的探测结果： 时延比较稳定，维持在50ms左右。 路径可视化分析中，当我们把鼠标落在某个节点上，网页会弹出交互式的信息，包括： 网络前缀信息，如：124.65.192.0/18 运营商AS信息，如：CNCGROUP Beijing Province (AS 4808) 地理位置，如：Beijing, Beijing, China DSCP 平均时延，指从探测的Agent到对应节点的平均时延 ThousandEyes的探针所收集的数据是常规的网络工具都能收集到的，ThousandEyes的平台对收集到的数据进行存储、分析、交叉关联，为用户的数字体验提供全方位的洞见。 本节是突发奇想的内容，故而放到番外篇。 全文完。","categories":[],"tags":[{"name":"traceroute","slug":"traceroute","permalink":"https://weiborao.link/tags/traceroute/"},{"name":"docker","slug":"docker","permalink":"https://weiborao.link/tags/docker/"},{"name":"tcpdump","slug":"tcpdump","permalink":"https://weiborao.link/tags/tcpdump/"}]},{"title":"通过Wireshark重新认识Traceroute","slug":"get-known-traceroute-by-wireshark","date":"2021-06-27T06:50:00.000Z","updated":"2021-06-28T06:15:06.568Z","comments":true,"path":"get-known-traceroute-by-wireshark.html","link":"","permalink":"https://weiborao.link/get-known-traceroute-by-wireshark.html","excerpt":"","text":"初识TracerouteTraceroute是一种常见的网络分析工具，用于探测数据包从源地址到目的地址经过的路由器的IP地址。 以下的示例显示从一个MAC电脑到8.8.8.8的路径探测结果： 123456789101112131415161718192021222324252627282930313233343536373839➜ ~ traceroute 8.8.8.8traceroute to 8.8.8.8 (8.8.8.8), 64 hops max, 52 byte packets 1 localhost (10.39.101.1) 8.662 ms 1.307 ms 1.155 ms 2 localhost (192.168.1.1) 2.287 ms 2.157 ms 1.897 ms 3 222.129.32.1 (222.129.32.1) 5.844 ms 13.092 ms 10.332 ms 4 114.244.95.105 (114.244.95.105) 7.541 ms 61.51.101.101 (61.51.101.101) 7.420 ms 61.148.163.81 (61.148.163.81) 7.075 ms 5 61.148.4.213 (61.148.4.213) 7.759 ms 219.232.11.65 (219.232.11.65) 5.976 ms bt-230-081.bta.net.cn (202.106.230.81) 6.021 ms 6 202.96.12.13 (202.96.12.13) 7.828 ms * * 7 219.158.112.26 (219.158.112.26) 39.486 ms * 219.158.7.22 (219.158.7.22) 45.959 ms 8 219.158.103.218 (219.158.103.218) 48.469 ms 219.158.97.2 (219.158.97.2) 47.146 ms 219.158.103.218 (219.158.103.218) 50.320 ms 9 219.158.103.30 (219.158.103.30) 50.739 ms 50.382 ms 48.348 ms10 219.158.10.30 (219.158.10.30) 49.927 ms 44.264 ms 56.353 ms11 219.158.33.174 (219.158.33.174) 54.123 ms 61.131 ms 47.302 ms12 108.170.241.97 (108.170.241.97) 56.082 ms 108.170.241.33 (108.170.241.33) 52.942 ms 54.393 ms13 142.250.58.189 (142.250.58.189) 48.958 ms 209.85.143.37 (209.85.143.37) 51.217 ms 108.170.226.115 (108.170.226.115) 141.544 ms14 dns.google (8.8.8.8) 48.935 ms 46.364 ms 51.043 ms➜ ~ ping 8.8.8.8PING 8.8.8.8 (8.8.8.8): 56 data bytes64 bytes from 8.8.8.8: icmp_seq=0 ttl=113 time=52.060 ms64 bytes from 8.8.8.8: icmp_seq=1 ttl=113 time=44.845 ms64 bytes from 8.8.8.8: icmp_seq=2 ttl=113 time=52.393 ms64 bytes from 8.8.8.8: icmp_seq=3 ttl=113 time=51.059 ms64 bytes from 8.8.8.8: icmp_seq=4 ttl=113 time=44.437 ms64 bytes from 8.8.8.8: icmp_seq=5 ttl=113 time=44.534 ms^C--- 8.8.8.8 ping statistics ---6 packets transmitted, 6 packets received, 0.0% packet lossround-trip min/avg/max/stddev = 44.437/48.221/52.393/3.640 ms 输出结果表明： 从MAC电脑到8.8.8.8的路径包含14个网络节点。 每个节点后面都显示时延信息，表明从MAC电脑到该节点的往返时延。您可能会发现，中间的某些节点往返时延可能要高于更远的节点的往返时延。这是由于该时延包含了路由器将TTL=0的报文交给控制面处理的时延，因而往往比路径上的传播时延要高，尤其是当控制面CPU繁忙时。Traceroute对每一个节点发出3个探测报文，每个报文的路径不尽相同，例如在第4、5、7、8、12、13跳经过不同的路由器转发。 有的探测没有得到回应，因此在有的节点本应显示时延信息的，显示了*。这并不能断定中间的网络不可达，很可能的原因是该节点的路由器在接口上配置了 no ip unreachable 命令，该接口不回应ICMP Unreachable消息，而该消息正是Traceroute探测路径所依赖的信息来源。 接下来，我们简要描述一下，在MAC电脑下，Traceroute的工作原理，并使用Wireshark进一步的探究Traceroute的工作过程。 Traceroute的工作原理每当IP数据包经过一个路由器，其存活时间TTL就会减1。当其存活时间是0时，主机便取消数据包，并发送一个ICMP TTL超时数据包给原数据包的发出者。Traceroute程序通过向目的地址发送一系列的探测包，设置探测包的TTL初始值分别为1,2,3…，根据返回的超时通知（ICMP Time Exceeded Message）得到源地址与目的地址之间的每一跳路由信息。 从源地址发出一个UDP探测包到目的地址，并将TTL设置为1； 到达路由器时，将TTL减1； 当TTL变为0时，包被丢弃，路由器向源地址发回一个ICMP超时通知（ICMP Time Exceeded Message），内含发送IP包的源地址，IP包的所有内容及路由器的IP地址； 当源地址收到该ICMP包时，显示这一跳路由信息； 重复1～5，并每次设置TTL加1； 直至目标地址收到探测数据包，并返回端口不可达通知（ICMP Port Unreachable）； 当源地址收到ICMP Port Unreachable包时停止traceroute。 以上内容引自Traceroute/tracert原理和实践 通过Wireshark探究Traceroute在Traceroute输出结果中显示AS号在Mac电脑的Traceroute命令中，通过’-a’的参数可以开启路径中遇到的IP地址所在的BGP AS号，以便于获知路径中每一跳所归属的运营商，通过’-q 1’可将缺省发送3个探测报文改为发送1个探测报文，输出示例如下： 12345678910111213141516➜ ~ traceroute -aq 1 8.8.8.8traceroute to 8.8.8.8 (8.8.8.8), 64 hops max, 52 byte packets 1 [AS0] bogon (10.39.101.1) 2.424 ms 2 [AS0] localhost (192.168.1.1) 3.031 ms 3 [AS4808] 222.129.32.1 (222.129.32.1) 6.496 ms 4 [AS4808] 61.51.101.101 (61.51.101.101) 8.114 ms 5 [AS17431] 219.232.11.29 (219.232.11.29) 5.865 ms 6 [AS4808] 202.96.12.13 (202.96.12.13) 6.558 ms 7 [AS4837] 219.158.112.46 (219.158.112.46) 44.826 ms 8 [AS4837] 219.158.103.218 (219.158.103.218) 52.181 ms 9 [AS4837] 219.158.103.30 (219.158.103.30) 47.851 ms10 [AS4837] 219.158.10.30 (219.158.10.30) 56.963 ms11 [AS4837] 219.158.33.174 (219.158.33.174) 46.523 ms12 [AS15169] 108.170.241.1 (108.170.241.1) 48.503 ms13 [AS15169] 172.253.64.111 (172.253.64.111) 142.926 ms14 [AS15169] dns.google (8.8.8.8) 52.208 ms 上述的AS编号信息是如何获得的？下面我们通过Wireshark抓取报文进行分析。 上述Traceroute的收发包过程简述开启Wireshark，并迅速执行traceroute -aq 1 8.8.8.8，Traceroute完整输出后，停止Wireshark的报文抓取。针对抓取的报文进行过滤，在过滤框中输入 udp or icmp or ip.addr == 198.108.0.18经过Wireshark的报文分析，以下为Traceroute的简要的收发包过程： 发起DNS查询，查询whois.radb.net域名，得到应答为198.108.0.18—-上述过滤器中IP地址来源于此。 发起TCP连接请求，与198.108.0.18建立TCP连接。 向8.8.8.8发起UDP报文，目的端口为33435，TTL设置为1 收到路由器的ICMP TTL超时消息，获取路由器对应的IP地址 向whois.radb.net发起查询，询问路由器IP地址对应的AS号，并得到回应，如果是私有地址，其AS号为0。 尝试向DNS服务器查询路由器IP地址的反向域名解析，如果得到域名，就将其显示在Traceroute的输出结果中。 重复上述第3~6步，每次将TTL加1，直至目标地址接收到探测报文，并返回ICMP Port Unreachable消息。 Wireshark抓包详细分析下图为Wireshark抓取的报文的前36个： 第1和第2个报文为DNS查询，查询whois.radb.net的地址为198.108.0.18。 第3~5的报文为TCP三次握手，并成功建立TCP 连接10.39.101.141:51705&lt;—&gt;198.108.0.18:43，TCP 43端口通常是whois server的端口。 第6个报文由10.39.101.141发向198.108.0.18，并将PSH置位，请求接收端一收到就进行向上交付，以缩短Traceroute的等待时间。 第7个报文发出第一个UDP的探测报文，目标端口号为33435，TTL=1，具体如下： 第8个报文为网关回复的ICMP Time-to-live exceeded (Time to live exceeded in transit)报文，Traceroute从IP报文中的源地址获取路由器的IP地址10.39.101.1。该报文包含原始的UDP报文信息，具体如下： 第9个报文为第6个报文的TCP确认报文，第10个报文为向whois.radb.net查询网关10.39.101.1/32所在的AS号。第11个报文为第10个报文的TCP确认，第12个报文答复查询结果，由于该地址为私有IP地址段，没有查询到结果，因此Traceroute 将AS号显示为[AS0]。以下使用第29和30报文，用于展示AS号查询的报文，如下： 查询结果的文本内容如下： 1234567891011121314151617181920212223route: 222.129.0.0/18descr: CMI (Customer Route)origin: AS4808mnt-by: MAINT-AS58453changed: qas_support@cmi.chinamobile.com 20160525source: RADBroute: 222.128.0.0/14descr: China Unicom Beijing Province Networkcountry: CNorigin: AS4808mnt-by: MAINT-CNCGROUP-RRchanged: abuse@cnc-noc.net 20160516source: APNICroute: 222.129.0.0/18descr: CMI IP Transitorigin: AS4808admin-c: MAINT-CMI-INT-HKtech-c: MAINT-CMI-INT-HKmnt-by: MAINT-CMI-INT-HKchanged: qas_support@cmi.chinamobile.com 20160525source: NTTCOM 第14~17报文是DNS解析报文，分别对主机名WERAO-M-40KA进行域名解析，以及对网关IP 10.39.101.1进行反向域名解析，由于是私有主机名和私有地址，自然公网DNS是解析不出来。以下将针对8.8.8.8进行反向域名解析为dns.google的截图作为示例： 报文18~26重复报文7~17的过程，其中报文18的TTL为2，如下： 过程持续至8.8.8.8返回Code: 3 (Port unreachable)的消息，并对8.8.8.8进行反向域名解析。 以上详细的分析了traceroute -aq 1 8.8.8.8的报文收发过程。 后记在上述的Traceroute完成输出后，Wireshark成功的将Whois的交互报文进行重新组装，并完成内容的解析。 在Wireshark分析完第133个报文后，Wireshark成功的组装出whois的查询报文，如下： 在Wireshark分析完第135个报文后，Wireshark成功的组装出Whois的应答报文，如下： 在Terminal执行whois 8.8.8.8/32，并进行报文抓取，解析出Whois的报文应答如下： 该报文也是Wireshark 拼装了多个TCP Segment，#101、#103、#104、#106、#109后解析出来的。对比该解析结果和上文中，Traceroute过程中的查询结果，可以发现，报文结构是一致的，都被Wireshark解析成Whois的报文。 据此，我推测，MAC电脑上的Traceroute 在增加了-a的参数后，会主动发起Whois查询，缺省的Whois服务器是whois.radb.net。 此部分内容是在Traceroute收发包过程分析完成后，再后知后觉发现的，因此将此部分内容作为“后记”进行记录。 Traceroute 进阶版工具MTRMTR （My Traceroute）工具将ping和traceroute命令的功能并入了同一个工具中，实现更强大的功能。相对于traceroute命令只会做一次链路跟踪测试，mtr命令会对链路上的相关节点做持续探测并给出相应的统计信息。 如下为MTR的输出： 1234567891011121314151617➜ ~ sudo mtr -ry 4 8.8.8.8Start: 2021-06-27T23:37:21+0800HOST: WERAO-M-40KA Loss% Snt Last Avg Best Wrst StDev 1. ??? localhost 0.0% 10 1.8 2.2 1.2 10.1 2.8 2. ??? bogon 0.0% 10 2.2 2.3 1.9 3.3 0.5 3. 2003-11-19 222.129.32.1 0.0% 10 6.3 9.5 5.0 29.2 7.7 4. 2000-03-14 61.148.163.181 0.0% 10 5.3 7.4 5.3 8.7 1.2 5. 2002-04-17 219.232.11.65 70.0% 10 5.4 6.1 5.4 6.8 0.7 6. 2006-01-09 124.65.194.153 30.0% 10 28.9 9.6 5.7 28.9 8.5 7. 2002-03-21 219.158.112.26 40.0% 10 38.9 44.2 38.5 63.7 9.7 8. 2002-03-21 219.158.19.66 0.0% 10 74.4 64.1 46.4 74.4 8.8 9. 2002-03-21 219.158.97.25 0.0% 10 90.8 90.8 82.2 96.7 4.3 10. 2002-03-21 219.158.20.94 0.0% 10 97.5 89.8 74.1 104.6 11.8 11. 2002-03-21 219.158.33.174 0.0% 10 73.0 62.1 49.7 74.0 8.5 12. 2012-02-07 108.170.241.65 0.0% 10 59.0 67.3 53.1 75.4 7.2 13. 2013-04-04 172.253.69.229 10.0% 10 62.9 68.3 54.5 76.6 6.5 14. 1992-12-01 dns.google 0.0% 10 71.0 65.6 56.4 71.0 5.7 MTR命令，通过-r 输出报告，-z 可输出地址所在的AS号，-y 4 可输出该地址的分配时间。 输出信息解释如下： 第一列（Host）：节点IP地址和域名。 第二列（Loss%）：节点丢包率。 第三列（Snt）：发送的Ping包数。默认值是10，可以通过参数“-c”指定。 第四列（Last）：最近一次的探测延迟值。 第五、六、七列（Avg、Best、Wrst）：分别是探测延迟的平均值、最小值和最大值。 第八列（StDev）：标准偏差。越大说明相应节点越不稳定。 上述解释引用自：MTR工具使用说明与结果分析，略有改动。 Q&amp;A David Tian: 赞，请教下，为什么ipv4的traceroute 会触发用ipv6去查dns呢？ 答：好眼力，好问题~~实际上，我抓取的报文中，每次得到ICMP报文后，Traceroute也会向208.67.222.222发送DNS请求。但是报文是加密的，看不出是什么内容，不能猜测，所以我将这些不确定的报文删除了。208.67.222.222是OpenDNS的DNS服务器，这是Cisco IT设置的DNS服务器。😄 附录以下是Wireshark抓包文件的前60个报文的概览。 No. Source Destination Protocol Info 1 fe80::1c7a:24fd:c837:3017 fe80::1 DNS Standard query 0xc7d5 A whois.radb.net 2 fe80::1 fe80::1c7a:24fd:c837:3017 DNS Standard query response 0xc7d5 A whois.radb.net A 198.108.0.18 3 10.39.101.141 198.108.0.18 TCP 51705 → 43 [SYN] Seq=0 Win=65535 Len=0 MSS=1460 WS=64 TSval=2785245374 TSecr=0 SACK_PERM=1 4 198.108.0.18 10.39.101.141 TCP 43 → 51705 [SYN, ACK] Seq=0 Ack=1 Win=28960 Len=0 MSS=1412 TSval=3642352050 TSecr=2785245374 WS=256 5 10.39.101.141 198.108.0.18 TCP 51705 → 43 [ACK] Seq=1 Ack=1 Win=131584 Len=0 TSval=2785245692 TSecr=3642352050 6 10.39.101.141 198.108.0.18 TCP 51705 → 43 [PSH, ACK] Seq=1 Ack=1 Win=131584 Len=3 TSval=2785245692 TSecr=3642352050 [TCP segment of a reassembled PDU] 7 10.39.101.141 8.8.8.8 UDP 36904 → 33435 Len=24 8 10.39.101.1 10.39.101.141 ICMP Time-to-live exceeded (Time to live exceeded in transit) 9 198.108.0.18 10.39.101.141 TCP 43 → 51705 [ACK] Seq=1 Ack=4 Win=29184 Len=0 TSval=3642352370 TSecr=2785245692 10 10.39.101.141 198.108.0.18 TCP 51705 → 43 [PSH, ACK] Seq=4 Ack=1 Win=131584 Len=19 TSval=2785246039 TSecr=3642352370 [TCP segment of a reassembled PDU] 11 198.108.0.18 10.39.101.141 TCP 43 → 51705 [ACK] Seq=1 Ack=23 Win=29184 Len=0 TSval=3642352717 TSecr=2785246039 12 198.108.0.18 10.39.101.141 TCP 43 → 51705 [PSH, ACK] Seq=1 Ack=23 Win=29184 Len=2 TSval=3642352717 TSecr=2785246039 [TCP segment of a reassembled PDU] 13 10.39.101.141 198.108.0.18 TCP 51705 → 43 [ACK] Seq=23 Ack=3 Win=131584 Len=0 TSval=2785246448 TSecr=3642352717 14 fe80::1c7a:24fd:c837:3017 fe80::1 DNS Standard query 0x93fc A WERAO-M-40KA 15 fe80::1 fe80::1c7a:24fd:c837:3017 DNS Standard query response 0x93fc No such name A WERAO-M-40KA 16 fe80::1c7a:24fd:c837:3017 fe80::1 DNS Standard query 0xbcf9 PTR 1.101.39.10.in-addr.arpa 17 fe80::1 fe80::1c7a:24fd:c837:3017 DNS Standard query response 0xbcf9 PTR 1.101.39.10.in-addr.arpa PTR bogon 18 10.39.101.141 8.8.8.8 UDP 36904 → 33436 Len=24 19 192.168.1.1 10.39.101.141 ICMP Time-to-live exceeded (Time to live exceeded in transit) 20 10.39.101.141 198.108.0.18 TCP 51705 → 43 [PSH, ACK] Seq=23 Ack=3 Win=131584 Len=19 TSval=2785248590 TSecr=3642352717 [TCP segment of a reassembled PDU] 21 10.39.101.141 10.39.101.1 DNS Standard query 0xc81a A WERAO-M-40KA 22 10.39.101.1 10.39.101.141 DNS Standard query response 0xc81a A WERAO-M-40KA A 10.39.101.141 23 198.108.0.18 10.39.101.141 TCP 43 → 51705 [PSH, ACK] Seq=3 Ack=42 Win=29184 Len=2 TSval=3642355278 TSecr=2785248590 [TCP segment of a reassembled PDU] 24 10.39.101.141 198.108.0.18 TCP 51705 → 43 [ACK] Seq=42 Ack=5 Win=131584 Len=0 TSval=2785248902 TSecr=3642355278 25 fe80::1c7a:24fd:c837:3017 fe80::1 DNS Standard query 0x7504 PTR 1.1.168.192.in-addr.arpa 26 fe80::1 fe80::1c7a:24fd:c837:3017 DNS Standard query response 0x7504 PTR 1.1.168.192.in-addr.arpa PTR localhost 27 10.39.101.141 8.8.8.8 UDP 36904 → 33437 Len=24 28 222.129.32.1 10.39.101.141 ICMP Time-to-live exceeded (Time to live exceeded in transit) 29 10.39.101.141 198.108.0.18 TCP 51705 → 43 [PSH, ACK] Seq=42 Ack=5 Win=131584 Len=20 TSval=2785249974 TSecr=3642355278 [TCP segment of a reassembled PDU] 30 198.108.0.18 10.39.101.141 TCP 43 → 51705 [PSH, ACK] Seq=5 Ack=62 Win=29184 Len=643 TSval=3642356666 TSecr=2785249974 [TCP segment of a reassembled PDU] 31 10.39.101.141 198.108.0.18 TCP 51705 → 43 [ACK] Seq=62 Ack=648 Win=130944 Len=0 TSval=2785250324 TSecr=3642356666 32 fe80::1c7a:24fd:c837:3017 fe80::1 DNS Standard query 0x4f56 PTR 1.32.129.222.in-addr.arpa 33 fe80::1 fe80::1c7a:24fd:c837:3017 DNS Standard query response 0x4f56 No such name PTR 1.32.129.222.in-addr.arpa 34 10.39.101.141 8.8.8.8 UDP 36904 → 33438 Len=24 35 61.51.101.101 10.39.101.141 ICMP Time-to-live exceeded (Time to live exceeded in transit) 36 10.39.101.141 198.108.0.18 TCP 51705 → 43 [PSH, ACK] Seq=62 Ack=648 Win=131072 Len=21 TSval=2785251389 TSecr=3642356666 [TCP segment of a reassembled PDU] 37 198.108.0.18 10.39.101.141 TCP 43 → 51705 [PSH, ACK] Seq=648 Ack=83 Win=29184 Len=639 TSval=3642358087 TSecr=2785251389 [TCP segment of a reassembled PDU] 38 10.39.101.141 198.108.0.18 TCP 51705 → 43 [ACK] Seq=83 Ack=1287 Win=130432 Len=0 TSval=2785251697 TSecr=3642358087 39 10.39.101.141 10.39.101.1 DNS Standard query 0x521b PTR 1.32.129.222.in-addr.arpa 40 10.39.101.1 10.39.101.141 DNS Standard query response 0x521b No such name PTR 1.32.129.222.in-addr.arpa 41 fe80::1c7a:24fd:c837:3017 fe80::1 DNS Standard query 0x5931 PTR 101.101.51.61.in-addr.arpa 42 fe80::1 fe80::1c7a:24fd:c837:3017 DNS Standard query response 0x5931 No such name PTR 101.101.51.61.in-addr.arpa 43 10.39.101.141 8.8.8.8 UDP 36904 → 33439 Len=24 44 219.232.11.29 10.39.101.141 ICMP Time-to-live exceeded (Time to live exceeded in transit) 45 10.39.101.141 198.108.0.18 TCP 51705 → 43 [PSH, ACK] Seq=83 Ack=1287 Win=131072 Len=21 TSval=2785252732 TSecr=3642358087 [TCP segment of a reassembled PDU] 46 198.108.0.18 10.39.101.141 TCP 43 → 51705 [PSH, ACK] Seq=1287 Ack=104 Win=29184 Len=418 TSval=3642359437 TSecr=2785252732 [TCP segment of a reassembled PDU] 47 10.39.101.141 198.108.0.18 TCP 51705 → 43 [ACK] Seq=104 Ack=1705 Win=130624 Len=0 TSval=2785253042 TSecr=3642359437 48 fe80::1c7a:24fd:c837:3017 fe80::1 DNS Standard query 0x7d09 PTR 29.11.232.219.in-addr.arpa 49 fe80::1 fe80::1c7a:24fd:c837:3017 DNS Standard query response 0x7d09 No such name PTR 29.11.232.219.in-addr.arpa 50 10.39.101.141 8.8.8.8 UDP 36904 → 33440 Len=24 51 202.96.12.13 10.39.101.141 ICMP Time-to-live exceeded (Time to live exceeded in transit) 52 10.39.101.141 198.108.0.18 TCP 51705 → 43 [PSH, ACK] Seq=104 Ack=1705 Win=131072 Len=20 TSval=2785254068 TSecr=3642359437 [TCP segment of a reassembled PDU] 53 10.39.101.141 10.39.101.1 DNS Standard query 0x961b PTR 101.101.51.61.in-addr.arpa 54 10.39.101.1 10.39.101.141 DNS Standard query response 0x961b No such name PTR 101.101.51.61.in-addr.arpa 55 198.108.0.18 10.39.101.141 TCP 43 → 51705 [PSH, ACK] Seq=1705 Ack=124 Win=29184 Len=642 TSval=3642360779 TSecr=2785254068 [TCP segment of a reassembled PDU] 56 10.39.101.141 198.108.0.18 TCP 51705 → 43 [ACK] Seq=124 Ack=2347 Win=130368 Len=0 TSval=2785254400 TSecr=3642360779 57 fe80::1c7a:24fd:c837:3017 fe80::1 DNS Standard query 0xb016 PTR 13.12.96.202.in-addr.arpa 58 fe80::1 fe80::1c7a:24fd:c837:3017 DNS Standard query response 0xb016 No such name PTR 13.12.96.202.in-addr.arpa SOA beijing.cn.net 59 10.39.101.141 8.8.8.8 UDP 36904 → 33441 Len=24 60 219.158.112.46 10.39.101.141 ICMP Time-to-live exceeded (Time to live exceeded in transit) 抓包文件下载","categories":[],"tags":[{"name":"traceroute","slug":"traceroute","permalink":"https://weiborao.link/tags/traceroute/"},{"name":"wireshark","slug":"wireshark","permalink":"https://weiborao.link/tags/wireshark/"}]},{"title":"在AWS上部署TE Agent并进行测试","slug":"deploy-thousandeyes-agent-on-aws","date":"2021-06-17T13:00:00.000Z","updated":"2021-06-17T13:06:53.306Z","comments":true,"path":"deploy-thousandeyes-agent-on-aws.html","link":"","permalink":"https://weiborao.link/deploy-thousandeyes-agent-on-aws.html","excerpt":"","text":"ThousandEyes 简介ThousandEyes是一个网络性能监控的SAAS云服务，结合了各种主动和被动的监控技术，让您深入了解您提供的以及您消费的应用和服务的用户体验。ThousandEyes使用的监控技术包括网络的可达性探测、时延、丢包、抖动、可视化的逐跳路径分析、可视化的BGP路由分析、DNS监控、HTTP服务监控等。ThousandEyes平台对这些监控收集而来的数据进行分析、交叉关联，将涉及用户体验的方方面面，包括网络和应用的状况统一的呈现在同一个界面之下，让您能够轻松的隔离问题，采取行动，从而快速的解决问题。 ThousandEyes提供了三种Agent进行网络和应用的探测，分别是Cloud Agent、Enterprise Agent和Endpoint Agent。Cloud Agent 由ThousandEyes在全球部署和维护，当前，ThousandEyes在全球200多个城市共部署了400多个Cloud Agent，可供全球用户使用。Enterprise Agent由用户自己部署，可以部署为虚拟机或者容器，可以安装在物理硬件，如Intel NCU或者树莓派中，支持Windows、Linux系统，还可以部署在思科或Juniper的网络设备中，也能通过CloudFormation在AWS云中自动部署。Endpoint Agent是浏览器插件，用户在访问网站时，可以自助的使用Endpoint Agent进行测试，由ThousandEyes进行数据分析，从而帮助用户快速了解其数字体验，以及快速定位问题所在。用户可以根据自己的需要来选择一种或多种Agent进行探测，ThousandEyes平台会自动完成分析和展现，提供网络和应用状况的洞见分析。 在AWS上部署TE Agent了解部署过程在正式部署之前，快速阅读了一下AWS Deployment Guide，部署过程是通过AWS的CloudFormation创建一个Stack，整个过程全部自动化。 部署的前提是需要设置好SSH Key-pair，VPC网络、公共子网、以及使用CloudFormation部署EC2的权限。 部署的内容包括：启动基于Ubuntu的EC2实例，并自动安装TE Agent，创建Security Group，并基于最小权限配置Inbound Rules。 准备SSH Key-pair首先，在MAC电脑中，执行以下命令，用于创建一个RSA的秘钥对，并将公钥拷贝至桌面。 1234ssh-keygen -t rsa -b 4096 -f .ssh/aws_kp -m PEMmv .ssh/aws_kp .ssh/aws_kp.pemchmod 400 .ssh/aws_kp.pemcp .ssh/aws_kp.pub ~/Desktop 登录AWS的Console管理界面后，选择Region，比如us-east-1，定位到EC2—Network Security—Key Pair，在界面右上侧的Actions下拉框中，选择Import key pair，将aws_kp.pub上传。 在其他的Region，重复上述动作，将key pair上传，这样在多个Region创建的EC2可以使用同一个私钥进行登录。 在AWS上部署TE Agent部署TE Agent的操作路径为：在ThousandEyes的界面中，选择Cloud &amp; Enterprise Agents，再选择Agent Settings，进一步选择Enterprise Agents，点击Add New Enterprise Agent。在右侧出现的界面中，选择IaaS Marketplaces，在该界面中点击Launch in AWS，跳转到AWS的登录界面，并自动进入CloudFormation的界面，该界面已将Stack模板选择好了。 Stack模板的链接： https://s3-us-west-1.amazonaws.com/oneclick-ea.aws.thousandeyes/aws-ea-oneclick.yaml 该模板包含两个组件：EC2 Instance 和 Security Group。 按照上文的部署指南填写表单，并点击下一步，直至完成部署。 在完成安装后，可以使用私钥登录到虚机，注意：用户名为 ubuntu，不是ec2-user 或 root。 1ssh -i .ssh/aws_kp.pem -v [ubuntu@](mailto:ubuntu@18.141.230.240)x.x.x.x 在两个不同的Region，us-east-1 和 ap-southeast-1 分别部署一个TE Agent。 大约5分钟后，即可在app.thousandeyes.com的界面中看到两个TE Agent上线。 执行Agent to Agent 测试分别执行两种测试，一个是通过公网IP进行双向测试，一个是创建VPC Peering后，使用私有地址进行测试，比较两者之间的差异。 通过公网IP地址进行双向测试在ThousandEyes的Agent Settings界面中，点击Agent，在右侧的网页中，选择Advanced Settings，将地址设置为Agent的公网IP地址。 在Test Settings中，选择Add New Test，Layer选择Network，Test Type选择Agent to Agent。 选择一个Agent作为Target，另一个Agent作为源，测试方向选择双向。 经过一段时间后，ThousandEyes上即可呈现测试结果。 测试持续了一个小时左右，测试结果如下： AWS us-east-1 to ap-southeast-1 Using Public IP Items Result Loss 0% Latency 212ms Jitter &lt;1ms Throughput 55Mbps 在可视化的路径分析图中，观察到如下信息： 从us-east-1到ap-southeast-1往返，至少各有三条路径，每条路径显示的跳数约有20跳。 该测试每隔2分钟测试一次，共测试了25次。在这25次中，鲜有路径一致的，有时候全程没有公共节点，有时候两条、甚至三条路径中间有公共节点。 可视化路径中，从两侧的Agent出发，均有连续的5个或6个连续未知节点。这是由于这些节点不对Traceroute作出回应，导致路径中不可见。 在整个可视化路径中，可见的公网节点的IP地址属于AS 16509或者AS 14618，这两个都是AWS的BGP AS域。其他节点的地址为100.65.x.x或100.100.x.x，这段地址属于100.64.0.0/10，为IANA保留地址，用于给运营商使用。由此可以推断，两个Agent之间通讯的报文并未离开过AWS的网络，这也解释了为何上述的时延、抖动是很稳定的值，并且整个测试期间，没有丢包。 在可视化路径中，我还能发现有的路径是一段MPLS 隧道，并给出了转发时用到的Label值。ThousandEyes是如何发现路径中间有MPLS Tunnel呢？这个是值得仔细思考的问题。经查询，这是通过ThousandEyes的专利技术Deep Path Analysis (DPA)实现的。 通过私网IP地址进行双向测试本次测试的方法同上，只是需要将Agent的IP地址修改为使用私有IP地址，两个Region的VPC之间建立VPC Peering。 测试结果如下： AWS us-east-1 to ap-southeast-1 Using Private IP Items Result Loss 0% Latency 212ms Jitter &lt;1ms Throughput 56Mbps 在可视化的路径分析图中，观察到如下信息： 两个Agent之间是直连的，没有任何中间节点，两者之间时延为211ms。 做完两次测试，第二天检查了一下AWS的账单，预计花费了0.29美金。 测试结果的共享链接https://zwskwtsea.share.thousandeyes.com/ https://aieajezsh.share.thousandeyes.com/","categories":[],"tags":[{"name":"ThousandEyes","slug":"ThousandEyes","permalink":"https://weiborao.link/tags/ThousandEyes/"},{"name":"AWS","slug":"AWS","permalink":"https://weiborao.link/tags/AWS/"}]},{"title":"Mastering KVM Virtualization 学习笔记(1)","slug":"master-kvm-notes-1","date":"2021-02-22T06:23:47.000Z","updated":"2021-02-22T06:30:40.658Z","comments":true,"path":"master-kvm-notes-1.html","link":"","permalink":"https://weiborao.link/master-kvm-notes-1.html","excerpt":"","text":"第一章和第二章之前快速阅读过了，没有笔记，等回头再回来整理。 Chapter 3 KVM、virsh、oVirt等CentOS 8 的KVM安装123yum module install virtdnf install qemu-img qemu-kvm libvirt libvirt-client virt-manager \\virt-install virt-viewer -y 安装完后，使用virt-host-validate验证12345678910111213141516171819202122232425262728293031323334353637383940[root@centos7 ~]# virt-host-validate QEMU: Checking for hardware virtualization : PASS QEMU: Checking if device /dev/kvm exists : PASS QEMU: Checking if device /dev/kvm is accessible : PASS QEMU: Checking if device /dev/vhost-net exists : PASS QEMU: Checking if device /dev/net/tun exists : PASS QEMU: Checking for cgroup &#x27;memory&#x27; controller support : PASS QEMU: Checking for cgroup &#x27;memory&#x27; controller mount-point : PASS QEMU: Checking for cgroup &#x27;cpu&#x27; controller support : PASS QEMU: Checking for cgroup &#x27;cpu&#x27; controller mount-point : PASS QEMU: Checking for cgroup &#x27;cpuacct&#x27; controller support : PASS QEMU: Checking for cgroup &#x27;cpuacct&#x27; controller mount-point : PASS QEMU: Checking for cgroup &#x27;cpuset&#x27; controller support : PASS QEMU: Checking for cgroup &#x27;cpuset&#x27; controller mount-point : PASS QEMU: Checking for cgroup &#x27;devices&#x27; controller support : PASS QEMU: Checking for cgroup &#x27;devices&#x27; controller mount-point : PASS QEMU: Checking for cgroup &#x27;blkio&#x27; controller support : PASS QEMU: Checking for cgroup &#x27;blkio&#x27; controller mount-point : PASS QEMU: Checking for device assignment IOMMU support : PASS QEMU: Checking if IOMMU is enabled by kernel : PASS LXC: Checking for Linux &gt;= 2.6.26 : PASS LXC: Checking for namespace ipc : PASS LXC: Checking for namespace mnt : PASS LXC: Checking for namespace pid : PASS LXC: Checking for namespace uts : PASS LXC: Checking for namespace net : PASS LXC: Checking for namespace user : PASS LXC: Checking for cgroup &#x27;memory&#x27; controller support : PASS LXC: Checking for cgroup &#x27;memory&#x27; controller mount-point : PASS LXC: Checking for cgroup &#x27;cpu&#x27; controller support : PASS LXC: Checking for cgroup &#x27;cpu&#x27; controller mount-point : PASS LXC: Checking for cgroup &#x27;cpuacct&#x27; controller support : PASS LXC: Checking for cgroup &#x27;cpuacct&#x27; controller mount-point : PASS LXC: Checking for cgroup &#x27;cpuset&#x27; controller support : PASS LXC: Checking for cgroup &#x27;cpuset&#x27; controller mount-point : PASS LXC: Checking for cgroup &#x27;devices&#x27; controller support : PASS LXC: Checking for cgroup &#x27;devices&#x27; controller mount-point : PASS LXC: Checking for cgroup &#x27;blkio&#x27; controller support : PASS LXC: Checking for cgroup &#x27;blkio&#x27; controller mount-point : PASS LXC: Checking if device /sys/fs/fuse/connections exists : PASS 使用virt-install 安装虚拟机编辑anaconda-ks.cfg文件，使用以下参数可以实现虚机部署的静默安装。 1234virt-install --virt-type=kvm --name=MasteringKVM02 --ram=4096--vcpus=4 --os-variant=rhel8.0 --location=/var/lib/libvirt/images/ CentOS-8-x86_64-1905-dvd1.iso --network=default--graphics vnc --disk size=16 -x &quot;ks=http://10.10.48.1/ks.cfg&quot; oVirtoVirt（Open Virtualization Manager）是一款免费开源虚拟化软件，是RedHat商业版本虚拟化软件RHEV的开源版本。 oVirt基于kvm，并整合使用了libvirt、gluster、patternfly、ansible等一系列优秀的开源软件。 探索虚拟机Qemu-kvm进程12root@ubuntu-kvm:/home/ubuntu# ps aux | grep qemulibvirt+ 8526 255 0.1 13876708 653744 ? SLl Feb01 61850:36 /usr/bin/qemu-system-x86_64 -name guest=csr1kv-1,debug-threads=on -S -object secret,id=masterKey0,format=raw,file=/var/lib/libvirt/qemu/domain-4-csr1kv-1/master-key.aes -machine pc-q35-4.2,accel=kvm,usb=off,vmport=off,dump-guest-core=off,mem-merge=off -cpu host -m 8192 -mem-prealloc -mem-path /dev/hugepages/libvirt/qemu/4-csr1kv-1 -overcommit mem-lock=on -smp 8,sockets=8,cores=1,threads=1 -uuid 83f90c1c-f0ea-4298-bcdf-3d676de2aeb4 -no-user-config -nodefaults -chardev socket,id=charmonitor,fd=31,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc,driftfix=slew -global kvm-pit.lost_tick_policy=delay -no-hpet -no-shutdown -global ICH9-LPC.disable_s3=1 -global ICH9-LPC.disable_s4=1 -boot strict=on -device pcie-root-port,port=0x10,chassis=1,id=pci.1,bus=pcie.0,multifunction=on,addr=0x2 -device pcie-root-port,port=0x11,chassis=2,id=pci.2,bus=pcie.0,addr=0x2.0x1 -device pcie-root-port,port=0x12,chassis=3,id=pci.3,bus=pcie.0,addr=0x2.0x2 -device pcie-root-port,port=0x13,chassis=4,id=pci.4,bus=pcie.0,addr=0x2.0x3 -device pcie-root-port,port=0x14,chassis=5,id=pci.5,bus=pcie.0,addr=0x2.0x4 -device pcie-root-port,port=0x15,chassis=6,id=pci.6,bus=pcie.0,addr=0x2.0x5 -device pcie-root-port,port=0x16,chassis=7,id=pci.7,bus=pcie.0,addr=0x2.0x6 -device pcie-root-port,port=0x17,chassis=8,id=pci.8,bus=pcie.0,addr=0x2.0x7 -device qemu-xhci,p2=15,p3=15,id=usb,bus=pci.3,addr=0x0 -device virtio-serial-pci,id=virtio-serial0,bus=pci.4,addr=0x0 -blockdev &#123;&quot;driver&quot;:&quot;file&quot;,&quot;filename&quot;:&quot;/var/lib/libvirt/images/csr1000v-universalk9.17.02.01v.qcow2&quot;,&quot;node-name&quot;:&quot;libvirt-1-storage&quot;,&quot;auto-read-only&quot;:true,&quot;discard&quot;:&quot;unmap&quot;&#125; -blockdev &#123;&quot;node-name&quot;:&quot;libvirt-1-format&quot;,&quot;read-only&quot;:false,&quot;driver&quot;:&quot;qcow2&quot;,&quot;file&quot;:&quot;libvirt-1-storage&quot;,&quot;backing&quot;:null&#125; -device virtio-blk-pci,scsi=off,bus=pci.5,addr=0x0,drive=libvirt-1-format,id=virtio-disk0,bootindex=1 -netdev tap,fd=33,id=hostnet0,vhost=on,vhostfd=34 -device virtio-net-pci,netdev=hostnet0,id=net0,mac=52:54:00:69:ec:73,bus=pci.1,addr=0x0 -chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0 -chardev socket,id=charchannel0,fd=35,server,nowait -device virtserialport,bus=virtio-serial0.0,nr=1,chardev=charchannel0,id=channel0,name=org.qemu.guest_agent.0 -chardev spicevmc,id=charchannel1,name=vdagent -device virtserialport,bus=virtio-serial0.0,nr=2,chardev=charchannel1,id=channel1,name=com.redhat.spice.0 -spice port=5900,addr=127.0.0.1,disable-ticketing,image-compression=off,seamless-migration=on -device qxl-vga,id=video0,ram_size=67108864,vram_size=67108864,vram64_size_mb=0,vgamem_mb=16,max_outputs=1,bus=pcie.0,addr=0x1 -device vfio-pci,host=0000:5e:02.0,id=hostdev0,bus=pci.2,addr=0x0 -device vfio-pci,host=0000:5e:0a.0,id=hostdev1,bus=pci.8,addr=0x0 -sandbox on,obsolete=deny,elevateprivileges=deny,spawn=deny,resourcecontrol=deny -msg timestamp=on 使用pstree检查线程信息： 123456789101112root@ubuntu-kvm:/home/ubuntu# pstree -p 8526qemu-system-x86(8526)─┬─&#123;qemu-system-x86&#125;(8530) ├─&#123;qemu-system-x86&#125;(8534) ├─&#123;qemu-system-x86&#125;(8535) ├─&#123;qemu-system-x86&#125;(8536) ├─&#123;qemu-system-x86&#125;(8537) ├─&#123;qemu-system-x86&#125;(8538) ├─&#123;qemu-system-x86&#125;(8539) ├─&#123;qemu-system-x86&#125;(8540) ├─&#123;qemu-system-x86&#125;(8541) ├─&#123;qemu-system-x86&#125;(8542) ├─&#123;qemu-system-x86&#125;(8552) 使用gbd 调试进程，这里参考https://www.cnblogs.com/hukey/p/11138768.html 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051root@ubuntu-kvm:/home/ubuntu# gdb attach 8526GNU gdb (Ubuntu 9.2-0ubuntu1~20.04) 9.2Copyright (C) 2020 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law.Type &quot;show copying&quot; and &quot;show warranty&quot; for details.This GDB was configured as &quot;x86_64-linux-gnu&quot;.Type &quot;show configuration&quot; for configuration details.For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;.Find the GDB manual and other documentation resources online at: &lt;http://www.gnu.org/software/gdb/documentation/&gt;.For help, type &quot;help&quot;.Type &quot;apropos word&quot; to search for commands related to &quot;word&quot;.Attaching to process 8526[New LWP 8530][New LWP 8534][New LWP 8535][New LWP 8536][New LWP 8537][New LWP 8538][New LWP 8539][New LWP 8540][New LWP 8541][New LWP 8542][New LWP 8552][New LWP 250213]warning: &quot;target:/usr/bin/qemu-system-x86_64 (deleted)&quot;: could not open as an executable file: No such file or directory.warning: `target:/usr/bin/qemu-system-x86_64 (deleted)&#x27;: can&#x27;t open to read symbols: No such file or directory.warning: Could not load vsyscall page because no executable was specified0x00007fdcda986bf6 in ?? ()(gdb) info threads Id Target Id Frame* 1 LWP 8526 &quot;qemu-system-x86&quot; 0x00007fdcda986bf6 in ?? () 2 LWP 8530 &quot;call_rcu&quot; 0x00007fdcda98c89d in ?? () 3 LWP 8534 &quot;IO mon_iothread&quot; 0x00007fdcda986aff in ?? () 4 LWP 8535 &quot;CPU 0/KVM&quot; 0x00007fdcda98850b in ?? () 5 LWP 8536 &quot;CPU 1/KVM&quot; 0x00007fdcda98850b in ?? () 6 LWP 8537 &quot;CPU 2/KVM&quot; 0x00007fdcda98850b in ?? () 7 LWP 8538 &quot;CPU 3/KVM&quot; 0x00007fdcda98850b in ?? () 8 LWP 8539 &quot;CPU 4/KVM&quot; 0x00007fdcda98850b in ?? () 9 LWP 8540 &quot;CPU 5/KVM&quot; 0x00007fdcda98850b in ?? () 10 LWP 8541 &quot;CPU 6/KVM&quot; 0x00007fdcda98850b in ?? () 11 LWP 8542 &quot;CPU 7/KVM&quot; 0x00007fdcda98850b in ?? () 12 LWP 8552 &quot;SPICE Worker&quot; 0x00007fdcda986aff in ?? () 13 LWP 250213 &quot;worker&quot; 0x00007fdcdaa76618 in ?? () Chapter 4 KVM 网络本章节介绍了以下内容 Virtual Network – 阐述为什么需要虚拟网络。从虚拟交换机开始，设想一下如果没有虚拟交换机，有20个虚拟机需要20个物理接口连接至20个物理交换机的端口。引入虚拟交换机，减少服务器的物理接口和物理交换机的接口需求。 Libvirt 的三类网络：NAT、Routed(Bridged)、Isolated；使用virsh net-dumpxml default输出xml文件。 TUN/TAP设备：属于用户空间的设备，an application can open /dev/net/tun and use an ioctl() function to register a network device in the kernel, which, in turn, presents itself as a tunXX or tapXX device. TUN设备是一个3层设备，TAP设备是一个2层设备。 简单介绍了Linux Bridge，此处可以使用ip link 命令替代brctl命令，并使用NetworkManager来持久化配置。 Open vSwitch简单介绍 SR-IOV的介绍：此处可以参考我自己的文档。 MACVTAP介绍：It’s a newer driver that should simplify our virtualized networking by completely removing tun/tap and bridge drivers with a single module. 有四种模式，VEPA，Bridge，Private和Pass-through。 Chapter 5 KVM 存储本章属于囫囵吞枣的看完了，一知半解的记录一下，大致了解了一些存储方面的概念和存储的最新发展。 存储资源池CentOS支持的存储池种类如下： Logical Volume Manager (LVM)-based storage pools Directory-based storage pools Partition-based storage pools GlusterFS-based storage pools iSCSI-based storage pools Disk-based storage pools HBA-based storage pools, which use SCSI devices 对于libvirt来说，存储资源池可能是一个目录，一个存储设备，或者是一个文件。 介绍了两种文件系统： Brtfs is a type of filesystem that supports snapshots, RAID and LVM-like functionality, compression, defragmentation, online resizing, and many other advanced features. It was deprecated after it was discovered that its RAID5/6 can easily lead to a loss of data. –这里说的是Red Hat ZFS is a type of filesystem that supports everything that Brtfs does, plus read and write caching. ZFS is not a part of the Linux kernel. 本地存储池Stratis是随RHEL 8引入的本地管理存储解决方案，它使系统管理员可以配置高级存储功能： Pool-based management Thin provisioning File system snapshots Monitoring Stratis使用示例： 123456mdadm --create /dev/md0 --verbose --level=10 --raid-devices=4 /dev/sdb /dev/sdc /dev/sdd /dev/sde --spare-devices=1 /dev/sdf2stratis pool create PacktStratisPool01 /dev/md0stratis pool add-cache PacktStratisPool01 /dev/sdgstratis fs create PackStratisPool01 PacktStratisXFS01mkdir /mnt/packtStratisXFS01mount /stratis/PacktStratisPool01/PacktStratisXFS01 /mnt/packtStratisXFS01 Libvirt Storage PoolLibvirt 支持多种Storage Pool，详见https://libvirt.org/storage.html Directory pool Filesystem pool Network filesystem pool Logical volume pool Disk pool iSCSI pool iSCSI direct pool SCSI pool Multipath pool RBD pool Sheepdog pool Gluster pool ZFS pool Vstorage pool NFS Storage PoolNFS自上世纪80年代就出现了，至今依然活跃，尤其是4.2版本以后增强了不少功能；VMware Virtual Volume 6.0 可以提供基于Block和NFS的两种存储访问。 Libvirt 使用NFS Storage Pool示例，也可以使用virt-manager 图形界面创建： 1234567891011121314151617&lt;pool type=&#x27;netfs&#x27;&gt; &lt;name&gt;NFSpooll&lt;/name&gt; &lt;source&gt; &lt;host name=&#x27;192.168.159.144&#x27; /&gt; &lt;dir path=&#x27;/mnt/packtStratisXF501&#x27; /&gt; &lt;format type=&#x27;auto&#x27;/&gt; &lt;/source&gt; &lt;target&gt; &lt;path&gt;/var/lib/libvirt/images/NFSpooll&lt;/path&gt; &lt;permissions&gt; &lt;mode&gt;0755&lt;/mode&gt; &lt;owner&gt;0&lt;/owner&gt; &lt;group&gt;0&lt;/group&gt; &lt;label&gt;system_u:object r:nfs t:s0&lt;/label&gt; &lt;/permissions&gt; &lt;/target&gt;&lt;/pool&gt; iSCIS 和 SAN 存储iSCIS协议有两个主要原因影响其高效性： IP协议封装 iSCSI encapsulates SCSI commands into regular IP packages, which means segmentation and overhead as IP packages have a pretty large header, which means less efficiency. 基于TCP传输 Even worse, it’s TCP-based, which means that there are sequence numbers and retransmissions, which can lead to queueing and latency, and the bigger the environment is, the more you usually feel these effects affect your virtual machine performance. The iSCSI and FC architectures are very similar—they both need a target (an iSCSI target and an FC target) and an initiator (an iSCS initiator and an FC initiator)，the initiator connects to a target to get access to block storage that’s presented via that target. LUN的概念LUNs are just raw, block capacities that we export via an iSCSI target toward initiators. LUNs are indexed, or numbered, usually from 0 onward. Every LUN number represents a different storage capacity that an initiator can connect to. For example, we can have an iSCSI target with three different LUNs—LUN0 with 20 GB, LUN1 with 40 GB, and LUN2 with 60 GB. These will all be hosted on the same storage system’s iSCSI target. We can then configure the iSCSI target to accept an IQN to see all the LUNs, another IQN to only see LUN1, and another IQN to only see LUN1 and LUN2. 使用 targetcli命令创建iSCIS target，步骤简要说明如下： 创建/dev/sdb1分区和XFS文件系统，并mount /dev/sdb1 /LUN0，用targetcli创建一个fileio 类型的target后端 创建/dev/sdc1分区，并直接使用targetcli创建一个block类型的target后端 基于/dev/sdd创建LVM，并使用targetcli创建一个block类型的target后端 在KVM主机上设置好iscsi的initiator参数 回到iSCSI上，创建ACL，允许KVM host，基于1~3创建的target发布LUNs 在KVM Host上定义Storage Pool的XML文件，用virsh pool-define创建Storage Pool Storage redundancy and multipathing避免单点故障SPOF，网卡、线缆、交换机、存储控制器等等。 KVM Host基于iSCSI做冗余和多路径的配置较为复杂，并且缺少支持，文章建议使用oVirt或者RHEV-H(Red Hat Enterprise Virtualization Hypervisor. ) 使用oVirt 创建一个iSCSI Bond来实现冗余和多路径。 GlusterGluster 是一个分布式文件系统，其突出优势是可扩展性，数据复制和快照功能；注意Gluster是一个文件存储服务。 生产环境中，Gluster至少配置三台服务器。 配置步骤简要说明如下： 创建磁盘分区以及文件系统 123456mkfs.xfs /dev/sdbmkdir /gluster/bricks/1 -pecho &#x27;/dev/sdb /gluster/bricks/1 xfs defaults 0 0&#x27; &gt;&gt; /etc/fstabmount -amkdir /gluster/bricks/1/brick 创建Gluster分布式文件系统 12345678gluster volume create kvmgluster replica 3 \\ gluster1:/gluster/bricks/1/brick gluster2:/gluster/bricks/1/brick \\ gluster3:/gluster/bricks/1/brickgluster volume start kvmglustergluster volume set kvmgluster auth.allow 192.168.159.0/24gluster volume set kvmgluster allow-insecure ongluster volume set kvmgluster storage.owner-uid 107gluster volume set kvmgluster storage.owner-gid 107 挂在Gluster文件系统为NFS服务 123echo &#x27;localhost:/kvmgluster /mnt glusterfs \\ defaults,_netdev,backupvolfile-server=localhost 0 0&#x27; &gt;&gt; /etc/fstabmount.glusterfs localhost:/kvmgluster /mnt 在KVM主机上挂在Gluster 123456wget \\ https://download.gluster.org/pub/gluster/glusterfs/6/LATEST/CentOS/gl\\ usterfs-rhel8.repo -P /etc/yum.repos.dyum install glusterfs glusterfs-fuse attr -ymount -t glusterfs -o context=&quot;system_u:object_r:virt_image_t:s0&quot; \\ gluster1:/kvmgluster /var/lib/libvirt/images/GlusterFS 在KVM上定义存储资源池： 123456789101112&lt;pool type=&#x27;dir&#x27;&gt; &lt;name&gt;glusterfs-pool&lt;/name&gt; &lt;target&gt; &lt;path&gt;/var/lib/libvirt/images/GlusterFS&lt;/path&gt; &lt;permissions&gt; &lt;mode&gt;0755&lt;/mode&gt; &lt;owner&gt;107&lt;/owner&gt; &lt;group&gt;107&lt;/group&gt; &lt;label&gt;system_u:object_r:virt_image_t:s0&lt;/label&gt; &lt;/permissions&gt; &lt;/target&gt;&lt;/pool&gt; 123virsh pool-define --file gluster.xmlvirsh pool-start --pool glusterfs-poolvirsh pool-autostart --pool glusterfs-pool 将Gluster挂在为目录，可以避免Libvirt在使用Gluster的两个问题： We can use Gluster’s failover capability, which will be managed automatically by the Gluster utilities that we installed directly, as libvirt doesn’t support them yet. We will avoid creating virtual machine disks manually, which is another limitation of libvirt’s implementation of Gluster support, while directory-based storage pools support it without any issues. CephCeph offers block and object-based storage. Object-based storage for block-based devices means direct, binary storage, directly to a LUN. There are no filesystems involved, which theoretically means less overhead as there’s no filesystem, filesystem tables, and other constructs that might slow the I/O process down. Architecture-wise, Ceph has three main services: ceph-mon : Used for cluster monitoring, CRUSH maps, and Object Storage Daemon (OSD) maps. ceph-osd: This handles actual data storage, replication, and recovery. It requires at least two nodes; we’ll use three for clustering reasons. ceph-mds: Metadata server, used when Ceph needs filesystem access. Ceph 集群中，所有的数据节点要求采用相同的硬件配置，相同的CPU、内存、硬盘，不适用RAID控制器，仅仅使用HBA。 1234567891011ceph-deploy install ceph-admin ceph-monitor ceph-osd1 ceph-osd2ceph-osd3ceph-deploy mon create-initialceph-deploy gatherkeys ceph-monitorceph-deploy disk list ceph-osd1 ceph-osd2 ceph-osd3ceph-deploy disk zap ceph-osd1:/dev/sdb ceph-osd2:/dev/sdbceph-osd3:/dev/sdbceph-deploy osd prepare ceph-osd1:/dev/sdb ceph-osd2:/dev/sdbceph-osd3:/dev/sdbceph-deploy osd activate ceph-osd1:/dev/sdb1 ceph-osd2:/dev/sdb1 ceph-osd3:/dev/sdb1 对上述命令的说明： The first command starts the actual deployment process—for the admin, monitor, and OSD nodes, with the installation of all the necessary packages. The second and third commands configure the monitor host so that it’s ready to accept external connections. The two disk commands are all about disk preparation—Ceph will clear the disks that we assigned to it (/dev/sdb per OSD host) and create two partitions on them, one for Ceph data and one for the Ceph journal. The last two commands prepare these filesystems for use and activate Ceph. If at any time your ceph-deploy script stops, check your DNS and /etc/hosts and firewalld configuration, as that’s where the problems usually are. 使用以下命令将Ceph存储发布给KVM主机作为存储池： 1ceph osd pool create KVMpool 128 128 还需要几个步骤来实现安全的访问，为Ceph的访问增加密码验证。 Ceph生成验证的密码 Key 123456ceph auth get-or-create client.KVMpool mon &#x27;allow r&#x27; osd &#x27;allowrwx pool=KVMpool&#x27;#It&#x27;s going to throw us a status message, something like this:key = AQB9p8RdqS09CBAA1DHsiZJbehb7ZBffhfmFJQ== 定义一个Secret 123456&lt;secret ephemeral=&#x27;no&#x27; private=&#x27;no&#x27;&gt; &lt;usage type=&#x27;ceph&#x27;&gt; &lt;name&gt;client.KVMpool secret&lt;/name&gt; &lt;/usage&gt;&lt;/secret&gt; 将Secret的UUID与Ceph生成的Key进行关联。 123456virsh secret-define --file secret.xmlSecret 95b1ed29-16aa-4e95-9917-c2cd4f3b2791 createdvirsh secret-set-value 95b1ed29-16aa-4e95-9917-c2cd4f3b2791AQB9p8RdqS09CBAA1DHsiZJbehb7ZBffhfmFJQ== 定义Ceph存储池 123456789&lt;pool type=&quot;rbd&quot;&gt; &lt;source&gt; &lt;name&gt;KVMpool&lt;/name&gt; &lt;host name=&#x27;192.168.159.151&#x27; port=&#x27;6789&#x27;/&gt; &lt;auth username=&#x27;KVMpool&#x27; type=&#x27;ceph&#x27;&gt; &lt;secret uuid=&#x27;95b1ed29-16aa-4e95-9917-c2cd4f3b2791&#x27;/&gt; &lt;/auth&gt; &lt;/source&gt;&lt;/pool&gt; 1234virsh pool-define --file ceph.xmlvirsh pool-start KVMpoolvirsh pool-autostart KVMpoolvirsh pool-list --details 虚拟磁盘镜像和KVM存储基本操作 文章介绍了使用dd命令生成磁盘镜像文件，如下： 生成10G的预分配磁盘文件： 1dd if=/dev/zero of=/vms/dbvm_disk2.img bs=1G count=10 生成10G的磁盘文件，使用seek关键字，不做预分配 1dd if=/dev/zero of=/vms/dbvm_disk2_seek.imgbs=1G seek=10 count=0 qemu-img 命令使用qemu-img info查看磁盘文件的信息，如下： 12345678910# qemu-img info /vms/dbvm_disk2.imgimage: /vms/dbvm_disk2.imgfile format: rawvirtual size: 10G (10737418240 bytes)disk size: 10G# qemu-img info /vms/dbvm_disk2_seek.imgimage: /vms/dbvm_disk2_seek.imgfile format: rawvirtual size: 10G (10737418240 bytes)disk size: 10M 使用virsh命令加载硬盘1virsh attach-disk CentOS8 /vms/dbvm_disk2.img vdb --live --config 说明如下： CentOS8 is the virtual machine to which a disk attachment is executed. Then, there is the path of the disk image, /vms/dbvm_disk2.img vdb is the target disk name that would be visible inside the guest operating system. –live means performing the action while the virtual machine is running. –config means attaching it persistently across reboot. Not adding a –config switch will keep the disk attached only until reboot. 查看虚拟机的硬盘信息： 123456virsh domblklist CentOS8 --detailsType Device Target Source------------------------------------------------file disk vda /var/lib/libvirt/images/fedora21.qcow2file disk vdb /vms/dbvm_disk2_seek.img 还有其他章节，比如创建ISO库、删除存储池、创建卷和删除卷。 有关NVMe和NVMe over FabricSSD存储的存取方式发生改变，主要体现在性能和延时这两方面。传统AHCI (Advanced Host Controller Interface) 已不能满足SSD的性能要求。 NVMe (Non-Volatile Memory Express) 是一个全新的协议，基于PCIe技术，目的在于满足SSD的高性能、低时延的访问要求。越来越多的存储设备已经集成了SSD和NVMe，主要用于缓存，同时也有用于数据存储的。 Fiber Channel，虽然10多年被人争执说要从市场消失，主要原因无外乎：FC需要专用的交换机、专门的网卡和线缆，FC设备贵，需要专门的知识，而且FC的速率没有以太网发展快。 但是，目前市场上发生的情况有所不同，FC可以满足NVMe SSD带来的性能提升的需求。 Two of these concepts derived from Intel Optane—Storage Class Memory (SCM) and Persistent Memory (PM) are the latest technologies that storage companies and customers want adopted into their storage systems, and fast. 将工作负载移到内存当中是符合逻辑的，设想一下内存型数据库(Microsoft SQL, SAP HANA, Oracle). 如果一个存储厂商生产了使用SCM SSD的存储设备，那么只有内存可用于缓存(Cache).","categories":[],"tags":[{"name":"kvm","slug":"kvm","permalink":"https://weiborao.link/tags/kvm/"}]},{"title":"Cisco NFVIS CSR1Kv ZTP Onboarding for SDWAN","slug":"csr1kv-nfvis-ztp-demo","date":"2021-02-08T03:02:43.000Z","updated":"2021-09-17T06:27:54.266Z","comments":true,"path":"csr1kv-nfvis-ztp-demo.html","link":"","permalink":"https://weiborao.link/csr1kv-nfvis-ztp-demo.html","excerpt":"","text":"","categories":[],"tags":[{"name":"ztp","slug":"ztp","permalink":"https://weiborao.link/tags/ztp/"},{"name":"nfvis","slug":"nfvis","permalink":"https://weiborao.link/tags/nfvis/"}]},{"title":"Cisco CSR 1000v & Catalyst 8000v KVM SRIOV安装设置演示","slug":"csr1kv-kvm-sriov-ubuntu-demo","date":"2021-02-08T02:36:08.000Z","updated":"2021-09-17T06:29:03.172Z","comments":true,"path":"csr1kv-kvm-sriov-ubuntu-demo.html","link":"","permalink":"https://weiborao.link/csr1kv-kvm-sriov-ubuntu-demo.html","excerpt":"","text":"鸣谢：Johnny Rong Hexo博客中添加B站视频播放器","categories":[],"tags":[{"name":"sriov","slug":"sriov","permalink":"https://weiborao.link/tags/sriov/"},{"name":"csr1000v","slug":"csr1000v","permalink":"https://weiborao.link/tags/csr1000v/"},{"name":"ztp","slug":"ztp","permalink":"https://weiborao.link/tags/ztp/"}]},{"title":"Hexo 安装设置遇到的问题及解决办法","slug":"hexo-setup-log-md","date":"2021-02-07T15:36:19.000Z","updated":"2021-02-08T15:43:30.966Z","comments":true,"path":"hexo-setup-log-md.html","link":"","permalink":"https://weiborao.link/hexo-setup-log-md.html","excerpt":"","text":"遇到的问题正常按照brew install node 以及 npm install -g hexo-cli 安装完以后，运行hexo deploy报错，如下： 12345678910➜ blog hexo dINFO Validating configINFO Deploying: gitINFO Clearing .deploy_git folder...INFO Copying files from public folder...FATAL &#123; err: TypeError [ERR_INVALID_ARG_TYPE]: The &quot;mode&quot; argument must be integer. Received an instance of Object at copyFile (node:fs:2019:10) at tryCatcher (/Users/werao/blog/node_modules/bluebird/js/release/util.js:16:23) at ret (eval at makeNodePromisifiedEval (/usr/local/lib/node_modules/hexo-cli/node_modules/bluebird/js/release/promisify.js:184:12), &lt;anonymous&gt;:13:39) 网上查找了一下，报错的原因是Hexo与Node.js的兼容性问题。 解决办法非常幸运，找到一篇文章：重装Hexo遇到的坑 作者 成百川。 简要记录步骤如下： 卸载nodebrew uninstall node 安装nvmbrew install nvm记得按提示修改**~/.zshrc文件，否则下次启动会找不到nvm**。 使用nvm安装低于14.0版本的node.js–如果后续版本升级，还可以使用nvm安装新版本。nvm install v13.14.0 12Now using node v13.14.0 (npm v6.14.4)Creating default alias: default -&gt; v13.14.0 重新运行hexo deploy -g 发布本文。 鸣谢成百川。","categories":[],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://weiborao.link/tags/hexo/"}]},{"title":"Cisco CSR1000v KVM SRIOV Setting Guide on Ubuntu 20.04 With NetworkManager","slug":"csr1kv-kvm-Ubuntu20-04-md","date":"2021-02-03T14:37:33.000Z","updated":"2021-02-08T03:38:30.670Z","comments":true,"path":"csr1kv-kvm-Ubuntu20-04-md.html","link":"","permalink":"https://weiborao.link/csr1kv-kvm-Ubuntu20-04-md.html","excerpt":"","text":"Last Update: February 3, 2021 Author: Rao Weibo Version: 1.0 This guide provides the steps on how to install CSR1000v/Catalyst 8000v on KVM, the setting of SRIOV and KVM tuning. Ubuntu 20.04 Installation and Settings(1) BIOS settings Configuration Recommended Setting Intel Hyper-Threading Technology Disabled Number of Enable Cores ALL Execute Disable Enabled Intel VT Enabled Intel VT-D Enabled Intel VT-D coherency support Enabled Intel VT-D ATS support Enabled CPU Performance High throughput Hardware Perfetcher Disabled Adjacent Cache Line Prefetcher Disabled DCU Streamer Prefetch Disable Power Technology Custom Enhanced Intel Speedstep Technology Disabled Intel Turbo Boost Technology Enabled Processor Power State C6 Disabled Processor Power State C1 Enhanced Disabled Frequency Poor Override Enabled P-State Coordination HW_ALL Energy Performance Performance The above settings are from the Installation guide from Cisco CSR 1000v and Cisco ISRv Software Configuration Guide.Note: some platform such as Cisco NFVIS, does not disable the Hyper-Threading. (2) Ubuntu 20.04 installation tipsIf you need the Gnome GUI, you can install the Ubuntu 20.04 Desktop.After the system bootup, ‘apparmor’ is recommended to be disabled, otherwise, you may meet permission denied when you assign SRIOV devices to the CSR1kV. You need to reboot to take effect. 12sudo systemctl disable apparmoregrep -o &#x27;(vmx|svm)&#x27; /proc/cpuinfo | sort | uniq You can find the vmx on Intel platform, or svm on AMD platform. 12sudo systemctl stop ufwsystemctl disable ufw cat /etc/sysctl.conf 1234net.ipv4.ip_forward = 1net.bridge.bridge-nf-call-ip6tables = 0net.bridge.bridge-nf-call-iptables = 0net.bridge.bridge-nf-call-arptables = 0 1$ sudo apt install qemu-kvm libvirt-clients libvirt-daemon-system bridge-utils virt-manager numactl virt-top After installation, you can check the version 123456789101112ubuntu@ubuntu-kvm:~$ libvirtd -Vlibvirtd (libvirt) 6.0.0ubuntu@ubuntu-kvm:~$ qemu-system-x86_64 --versionQEMU emulator version 4.2.1 (Debian 1:4.2-3ubuntu6.11)Copyright (c) 2003-2019 Fabrice Bellard and the QEMU Project developersubuntu@ubuntu-kvm:~$ virt-manager --version2.2.1ubuntu@ubuntu-kvm:~$ modinfo kvm-intelfilename: /lib/modules/5.4.0-62-generic/kernel/arch/x86/kvm/kvm-intel.koubuntu@ubuntu-kvm:~$ modinfo i40evffilename: /lib/modules/5.4.0-62-generic/kernel/drivers/net/ethernet/intel/iavf/iavf.koversion: 3.2.3-k (3) NIC SettingIn Ubuntu 20.04, we use NetworkManager to config the NIC, you can use nmtui on the terminal, or nmcli.First, check the netplan. cat /etc/netplan/ 00-installer-config.yaml Edit the yaml file: 1234# Let NetworkManager manage all devices on this systemnetwork: version: 2 renderer: NetworkManager 1sudo netplan apply Then you can set the network config with nmcli or nmtui. 123456789101112131415161718192021222324252627282930313233ubuntu@ubuntu-kvm:~$ sudo nmcli connection modify netplan-eno1 ipv4.method manual ipv4.addresses 10.75.59.50/24 ipv4.gateway 10.75.59.1 ipv4.dns 64.104.123.245ubuntu@ubuntu-kvm:~$ sudo nmcli connection modify netplan-eno1 connection.id eno1ubuntu@ubuntu-kvm:~$ sudo nmcli connection up eno1# Change the connections name to the devices name.sudo nmcli connectionubuntu@ubuntu-kvm:~$ sudo nmcli connectionNAME UUID TYPE DEVICEeno1 10838d80-caeb-349e-ba73-08ed16d4d666 ethernet eno1enp216s0f0 6556c191-c253-3a5e-b440-c5b071ec29a4 ethernet enp216s0f0enp216s0f1 8080672c-5784-375f-8eb9-a6ef57cbd4f7 ethernet enp216s0f1ens1f0 58fb5b1f-c10a-3e7a-9ab9-a8c449840ce6 ethernet ens1f0ens1f1 2a06a9d9-b761-3bdf-aa0b-3d44fff2158f ethernet ens1f1virbr0 ca7d1e11-a82f-429c-9d91-fc985776232c bridge virbr0#Set the MTU and disable ipv4 for the NIC to enable SRIOV. sudo nmcli connection modify ens1f0 ethernet.mtu 9216sudo nmcli connection modify ens1f0 ipv4.method disabledsudo nmcli connection up ens1f0sudo ip link show ens1f02: ens1f0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9216 qdisc mq state UP mode DEFAULT group default qlen 1000link/ether 40:a6:b7:0f:a7:74 brd ff:ff:ff:ff:ff:ff# Note: This value 9216 of MTU is derived from Cisco NFVIS.CSP5228-1# show pnic-detail mtuName MTU=============================eth0-1 9216eth0-2 9216eth1-1 9216eth1-2 9216 SR-IOV Configuration(1) Check the NIC to support SR-IOVCheck the NIC hardware infor. 1234567891011sudo lshw -c network -businfoBus info Device Class Description========================================================pci@0000:3b:00.0 eno1 network Ethernet Controller 10G X550Tpci@0000:3b:00.1 eno2 network Ethernet Controller 10G X550Tpci@0000:5e:00.0 ens1f0 network Ethernet Controller XXV710 for 25GbE SFP28pci@0000:5e:00.1 ens1f1 network Ethernet Controller XXV710 for 25GbE SFP28pci@0000:d8:00.0 enp216s0f0 network Ethernet Controller XXV710 for 25GbE SFP28pci@0000:d8:00.1 enp216s0f1 network Ethernet Controller XXV710 for 25GbE SFP28 Check the capabilities of NIC 1234567sudo lspci -vv -s 5e:00.0 | grep -A 5 -i SR-IOV Capabilities: [160 v1] Single Root I/O Virtualization (SR-IOV) IOVCap: Migration-, Interrupt Message Number: 000 IOVCtl: Enable+ Migration- Interrupt- MSE+ ARIHierarchy+ IOVSta: Migration- Initial VFs: 64, Total VFs: 64, Number of VFs: 2, Function Dependency Link: 00 VF offset: 16, stride: 1, Device ID: 154c (2) Change the GRUB parameters12sudo vi /etc/default/grub GRUB_CMDLINE_LINUX_DEFAULT=&quot;quiet hugepagesz=1G hugepages=64 default_hugepagesz=1G intel_iommu=on iommu=pt isolcpus=1-8,45-52&quot; Note：The hugepages should be no more than the physical memory, otherwise, it will fail to bootup. default_hugepagesz=1G hugepagesz=1G hugepages=64 will allocate 64 1GB huge pages at boot, which are static huge pages. CSR 1000v will use these static huge pages for best performance. isolcpus=1-8,45-52 will isolate the CPUs cores reserved for CSR 1000v, prevent other processes running on these cores, this can reduce latency for CSR 1000v. You can refer to [Check the capability of the platform][1] to get the CPU information. 1sudo update-grub After reboot, check the /proc/cmdline. 123456789cat /proc/cmdline |grep intel_iommu=ondmesg |grep -e DMAR -e IOMMUdmesg | grep -e DMAR -e IOMMU -e AMD-Viubuntu@ubuntu-kvm:~$ cat /proc/cmdline |grep intel_iommu=onBOOT_IMAGE=/vmlinuz-5.4.0-62-generic root=/dev/mapper/ubuntu--vg-ubuntu--lv ro quiet hugepagesz=1G hugepages=64 default_hugepagesz=1G intel_iommu=on iommu=pt isolcpus=1-8,45-52ubuntu@ubuntu-kvm:~ $ dmesg |grep -e DMAR -e IOMMU[ 0.020313] ACPI: DMAR 0x000000005DA8EB80 000270 (v01 Cisco0 CiscoUCS 00000001 INTL 20091013)[ 1.954441] DMAR: IOMMU enabled Check the CPU info, all the CPU cores and isolated CPU cores. 1234ubuntu@ubuntu-kvm:~$ cat /sys/devices/system/cpu/isolated1-8,45-52ubuntu@ubuntu-kvm:~$ cat /sys/devices/system/cpu/present0-87 (3) VFs PersistenceNetworkManager way: 123456789# nmcli can set the SRIOV, as follow:sudo nmcli connection modify ens1f0 sriov.total-vfs 2sudo nmcli connection modify ens1f1 sriov.total-vfs 2# We can set the MAC address and set the trust on:sudo nmcli connection modify ens1f0 sriov.vfs &#x27;0 mac=b6:4f:02:37:5a:d8 trust=true, 1 mac=26:04:1d:1f:3d:a9 trust=true&#x27;sudo nmcli connection modify ens1f1 sriov.vfs &#x27;0 mac=76:6c:4e:16:7f:e2 trust=true, 1 mac=6a:f2:bd:97:71:65 trust=true&#x27;sudo nmcli connection up ens1f0sudo nmcli connection up ens1f1 After reboot, check the dmesg. 1234567891011121314151617sudo dmesg | grep -i vf[ 6.599304] i40e 0000:5e:00.0: Allocating 2 VFs.[ 6.680111] i40e 0000:5e:00.1: Allocating 2 VFs.[ 6.730167] iavf: Intel(R) Ethernet Adaptive Virtual Function Network Driver - version 3.2.3-k&lt;&lt; snip &gt;&gt;[ 17.931493] i40e 0000:5e:00.0: Setting MAC b6:4f:02:37:5a:d8 on VF 0[ 18.111781] i40e 0000:5e:00.0: VF 0 is now trusted[ 18.112559] i40e 0000:5e:00.0: Setting MAC 26:04:1d:1f:3d:a9 on VF 1[ 18.291464] i40e 0000:5e:00.0: VF 1 is now trusted[ 18.292231] i40e 0000:5e:00.1: Setting MAC 76:6c:4e:16:7f:e2 on VF 0[ 18.475259] i40e 0000:5e:00.1: VF 0 is now trusted[ 18.475929] i40e 0000:5e:00.1: Setting MAC 6a:f2:bd:97:71:65 on VF 1[ 18.659465] i40e 0000:5e:00.1: VF 1 is now trusted[ 18.728124] iavf 0000:5e:02.0 ens1f0v0: NIC Link is Up Speed is 25 Gbps Full Duplex[ 18.752275] iavf 0000:5e:02.1 ens1f0v1: NIC Link is Up Speed is 25 Gbps Full Duplex[ 18.776329] iavf 0000:5e:0a.0 ens1f1v0: NIC Link is Up Speed is 25 Gbps Full Duplex[ 18.800569] iavf 0000:5e:0a.1 ens1f1v1: NIC Link is Up Speed is 25 Gbps Full Duplex (4) Check the VFsYou can check the VFs from lspci or ip link commands. 12ubuntu@ubuntu-kvm:~$ sudo lspci | grep -i Virtualubuntu@ubuntu-kvm:~$ sudo ip link show | grep -B2 vf Good news is that the VFs names are well related to the physical NICs. For example, ens1f0v1 is one of the VFs of the physical NIC ens1f0 . 123456789ubuntu@ubuntu-kvm:~$ ip link show | grep -E ens1f[0,1]v[0,1] -A 19: ens1f0v1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000 link/ether 26:04:1d:1f:3d:a9 brd ff:ff:ff:ff:ff:ff10: ens1f1v1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000 link/ether 6a:f2:bd:97:71:65 brd ff:ff:ff:ff:ff:ff15: ens1f0v0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000 link/ether b6:4f:02:37:5a:d8 brd ff:ff:ff:ff:ff:ff16: ens1f1v0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000link/ether 76:6c:4e:16:7f:e2 brd ff:ff:ff:ff:ff:ff We can change the MTU to 9216 (or you may not need 9216, 1504 is enough). 12345678sudo nmcli connection modify ens1f0v0 ethernet.mtu 9216 ipv4.method disabledsudo nmcli connection modify ens1f0v1 ethernet.mtu 9216 ipv4.method disabledsudo nmcli connection modify ens1f1v0 ethernet.mtu 9216 ipv4.method disabledsudo nmcli connection modify ens1f1v1 ethernet.mtu 9216 ipv4.method disabledsudo nmcli connection up ens1f0v0 sudo nmcli connection up ens1f0v1sudo nmcli connection up ens1f1v0sudo nmcli connection up ens1f1v1 The above configs will survive after reboot. Create SR-IOV Virtual Network Adapter PoolUsing this method, KVM creates a pool of network devices that can be inserted into VMs, and the size of that pool is determined by how many VFs were created on the physical function when they were initialized. (1) Create an xml fileubuntu@ubuntu-kvm:~/kvm$ cat ens1f0_sriov_pool.xml 123456&lt;network&gt; &lt;name&gt;ens1f0_sriov_pool&lt;/name&gt; &lt;!-- This is the name of the file you created --&gt; &lt;forward mode=&#x27;hostdev&#x27; managed=&#x27;yes&#x27;&gt; &lt;pf dev=&#x27;ens1f0&#x27;/&gt; &lt;!-- Use the netdev name of your SR-IOV devices PF here --&gt; &lt;/forward&gt;&lt;/network&gt; (2) Define a network and set to auto start.1234virsh net-define ens1f0_sriov_pool.xmlSleep 1virsh net-start ens1f0_sriov_poolvirsh net-autostart ens1f0_sriov_pool ubuntu@ubuntu-kvm:~/kvm$ virsh net-dumpxml ens1f0_sriov_pool 123456789&lt;network connections=&#x27;1&#x27;&gt; &lt;name&gt;ens1f0_sriov_pool&lt;/name&gt; &lt;uuid&gt;9125a600-6620-4ab6-9a47-8844a61b0918&lt;/uuid&gt; &lt;forward mode=&#x27;hostdev&#x27; managed=&#x27;yes&#x27;&gt; &lt;pf dev=&#x27;ens1f0&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x5e&#x27; slot=&#x27;0x02&#x27; function=&#x27;0x0&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x5e&#x27; slot=&#x27;0x02&#x27; function=&#x27;0x1&#x27;/&gt; &lt;/forward&gt;&lt;/network&gt; (3) Select the virtual adapter from the poolIt is simple to add an SR-IOV NIC, as follow: This is equivalent to the following XML: 123456&lt;interface type=&#x27;network&#x27;&gt; &lt;mac address=&#x27;52:54:00:0b:a5:2e&#x27;/&gt; &lt;source network=&#x27;ens1f0_sriov_pool&#x27;/&gt; &lt;model type=&#x27;virtio&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x02&#x27; slot=&#x27;0x00&#x27; function=&#x27;0x0&#x27;/&gt;&lt;/interface&gt; After the VM powered on, the network info is as follow:virsh dumpxml CSR1KV-1 12345678910&lt;interface type=&#x27;hostdev&#x27; managed=&#x27;yes&#x27;&gt; &lt;mac address=&#x27;52:54:00:0b:a5:2e&#x27;/&gt; &lt;driver name=&#x27;vfio&#x27;/&gt; &lt;source&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x5e&#x27; slot=&#x27;0x02&#x27; function=&#x27;0x0&#x27;/&gt; &lt;/source&gt; &lt;model type=&#x27;virtio&#x27;/&gt; &lt;alias name=&#x27;hostdev0&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x02&#x27; slot=&#x27;0x00&#x27; function=&#x27;0x0&#x27;/&gt;&lt;/interface&gt; Create the CSR1KV VM from virt-managerFrom the virt-manager, create a CSR1KV virtual machine step by step, and choose the virtual network interface from the SR-IOV pool. Note: The first interface of csr1kv-1 is configured to macvtap Bridge mode, so you do not need to create a Linux bridge. However, csr1kv-1 can not communicate to the Linux host through this interface, but it can go out of the Linux host through the eno1. This is a known issue with macvtap. After select the Virtual Network Interface, click the Begin Installation, and you can shut down the virtual machine.The actions above will create the csr1kv-1.xml file under the directory: /etc/libvirtd/qemu/ KVM Performance TunningKVM performance tuning are related to NUMA, Memory Hugepage and vCPU pinning. The main reference is Redhat Linux 7 PERFORMANCE TUNING GUIDE We will use virsh edit csr1kv-1 to do the performance tuning. (1) Check the capability of the platform123456789ubuntu@ubuntu-kvm:~$ virsh nodeinfoCPU model: x86_64CPU(s): 88CPU frequency: 1000 MHzCPU socket(s): 1Core(s) per socket: 22Thread(s) per core: 2NUMA cell(s): 2Memory size: 394929928 KiB [1]:ubuntu@ubuntu-kvm:~$ virsh capabilities 123456789101112&lt;capabilities&gt; &lt;host&gt; &lt;uuid&gt;a24f9760-48f1-f34e-a001-a848f08df7bb&lt;/uuid&gt; &lt;cpu&gt; &lt;arch&gt;x86_64&lt;/arch&gt; &lt;model&gt;Cascadelake-Server-noTSX&lt;/model&gt; &lt;vendor&gt;Intel&lt;/vendor&gt; &lt;microcode version=&#x27;83898371&#x27;/&gt; &lt;counter name=&#x27;tsc&#x27; frequency=&#x27;2095078000&#x27; scaling=&#x27;no&#x27;/&gt; &lt;topology sockets=&#x27;1&#x27; cores=&#x27;22&#x27; threads=&#x27;2&#x27;/&gt;... From the outputs, we can get the cores of CPU, the NUMA info and the hugepages. (2) NUMA Info123456789101112ubuntu@ubuntu-kvm:~$ numactl --hardwareavailable: 2 nodes (0-1)node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65node 0 size: 192172 MBnode 0 free: 152956 MBnode 1 cpus: 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87node 1 size: 193500 MBnode 1 free: 158037 MBnode distances:node 0 1 0: 10 21 1: 21 10 Two CPUs, each has 192GB memory, the NUMA node are node 0 and node 1. You can also check the NUMA info of the NIC, as follow: 12345678910111213ubuntu@ubuntu-kvm:~$ sudo lspci -vv | grep Ethernet -A 65e:00.0 Ethernet controller: Intel Corporation Ethernet Controller XXV710 for 25GbE SFP28 (rev 02) Subsystem: Cisco Systems Inc Ethernet Network Adapter XXV710 Control: I/O- Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr+ Stepping- SERR+ FastB2B- DisINTx+ Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast &gt;TAbort- &lt;TAbort- &lt;MAbort- &gt;SERR- &lt;PERR- INTx- Latency: 0, Cache Line Size: 32 bytes Interrupt: pin A routed to IRQ 137 NUMA node: 0#Or you can find the numa infor from udevadm.ubuntu@ubuntu-kvm:~$ sudo udevadm info -ap /sys/class/net/ens1f0 | grep numa ATTRS&#123;numa_node&#125;==&quot;0&quot; ATTRS&#123;numa_node&#125;==&quot;0&quot; If the memory and NICs are all on numa_node 0, that would be more efficient. Please check the server’s PCI-E slots mapping with the CPU slots. (3) HugePage and Transparent HugepageTo check the memory info 12345678910ubuntu@ubuntu-kvm:~$ cat /proc/meminfo | grep HugeAnonHugePages: 133120 kBShmemHugePages: 0 kBFileHugePages: 0 kBHugePages_Total: 64HugePages_Free: 56HugePages_Rsvd: 0HugePages_Surp: 0Hugepagesize: 1048576 kBHugetlb: 67108864 kB You can change the hugepages on the run time： 12345ubuntu@ubuntu-kvm:~$ cat /sys/devices/system/node/node0/hugepages/hugepages-1048576kB/nr_hugepages32sudo su - // switch to root user.root@ubuntu-kvm:~# echo 64 &gt; /sys/devices/system/node/node0/hugepages/hugepages-1048576kB/nr_hugepagesroot@ubuntu-kvm:~# echo 64 &gt; /sys/devices/system/node/node1/hugepages/hugepages-1048576kB/nr_hugepages Check and configure the transparent_hugepage: 123root@ubuntu-kvm:~# echo always &gt; /sys/kernel/mm/transparent_hugepage/enabledcat /sys/kernel/mm/transparent_hugepage/enabled[always] madvise never (4) vCPU PinningvCPU pinning can improve the cache meet rate and improve the performance.You can check the vcpu pinning.virsh vcpuinfo csr1kv-1 (5) Edit the XML file of the CSR1KVRun virsh edit csr1kv-1 to edit the parameters.Please pay attention to the following texts, other parameters might be different. 123456789101112131415161718192021222324&lt;memoryBacking&gt; &lt;hugepages&gt; &lt;page size=&#x27;1048576&#x27; unit=&#x27;KiB&#x27;/&gt; &lt;/hugepages&gt; &lt;locked/&gt; &lt;nosharepages/&gt;&lt;/memoryBacking&gt;&lt;vcpu placement=&#x27;static&#x27;&gt;8&lt;/vcpu&gt;&lt;cputune&gt; &lt;vcpupin vcpu=&#x27;0&#x27; cpuset=&#x27;1&#x27;/&gt; &lt;vcpupin vcpu=&#x27;1&#x27; cpuset=&#x27;2&#x27;/&gt; &lt;vcpupin vcpu=&#x27;2&#x27; cpuset=&#x27;3&#x27;/&gt; &lt;vcpupin vcpu=&#x27;3&#x27; cpuset=&#x27;4&#x27;/&gt; &lt;vcpupin vcpu=&#x27;4&#x27; cpuset=&#x27;5&#x27;/&gt; &lt;vcpupin vcpu=&#x27;5&#x27; cpuset=&#x27;6&#x27;/&gt; &lt;vcpupin vcpu=&#x27;6&#x27; cpuset=&#x27;7&#x27;/&gt; &lt;vcpupin vcpu=&#x27;7&#x27; cpuset=&#x27;8&#x27;/&gt; &lt;emulatorpin cpuset=&#x27;45-52&#x27;/&gt; &lt;!-- If Hyper-threading were enabled --&gt;&lt;/cputune&gt;&lt;numatune&gt; &lt;memory mode=&#x27;strict&#x27; nodeset=&#x27;0&#x27;/&gt;&lt;/numatune&gt;&lt;cpu mode=&#x27;host-passthrough&#x27; check=&#x27;none&#x27;/&gt; &lt;memballoon model=&#x27;none&#x27;/&gt; Please note that, if hyper-threading is enabled in BIOS setting, the parameter “emulatorpin” should be set, the cpuset are from the “virsh capabilities”, for example, siblings=’1,45’. When the core 1 is pinned, core 45 should be set in emulatorpin. From the guide https://libvirt.org/formatdomain.html and https://libvirt.org/kbase/kvm-realtime.html we set the CPU and memory tuning parameters as the highlighted text. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165&lt;domain type=&#x27;kvm&#x27;&gt; &lt;name&gt;csr1kv-1&lt;/name&gt; &lt;uuid&gt;83f90c1c-f0ea-4298-bcdf-3d676de2aeb4&lt;/uuid&gt; &lt;metadata&gt; &lt;libosinfo:libosinfo xmlns:libosinfo=&quot;http://libosinfo.org/xmlns/libvirt/domain/1.0&quot;&gt; &lt;libosinfo:os id=&quot;http://centos.org/centos/7.0&quot;/&gt; &lt;/libosinfo:libosinfo&gt; &lt;/metadata&gt; &lt;memory unit=&#x27;KiB&#x27;&gt;8388608&lt;/memory&gt; &lt;currentMemory unit=&#x27;KiB&#x27;&gt;8388608&lt;/currentMemory&gt; &lt;memoryBacking&gt; &lt;hugepages&gt; &lt;page size=&#x27;1048576&#x27; unit=&#x27;KiB&#x27;/&gt; &lt;/hugepages&gt; &lt;locked/&gt; &lt;nosharepages/&gt; &lt;/memoryBacking&gt; &lt;vcpu placement=&#x27;static&#x27;&gt;8&lt;/vcpu&gt; &lt;cputune&gt; &lt;vcpupin vcpu=&#x27;0&#x27; cpuset=&#x27;1&#x27;/&gt; &lt;vcpupin vcpu=&#x27;1&#x27; cpuset=&#x27;2&#x27;/&gt; &lt;vcpupin vcpu=&#x27;2&#x27; cpuset=&#x27;3&#x27;/&gt; &lt;vcpupin vcpu=&#x27;3&#x27; cpuset=&#x27;4&#x27;/&gt; &lt;vcpupin vcpu=&#x27;4&#x27; cpuset=&#x27;5&#x27;/&gt; &lt;vcpupin vcpu=&#x27;5&#x27; cpuset=&#x27;6&#x27;/&gt; &lt;vcpupin vcpu=&#x27;6&#x27; cpuset=&#x27;7&#x27;/&gt; &lt;vcpupin vcpu=&#x27;7&#x27; cpuset=&#x27;8&#x27;/&gt; &lt;emulatorpin cpuset=&#x27;45-52&#x27;/&gt; &lt;!-- If Hyper-threading were enabled --&gt; &lt;/cputune&gt; &lt;numatune&gt; &lt;memory mode=&#x27;strict&#x27; nodeset=&#x27;0&#x27;/&gt; &lt;/numatune&gt; &lt;os&gt; &lt;type arch=&#x27;x86_64&#x27; machine=&#x27;pc-q35-4.2&#x27;&gt;hvm&lt;/type&gt; &lt;boot dev=&#x27;hd&#x27;/&gt; &lt;/os&gt; &lt;features&gt; &lt;acpi/&gt; &lt;apic/&gt; &lt;vmport state=&#x27;off&#x27;/&gt; &lt;/features&gt; &lt;cpu mode=&#x27;host-passthrough&#x27; check=&#x27;none&#x27;/&gt; &lt;clock offset=&#x27;utc&#x27;&gt; &lt;timer name=&#x27;rtc&#x27; tickpolicy=&#x27;catchup&#x27;/&gt; &lt;timer name=&#x27;pit&#x27; tickpolicy=&#x27;delay&#x27;/&gt; &lt;timer name=&#x27;hpet&#x27; present=&#x27;no&#x27;/&gt; &lt;/clock&gt; &lt;on_poweroff&gt;destroy&lt;/on_poweroff&gt; &lt;on_reboot&gt;restart&lt;/on_reboot&gt; &lt;on_crash&gt;destroy&lt;/on_crash&gt; &lt;pm&gt; &lt;suspend-to-mem enabled=&#x27;no&#x27;/&gt; &lt;suspend-to-disk enabled=&#x27;no&#x27;/&gt; &lt;/pm&gt; &lt;devices&gt; &lt;emulator&gt;/usr/bin/qemu-system-x86_64&lt;/emulator&gt; &lt;disk type=&#x27;file&#x27; device=&#x27;disk&#x27;&gt; &lt;driver name=&#x27;qemu&#x27; type=&#x27;qcow2&#x27;/&gt; &lt;source file=&#x27;/var/lib/libvirt/images/csr1000v-universalk9.17.02.01v.qcow2&#x27;/&gt; &lt;target dev=&#x27;vda&#x27; bus=&#x27;virtio&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x05&#x27; slot=&#x27;0x00&#x27; function=&#x27;0x0&#x27;/&gt; &lt;/disk&gt; &lt;controller type=&#x27;usb&#x27; index=&#x27;0&#x27; model=&#x27;qemu-xhci&#x27; ports=&#x27;15&#x27;&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x03&#x27; slot=&#x27;0x00&#x27; function=&#x27;0x0&#x27;/&gt; &lt;/controller&gt; &lt;controller type=&#x27;sata&#x27; index=&#x27;0&#x27;&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x1f&#x27; function=&#x27;0x2&#x27;/&gt; &lt;/controller&gt; &lt;controller type=&#x27;pci&#x27; index=&#x27;0&#x27; model=&#x27;pcie-root&#x27;/&gt; &lt;controller type=&#x27;pci&#x27; index=&#x27;1&#x27; model=&#x27;pcie-root-port&#x27;&gt; &lt;model name=&#x27;pcie-root-port&#x27;/&gt; &lt;target chassis=&#x27;1&#x27; port=&#x27;0x10&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x02&#x27; function=&#x27;0x0&#x27; multifunction=&#x27;on&#x27;/&gt; &lt;/controller&gt; &lt;controller type=&#x27;pci&#x27; index=&#x27;2&#x27; model=&#x27;pcie-root-port&#x27;&gt; &lt;model name=&#x27;pcie-root-port&#x27;/&gt; &lt;target chassis=&#x27;2&#x27; port=&#x27;0x11&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x02&#x27; function=&#x27;0x1&#x27;/&gt; &lt;/controller&gt; &lt;controller type=&#x27;pci&#x27; index=&#x27;3&#x27; model=&#x27;pcie-root-port&#x27;&gt; &lt;model name=&#x27;pcie-root-port&#x27;/&gt; &lt;target chassis=&#x27;3&#x27; port=&#x27;0x12&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x02&#x27; function=&#x27;0x2&#x27;/&gt; &lt;/controller&gt; &lt;controller type=&#x27;pci&#x27; index=&#x27;4&#x27; model=&#x27;pcie-root-port&#x27;&gt; &lt;model name=&#x27;pcie-root-port&#x27;/&gt; &lt;target chassis=&#x27;4&#x27; port=&#x27;0x13&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x02&#x27; function=&#x27;0x3&#x27;/&gt; &lt;/controller&gt; &lt;controller type=&#x27;pci&#x27; index=&#x27;5&#x27; model=&#x27;pcie-root-port&#x27;&gt; &lt;model name=&#x27;pcie-root-port&#x27;/&gt; &lt;target chassis=&#x27;5&#x27; port=&#x27;0x14&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x02&#x27; function=&#x27;0x4&#x27;/&gt; &lt;/controller&gt; &lt;controller type=&#x27;pci&#x27; index=&#x27;6&#x27; model=&#x27;pcie-root-port&#x27;&gt; &lt;model name=&#x27;pcie-root-port&#x27;/&gt; &lt;target chassis=&#x27;6&#x27; port=&#x27;0x15&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x02&#x27; function=&#x27;0x5&#x27;/&gt; &lt;/controller&gt; &lt;controller type=&#x27;pci&#x27; index=&#x27;7&#x27; model=&#x27;pcie-root-port&#x27;&gt; &lt;model name=&#x27;pcie-root-port&#x27;/&gt; &lt;target chassis=&#x27;7&#x27; port=&#x27;0x16&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x02&#x27; function=&#x27;0x6&#x27;/&gt; &lt;/controller&gt; &lt;controller type=&#x27;pci&#x27; index=&#x27;8&#x27; model=&#x27;pcie-root-port&#x27;&gt; &lt;model name=&#x27;pcie-root-port&#x27;/&gt; &lt;target chassis=&#x27;8&#x27; port=&#x27;0x17&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x02&#x27; function=&#x27;0x7&#x27;/&gt; &lt;/controller&gt; &lt;controller type=&#x27;virtio-serial&#x27; index=&#x27;0&#x27;&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x04&#x27; slot=&#x27;0x00&#x27; function=&#x27;0x0&#x27;/&gt; &lt;/controller&gt; &lt;interface type=&#x27;direct&#x27;&gt; &lt;mac address=&#x27;52:54:00:69:ec:73&#x27;/&gt; &lt;source dev=&#x27;eno1&#x27; mode=&#x27;bridge&#x27;/&gt; &lt;model type=&#x27;virtio&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x01&#x27; slot=&#x27;0x00&#x27; function=&#x27;0x0&#x27;/&gt; &lt;/interface&gt; &lt;interface type=&#x27;network&#x27;&gt; &lt;mac address=&#x27;52:54:00:0b:a5:2e&#x27;/&gt; &lt;source network=&#x27;ens1f0_sriov_pool&#x27;/&gt; &lt;model type=&#x27;virtio&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x02&#x27; slot=&#x27;0x00&#x27; function=&#x27;0x0&#x27;/&gt; &lt;/interface&gt; &lt;interface type=&#x27;network&#x27;&gt; &lt;mac address=&#x27;52:54:00:bd:9f:54&#x27;/&gt; &lt;source network=&#x27;ens1f1_sriov_pool&#x27;/&gt; &lt;model type=&#x27;virtio&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x08&#x27; slot=&#x27;0x00&#x27; function=&#x27;0x0&#x27;/&gt; &lt;/interface&gt; &lt;serial type=&#x27;pty&#x27;&gt; &lt;target type=&#x27;isa-serial&#x27; port=&#x27;0&#x27;&gt; &lt;model name=&#x27;isa-serial&#x27;/&gt; &lt;/target&gt; &lt;/serial&gt; &lt;console type=&#x27;pty&#x27;&gt; &lt;target type=&#x27;serial&#x27; port=&#x27;0&#x27;/&gt; &lt;/console&gt; &lt;channel type=&#x27;unix&#x27;&gt; &lt;target type=&#x27;virtio&#x27; name=&#x27;org.qemu.guest_agent.0&#x27;/&gt; &lt;address type=&#x27;virtio-serial&#x27; controller=&#x27;0&#x27; bus=&#x27;0&#x27; port=&#x27;1&#x27;/&gt; &lt;/channel&gt; &lt;channel type=&#x27;spicevmc&#x27;&gt; &lt;target type=&#x27;virtio&#x27; name=&#x27;com.redhat.spice.0&#x27;/&gt; &lt;address type=&#x27;virtio-serial&#x27; controller=&#x27;0&#x27; bus=&#x27;0&#x27; port=&#x27;2&#x27;/&gt; &lt;/channel&gt; &lt;input type=&#x27;mouse&#x27; bus=&#x27;ps2&#x27;/&gt; &lt;input type=&#x27;keyboard&#x27; bus=&#x27;ps2&#x27;/&gt; &lt;graphics type=&#x27;spice&#x27; autoport=&#x27;yes&#x27;&gt; &lt;listen type=&#x27;address&#x27;/&gt; &lt;image compression=&#x27;off&#x27;/&gt; &lt;/graphics&gt; &lt;video&gt; &lt;model type=&#x27;qxl&#x27; ram=&#x27;65536&#x27; vram=&#x27;65536&#x27; vgamem=&#x27;16384&#x27; heads=&#x27;1&#x27; primary=&#x27;yes&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x01&#x27; function=&#x27;0x0&#x27;/&gt; &lt;/video&gt; &lt;redirdev bus=&#x27;usb&#x27; type=&#x27;spicevmc&#x27;&gt; &lt;address type=&#x27;usb&#x27; bus=&#x27;0&#x27; port=&#x27;2&#x27;/&gt; &lt;/redirdev&gt; &lt;redirdev bus=&#x27;usb&#x27; type=&#x27;spicevmc&#x27;&gt; &lt;address type=&#x27;usb&#x27; bus=&#x27;0&#x27; port=&#x27;3&#x27;/&gt; &lt;/redirdev&gt; &lt;memballoon model=&#x27;none&#x27;/&gt; &lt;/devices&gt;&lt;/domain&gt; The parameters are from Cisco CSR 1000v and Cisco ISRv Software Configuration GuideAfter you install the CSR1000v, and edit the XML file, you can run virsh to create the csr1kv. 123cd /etc/libvirtd/qemuvirsh define csr1kv-1.xmlvirsh start csr1kv-1 (6) Check CSR1000v’s tunningubuntu@ubuntu-kvm:/etc/libvirt/qemu$ virsh list 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253 Id Name State-------------------------- 1 csr1kv-1 runningubuntu@ubuntu-kvm:/etc/libvirt/qemu$ virsh vcpuinfo 1VCPU: 0CPU: 1State: runningCPU time: 167.0sCPU Affinity: -y--------------------------------------------------------------------------------------VCPU: 1CPU: 2State: runningCPU time: 193.3sCPU Affinity: --y-------------------------------------------------------------------------------------VCPU: 2CPU: 3State: runningCPU time: 152.5sCPU Affinity: ---y------------------------------------------------------------------------------------VCPU: 3CPU: 4State: runningCPU time: 174.3sCPU Affinity: ----y-----------------------------------------------------------------------------------VCPU: 4CPU: 5State: runningCPU time: 260.6sCPU Affinity: -----y----------------------------------------------------------------------------------VCPU: 5CPU: 6State: runningCPU time: 386.0sCPU Affinity: ------y---------------------------------------------------------------------------------VCPU: 6CPU: 7State: runningCPU time: 2189.9sCPU Affinity: -------y--------------------------------------------------------------------------------VCPU: 7CPU: 8State: runningCPU time: 2192.2sCPU Affinity: --------y------------------------------------------------------------------------------- From the above outputs, we can find that the vCPUa are pinned to NO. 1 to 8 CPU cores. 12345678910111213141516171819202122232425ubuntu@ubuntu-kvm:~$ sudo numastat -c qemuPer-node process memory usage (in MBs)PID Node 0 Node 1 Total--------------- ------ ------ -----4391 (qemu-syste 8373 0 83735891 (sudo) 4 1 5--------------- ------ ------ -----Total 8377 1 8377ubuntu@ubuntu-kvm:~$ sudo numastat -p 4391Per-node process memory usage (in MBs) for PID 4391 (qemu-system-x86) Node 0 Node 1 Total --------------- --------------- ---------------Huge 8192.00 0.00 8192.00Heap 10.00 0.00 10.00Stack 0.03 0.00 0.03Private 170.70 0.15 170.85---------------- --------------- --------------- ---------------Total 8372.73 0.16 8372.88 From the above outputs, the csr1kv-1 are using the memories from node 0. (7) Check the vNIC in CSR1KV123456789csr1kv-1#show platform software vnic-if interface-mapping------------------------------------------------------------- Interface Name Driver Name Mac Addr------------------------------------------------------------- GigabitEthernet3 net_i40e_vf 5254.00bd.9f54 GigabitEthernet2 net_i40e_vf 5254.000b.a52e GigabitEthernet1 net_virtio 5254.0069.ec73 The Driver Name net_i40e_vf indicates that the vNIC is a VF from the SR-IOV pool. CSR 1000v initial configuration and Smart License registration(1) CSR 1000v initial config examplePart of the configuration: 123456789101112131415161718192021222324252627282930313233343536373839404142CSR1000v-1#show sdwan running-configsystem system-ip 1.1.10.1 site-id 101 sp-organization-name CiscoBJ organization-name CiscoBJ vbond 10.75.58.51 port 12346!hostname CSR1000v-1username admin privilege 15 secret 9 $9$4&#x2F;QL2V2K4&#x2F;6H1k$XUmRNf.T7t3KDOj&#x2F;FmoNexpEypCxr482dExXHDnohSIip name-server 64.104.123.245ip route 0.0.0.0 0.0.0.0 10.75.59.1interface GigabitEthernet1 no shutdown arp timeout 1200 ip address 10.75.59.35 255.255.255.0 no ip redirects ip mtu 1500 mtu 1500 negotiation autoexitinterface Tunnel1 no shutdown ip unnumbered GigabitEthernet1 no ip redirects ipv6 unnumbered GigabitEthernet1 no ipv6 redirects tunnel source GigabitEthernet1 tunnel mode sdwanexitclock timezone CST 8 0ntp server x.x.x.x version 4sdwan interface GigabitEthernet1 tunnel-interface encapsulation ipsec allow-service sshd exit (2) Commands to check the status1234567show sdwan control local-propertiesshow sdwan control connectionsshow sdwan control connection-historyshow sdwan running-configshow sdwan bfd sessionsshow sdwan omp peersshow sdwan omp routes (3) CSR 1000v Smart License RegistrationBefore Smart License registration, you need ： CSR 1000v’s control connections are up； Configure ip http client source-interface GigabitEthernet2 If the version is 16.12.x and bellow, you need to allow service allsdwan interface GigabitEthernet2 tunnel-interface allow-service all In the 17.2.x and above versions, there is an allow-service https CSR 1000v can access URL：https://tools.cisco.com/its/service/oddce/services/DDCEServiceThe command license smart register idtoken xxxxxx will do the registration.You can find the idtoken from your Smart Account smart license inventory.show license status to check the status. 12CSR1000v-1#show platform hardware throughput levelThe current throughput level is 200000000 kb&#x2F;s Performances and limitations(1) The performance of the SR-IOVA performance test was done after the SR-IOV setup, the CSR1KV was configured as 8vCPU and 8G Memory, the packet drop rate was 0%. Packet Site SDWAN Performance (Mbps) CEF Performance (Mbps) 128Bytes 800 2843.75 256Bytes 1431.26 6500.00 512Bytes 2581.26 10578.13 1024Bytes 3731.26 15500.00 1400Bytes 4306.26 18171.88 Note: These test results are not to represent the official performance data. Different servers and network cards may have different test results. The above data is for demo only. (2) The limitation of the SR-IOVThe main limitation of the SR-IOV is the number of VLANs on each VF, the maximum VLAN of an VF is limited to 63 in ixgbevf.So, the active number of sub-interfaces on an interface of the CSR1KV that uses the SRIOV VFs is limited to 63.There is some notes in the Cisco CSR 1000v and Cisco ISRv Software Configuration Guide: SR-IOV (ixgbevf)Maximum VLANs: The maximum number of VLANs supported on PF is 64. Together, all VFs can have a total of 64 VLANs. (Intel limitation.) SR-IOV (i40evf)Maximum VLANs: The maximum number of VLANs supported on PF is 512. Together, all VFs can have a total of 512 VLANs. (Intel limitation.) Per-VF resources are managed by the PF (host) device driver. Referenceshttps://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html-single/performance_tuning_guide/index https://computingforgeeks.com/how-to-create-and-configure-bridge-networking-for-kvm-in-linux/ https://software.intel.com/content/www/us/en/develop/articles/configure-sr-iov-network-virtual-functions-in-linux-kvm.html https://linuxconfig.org/install-and-set-up-kvm-on-ubuntu-20-04-focal-fossa-linux https://kifarunix.com/how-to-fix-qemu-kvm-not-connected-error-on-ubuntu-20-04/ https://linuxconfig.org/how-to-disable-apparmor-on-ubuntu-20-04-focal-fossa-linux CSR1000v SD-WAN Installation on KVM by Jean-Marc Barozet Scripts and XML Example Files Ubuntu and KVM installation and setting NIC Settings with nmcli Check NIC SRIOV Capabilities GRUB Settings SRIOV Settings Check VFs Create SRIOV Adapter Pool KVM Tuning Check CSR1000v Tuning ens1f0_sriov_pool.xml csr1kv-1.xml","categories":[],"tags":[{"name":"CSR1000v","slug":"CSR1000v","permalink":"https://weiborao.link/tags/CSR1000v/"},{"name":"KVM","slug":"KVM","permalink":"https://weiborao.link/tags/KVM/"},{"name":"SRIOV","slug":"SRIOV","permalink":"https://weiborao.link/tags/SRIOV/"},{"name":"NetworkManager","slug":"NetworkManager","permalink":"https://weiborao.link/tags/NetworkManager/"}]},{"title":"CSR 1000v KVM SRIOV CentOS 7安装设置指南","slug":"csr1kv-kvm-CentOS7","date":"2021-02-03T05:58:32.000Z","updated":"2021-02-05T05:57:29.431Z","comments":true,"path":"csr1kv-kvm-CentOS7.html","link":"","permalink":"https://weiborao.link/csr1kv-kvm-CentOS7.html","excerpt":"","text":"作者：Rao Weibo 版本：1.0 更新日期：20210203 本文提供在 CentOS 7 版本上安装 KVM，并安装和设置 CSR 1000v/Catalyst 8000v 的指南，内容包括 SRIOV，KVM 调优以及 CSR1KV 初始化配置等内容。 CentOS7 安装及设置（1）BIOS 设置建议 Configuration Recommended Setting Intel Hyper-Threading Technology Disabled Number of Enable Cores ALL Execute Disable Enabled Intel VT Enabled Intel VT-D Enabled Intel VT-D coherency support Enabled Intel VT-D ATS support Enabled CPU Performance High throughput Hardware Perfetcher Disabled Adjacent Cache Line Prefetcher Disabled DCU Streamer Prefetch Disable Power Technology Custom Enhanced Intel Speedstep Technology Disabled Intel Turbo Boost Technology Enabled Processor Power State C6 Disabled Processor Power State C1 Enhanced Disabled Frequency Poor Override Enabled P-State Coordination HW_ALL Energy Performance Performance 以上建议来自 CSR 1000v 的安装指南。 （2）CentOS 7 的安装在安装的时候选择 Server with GUI Virtualization Client Virtualization Hypervisor Virtualization Tools 启动完毕以后关闭 selinux，重启生效。 123sed -i &#x27;s/SELINUX=enforcing/SELINUX=disabled/g&#x27; /etc/selinux/configgetenforce //结果为：Enforcing（开启状态）disabled(关闭状态) 123456789101112131415# 安装完后，SSH登录可能显示中文，可修改 .bash_profileLANG=&quot;en_US.UTF-8&quot;export LANGsource .bash_profileegrep -o &#x27;(vmx|svm)&#x27; /proc/cpuinfo | sort | uniq# 注：在生产环境中，需要在服务器连接的交换机以及出口防火墙上做好安全策略。systemctl stop firewalldsystemctl disable firewalld 本文档采用 NetworkManager 配置，故在此并不停用 NetworkManager。 123456789cat /etc/sysctl.confnet.ipv4.ip_forward = 1net.bridge.bridge-nf-call-ip6tables = 0net.bridge.bridge-nf-call-iptables = 0net.bridge.bridge-nf-call-arptables = 0 检查 KVM 组件版本： 123456789101112131415161718192021[root@centos7 ~]# libvirtd -Vlibvirtd (libvirt) 4.5.0[root@centos7 ~]# /usr/libexec/qemu-kvm --versionQEMU emulator version 1.5.3 (qemu-kvm-1.5.3-173.el7_8.3), Copyright (c) 2003-2008 Fabrice Bellard[root@centos7 ~]# virt-manager --version1.5.0[root@centos7 ~]# modinfo kvm-intelfilename: /lib/modules/3.10.0-1127.18.2.el7.x86_64/kernel/arch/x86/kvm/kvm-intel.ko.xz[root@centos7 ~]# modinfo ixgbevffilename: /lib/modules/3.10.0-1127.18.2.el7.x86_64/kernel/drivers/net/ethernet/intel/ixgbevf/ixgbevf.ko.xzversion: 4.1.0-k-rh7.7 （3）创建本地 Yum 源（可选）12345678910111213141516171819202122232425262728293031323334353637383940414243# 1、备份本地/etc/yum.repos.d 目录下的yum源cd /etc/yum.repos.d/mkdir bakmv C* bak/# 2、上传CentOS-7-x86_64-Everything-2009.iso镜像到/opt# 3、挂载镜像mkdir -p /media/cdrommount -t iso9660 -o loop /opt/CentOS-7-x86_64-Everything-2009.iso /media/cdrom/mount //查看挂载信息df -hvi /etc/fstab/opt/CentOS-7-x86_64-Everything-2009.iso /media/cdrom iso9660 loop 0 0tail -1 /etc/fstab //查看是否写入/etc/fstab# 4、配置本地yum源cd /etc/yum.repos.d/vi local.repo[local]name=localbaseurl=file:///media/cdrom //前面的file://是协议,后面的/media/cdrom是光盘挂载点gpgcheck=0 //1使用公钥验证rpm包的正确性,0不验证enabled=1 //1启用yum源,0禁用yum源yum install -y numactl telnet 运行 virt-manager 启动图形化界面。 如果对 virsh CLI 命令熟悉，可以使用 virsh 命令创建虚拟机。 （4）服务器网卡配置—NetworkManager 配置在终端界面，可以通过 nmtui 打开图形化界面进行设置；以下使用 nmcli 进行设置。 123456789nmcli connection add con-name eno1 type ethernet autoconnect yes ifname eno1nmcli connection modify eno1 ipv4.method manual ipv4.addresses 10.75.58.43/24 ipv4.gateway 10.75.58.1 ipv4.dns 64.104.123.245nmcli connection up eno1nmcli connection show eno1ping 10.75.58.1 上述命令完成后，在/etc/sysconfig/network-scripts 中会生成网卡的 ifcfg 配置文件。 12345678910111213141516171819202122232425262728293031323334353637383940414243cat /etc/sysconfig/network-scripts/ifcfg-eno1HWADDR=70:7D:B9:59:5B:AETYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=noneIPADDR=10.75.58.43PREFIX=24GATEWAY=10.75.58.1DNS1=64.104.123.245DEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=eno1UUID=2a1c8b39-7f44-321b-a65f-a93e70ab0616ONBOOT=yesAUTOCONNECT_PRIORITY=-999DEVICE=eno1 此时，可以将 network.service 停止和关闭。 123systemctl stop networksystemctl disable network 注意，如果 NetworkManager 未设置妥当，执行 systemctl stop network 后，会导致服务器无法管理。 准备开启 SRIOV 的网卡设置，以 eno2 为例： 123456789nmcli connection add con-name eno2 type ethernet autoconnect yes ifname eno2nmcli connection modify eno2 ethernet.mtu 9216 ipv4.method disablednmcli connection up eno2nmcli connection show eno2ip link show dev eno2 注：上述 MTU 值设置为 9216 是借鉴自 Cisco NFVIS 平台，如下： 12345678910111213CSP5228-1# show pnic-detail mtuName MTU&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;eth0-1 9216eth0-2 9216eth1-1 9216eth1-2 9216 （5）配置 Linux 网桥 （可选）网桥 br1 配置示例： 12345nmcli connection add con-name br1 type bridge autoconnect yes ipv4.method disabled ethernet.mtu 9216 ifname br1nmcli connection up br1ip link show dev br1 网桥 br1 的物理网卡配置 1234567nmcli connection add con-name eno5 type ethernet autoconnect yes ifname eno5nmcli connection modify eno5 ethernet.mtu 9216 ipv4.method disabled master br1nmcli connection up eno5ip link show dev eno5 创建 net-br1 网络 [root@centos7 ~]# cat net-br1.xml 123456789&lt;network&gt; &lt;name&gt;net-br1&lt;/name&gt; &lt;forward mode=&quot;bridge&quot;/&gt; &lt;bridge name=&quot;br1&quot;/&gt;&lt;/network&gt; 1234567891011[root@centos7 ~]# virsh net-define net-br1.xmlNetwork net-br1 defined from net-br1.xml[root@centos7 ~]# virsh net-start net-br1Network net-br1 started[root@centos7 ~]# virsh net-autostart net-br1Network net-br1 marked as autostarted 配置 SR-IOV（1）检查网卡对 SR-IOV 的支持，并配置网卡可使用 lshw 和 lspci 检查网卡对 SR-IOV 的支持 lshw -c network -businfo 123456789101112131415Bus info Device Class Description========================================================pci@0000:1d:00.0 eno5 network VIC Ethernet NICpci@0000:1d:00.1 eno6 network VIC Ethernet NICpci@0000:1d:00.2 eno7 network VIC Ethernet NICpci@0000:1d:00.3 eno8 network VIC Ethernet NICpci@0000:3b:00.0 eno1 network Ethernet Controller 10G X550Tpci@0000:3b:00.1 eno2 network Ethernet Controller 10G X550T lspci -vv -s 3b:00.1 | grep -A 5 -i SR-IOV 1234567891011Capabilities: [160 v1] Single Root I/O Virtualization (SR-IOV) IOVCap: Migration-, Interrupt Message Number: 000 IOVCtl: Enable+ Migration- Interrupt- MSE+ ARIHierarchy- IOVSta: Migration- Initial VFs: 64, Total VFs: 64, Number of VFs: 8, Function Dependency Link: 01 VF offset: 128, stride: 2, Device ID: 1565 （2）设置启动参数1234567891011vi /etc/default/grubGRUB_CMDLINE_LINUX=&quot;crashkernel=auto spectre_v2=retpoline rd.lvm.lv=centos/root rd.lvm.lv=centos/swap rhgb quiet hugepagesz=1G hugepages=32 default_hugepagesz=1G intel_iommu=on iommu=pt isolcpus=1-8,37-44&quot;注：页面数字不要过大，不然启动失败，如果后续不够，可以在运行时添加。grub2-mkconfig -o /boot/grub2/grub.cfggrub2-mkconfig -o /boot/efi/EFI/centos/grub.cfgiommu=pt 参数是将SRIOV设备支持PCI Passthrough 重启后验证 12345cat /proc/cmdline |grep intel_iommu=ondmesg |grep -e DMAR -e IOMMUdmesg | grep -e DMAR -e IOMMU -e AMD-Vi default_hugepagesz=1G hugepagesz=1G hugepages=32 参数设置主机在启动时分配 32 个 1GB 的内存大页，这些是静态内存大页。 CSR 1000v 虚拟机将试用这些静态大页以获得最优性能。 isolcpus=1-8,37-44 参数设置的作用是隔离 1-8，37-44 的 CPU 核，使其独立于内核的平衡调度算法，也就是内核本身不会将进程分配到被隔离的 CPU。之后我们可将指定的进程 CSR 1000v 虚拟机绑定到被隔离的 CPU 上运行，让进程独占 CPU，使其实时性可得到一定程度的提高。 可参考 这个章节获取主机 CPU 核的相关信息。 123456789root@centos7 ~]# cat /proc/cmdline |grep intel_iommu=onBOOT_IMAGE=/vmlinuz-3.10.0-1127.el7.x86_64 root=/dev/mapper/centos-root ro crashkernel=auto spectre_v2=retpoline rd.lvm.lv=centos/root rd.lvm.lv=centos/swap rhgb quiet hugepagesz=1G hugepages=32 default_hugepagesz=1G intel_iommu=on iommu=pt LANG=en_US.UTF-8[root@centos7 ~]# dmesg |grep -e DMAR -e IOMMU[ 0.000000] ACPI: DMAR 000000005d6f5d70 00250 (v01 Cisco0 CiscoUCS 00000001 INTL 20091013)[ 0.000000] DMAR: IOMMU enabled 查看隔离的 CPU 核以及所有的 CPU 核。 1234567[root@centos7 ~]# cat /sys/devices/system/cpu/isolated1-8,37-44[root@centos7 ~]# cat /sys/devices/system/cpu/present0-71 （3）通过 nmcli 持久化 VFs 配置nmcli 可以设置网卡的 sriov 参数，如下： 1nmcli connection modify eno2 sriov.total-vfs 4 还可以设置每一个 VF 设备的 MAC 地址，便于管理： 1nmcli connection modify eno2 sriov.vfs &#x27;0 mac=8E:DF:08:C1:D1:DE trust=true, 1 mac=5A:B9:2F:99:A6:CE trust=true, 2 mac=46:78:69:E3:71:3D trust=true, 3 mac=7E:A7:DB:3B:1B:B3 trust=true&#x27; 执行上述命令后： cat /etc/sysconfig/network-scripts/ifcfg-eno2 1234567891011121314151617181920212223242526272829TYPE=EthernetNAME=eno2UUID=64ffa204-0158-40c8-ba86-2b7aebf27619DEVICE=eno2ONBOOT=yesMTU=9216HWADDR=70:7D:B9:59:5B:AFPROXY_METHOD=noneBROWSER_ONLY=noIPV6INIT=noSRIOV_TOTAL_VFS=4SRIOV_VF0=&quot;mac=8E:DF:08:C1:D1:DE trust=true&quot;SRIOV_VF1=&quot;mac=5A:B9:2F:99:A6:CE trust=true&quot;SRIOV_VF2=&quot;mac=46:78:69:E3:71:3D trust=true&quot;SRIOV_VF3=&quot;mac=7E:A7:DB:3B:1B:B3 trust=true&quot; 重启后，检查 dmesg： dmesg | grep -i vf | grep -i eno2 12345678910111213141516171819202122232425[ 11.953333] ixgbe 0000:3b:00.1 eno2: SR-IOV enabled with 4 VFs[ 12.541801] ixgbe 0000:3b:00.1: setting MAC 8e:df:08:c1:d1:de on VF 0[ 12.541805] ixgbe 0000:3b:00.1: Reload the VF driver to make this change effective.[ 12.541841] ixgbe 0000:3b:00.1 eno2: VF 0 is trusted[ 12.541846] ixgbe 0000:3b:00.1: setting MAC 5a:b9:2f:99:a6:ce on VF 1[ 12.541850] ixgbe 0000:3b:00.1: Reload the VF driver to make this change effective.[ 12.541883] ixgbe 0000:3b:00.1 eno2: VF 1 is trusted[ 12.541887] ixgbe 0000:3b:00.1: setting MAC 46:78:69:e3:71:3d on VF 2[ 12.541891] ixgbe 0000:3b:00.1: Reload the VF driver to make this change effective.[ 12.541923] ixgbe 0000:3b:00.1 eno2: VF 2 is trusted[ 12.541928] ixgbe 0000:3b:00.1: setting MAC 7e:a7:db:3b:1b:b3 on VF 3[ 12.541932] ixgbe 0000:3b:00.1: Reload the VF driver to make this change effective.[ 12.541965] ixgbe 0000:3b:00.1 eno2: VF 3 is trusted （4）检查 VF可通过 lspci 和 ip link 检查 VF，如下： 123[root@centos7 ~]# lspci | grep -i Virtual[root@centos7 ~]# ip link show | grep -B2 vf 寻找 Physical Function 和 Virtual Function 之间的对应关系： 1[root@centos7 ~]# ls -l /sys/class/net/eno1/device/ | grep virtfn VF 被创建后，NetworkManager 自动给新的设备创建 Connection，可以修改名称，如下： nmcli connection 12345678910111213NAME UUID TYPE DEVICEeno1 2a1c8b39-7f44-321b-a65f-a93e70ab0616 ethernet eno1eno2 64ffa204-0158-40c8-ba86-2b7aebf27619 ethernet eno2enp59s16f1 19c28a93-aa36-38e6-a556-55a922a0a332 ethernet enp59s16f1enp59s16f3 428d9707-1515-3475-b356-7eb229c3f937 ethernet enp59s16f3enp59s16f5 21a8dd5f-239f-37b2-9b09-24ce0e7413bc ethernet enp59s16f5enp59s16f7 0ca0da65-64c4-314d-89a4-4213f9e4f478 ethernet enp59s16f7 修改名称： 1nmcli connection modify uuid 19c28a93-aa36-38e6-a556-55a922a0a332 connection.id enp59s16f1 修改 MTU 值，并禁用 IPv4 和 IPv6，网卡启动更快 12345nmcli connection modify enp59s16f1 ifname enp59s16f1 ipv4.method disabled ipv6.method ignore ethernet.mtu 9216 ethernet.mac-address &quot;&quot;nmcli connection up enp59s16f1nmcli connection show enp59s16f1 上述命令生成的 ifcfg 配置文件如下： [root@centos7 ~]# cat /etc/sysconfig/network-scripts/ifcfg-enp59s16f1 12345678910111213141516171819202122232425262728293031TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=noIPV6_AUTOCONF=noIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=enp59s16f1UUID=19c28a93-aa36-38e6-a556-55a922a0a332ONBOOT=yesAUTOCONNECT_PRIORITY=-999DEVICE=enp59s16f1MTU=9216 这样，即便系统重启，上述配置依然能生效。 使用 KVM 的虚拟网络适配器池主机上创建一个 VF 网络设备的资源池，资源池内的设备可以自动地分配给虚拟机使用。 （1）创建一个 xml 文件。[root@centos7 ~]# cat eno2_sriov_pool.xml 1234567891011&lt;network&gt; &lt;name&gt;eno2_sriov_pool&lt;/name&gt; &lt;!-- This is the name of the file you created --&gt; &lt;forward mode=&#x27;hostdev&#x27; managed=&#x27;yes&#x27;&gt; &lt;pf dev=&#x27;eno2&#x27;/&gt; &lt;!-- Use the netdev name of your SR-IOV devices PF here --&gt; &lt;/forward&gt;&lt;/network&gt; （2）根据 xml 定义一个网络，并设置为自动重启12345virsh net-define eno2_sriov_pool.xmlvirsh net-start eno2_sriov_poolvirsh net-autostart eno2_sriov_pool [root@centos7 ~]# virsh net-dumpxml eno2_sriov_pool 123456789101112131415161718192021&lt;network connections=&#x27;1&#x27;&gt; &lt;name&gt;eno2_sriov_pool&lt;/name&gt; &lt;uuid&gt;e0842451-0137-4255-8783-305ca27f082d&lt;/uuid&gt; &lt;forward mode=&#x27;hostdev&#x27; managed=&#x27;yes&#x27;&gt; &lt;pf dev=&#x27;eno2&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x3b&#x27; slot=&#x27;0x10&#x27; function=&#x27;0x1&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x3b&#x27; slot=&#x27;0x10&#x27; function=&#x27;0x3&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x3b&#x27; slot=&#x27;0x10&#x27; function=&#x27;0x5&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x3b&#x27; slot=&#x27;0x10&#x27; function=&#x27;0x7&#x27;/&gt; &lt;/forward&gt;&lt;/network&gt; （3）从网络适配器池中分配网卡给虚拟机用这种方法添加 SRIOV 网卡比较简单： 按照如上方法添加网卡，等同于以下 xml 配置： 1234567891011&lt;interface type=&#x27;network&#x27;&gt; &lt;mac address=&#x27;52:54:00:2d:87:99&#x27;/&gt; &lt;source network=&#x27;eno2_sriov_pool&#x27;/&gt; &lt;model type=&#x27;virtio&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x04&#x27; function=&#x27;0x0&#x27;/&gt;&lt;/interface&gt; 开机后 dumpxml 如下： 12345678910111213141516171819 &lt;interface type=&#x27;hostdev&#x27; managed=&#x27;yes&#x27;&gt; &lt;mac address=&#x27;52:54:00:2d:87:99&#x27;/&gt; &lt;driver name=&#x27;vfio&#x27;/&gt; &lt;source&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x3b&#x27; slot=&#x27;0x10&#x27; function=&#x27;0x1&#x27;/&gt; &lt;/source&gt; &lt;model type=&#x27;virtio&#x27;/&gt; &lt;alias name=&#x27;hostdev0&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x0f&#x27; function=&#x27;0x0&#x27;/&gt;&lt;/interface&gt; 使用 Virt-manager 安装 CSR1000v在 CentOS 图形界面中，打开 Terminal，运行 virt-manager，按照以下步骤创建 CSR1000v；添加网卡，并选择 en2_sriov_pool。 注：csr1kv-1 的第一个网口选择macvtap Bridge模式，这样就无需创建一个 Linux 网桥。但是，csr1kv-1 启动以后不能通过该接口与 Linux 主机进行通信，仅能通过该接口访问 Linux 主机外的网络。 添加完网卡后，点击开始安装，然后就可以关闭虚拟机了。上述操作完成后，virt-manager 会在**/etc/libvirtd/qemu/目录下创建csr1kv-1.xml**。 KVM 调优配置KVM 的调优比较复杂，主要是 NUMA、内存大页、vCPU PIN 等，参考资料为 Redhat Linux 7 PERFORMANCE TUNING GUIDE。 （1）检查平台的能力1234567891011121314151617[root@centos7 ~]# virsh nodeinfoCPU model: x86_64CPU(s): 72CPU frequency: 999 MHzCPU socket(s): 1Core(s) per socket: 18Thread(s) per core: 2NUMA cell(s): 2Memory size: 263665612 KiB 1234567891011121314151617181920212223[root@centos7 ~]# virsh capabilities&lt;capabilities&gt; &lt;host&gt; &lt;uuid&gt;4e53df1f-5b36-6842-99ee-1369d7c68730&lt;/uuid&gt; &lt;cpu&gt; &lt;arch&gt;x86_64&lt;/arch&gt; &lt;model&gt;Skylake-Server-IBRS&lt;/model&gt; &lt;vendor&gt;Intel&lt;/vendor&gt; &lt;microcode version=&#x27;33581318&#x27;/&gt; &lt;counter name=&#x27;tsc&#x27; frequency=&#x27;2294597000&#x27; scaling=&#x27;yes&#x27;/&gt; &lt;topology sockets=&#x27;1&#x27; cores=&#x27;18&#x27; threads=&#x27;2&#x27;/&gt;…… 可检查平台的 CPU 核数、分布，内存的 NUMA 分布等。 （2）NUMA 调优1234567891011121314151617181920212223[root@centos7 ~]# numactl --hardwareavailable: 2 nodes (0-1)node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53node 0 size: 128491 MBnode 0 free: 112157 MBnode 1 cpus: 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71node 1 size: 128994 MBnode 1 free: 124120 MBnode distances:node 0 1 0: 10 21 1: 21 10 两颗 CPU，每颗 CPU 各有 128GB 内存，分别是 node 0 和 node 1。 （3）内存大页 HugePage 以及透明大页 cat /proc/meminfo | grep HugePages 查看当前系统有多少个大页： 12345678910111213[root@centos7 ~]# cat /proc/meminfo | grep HugeAnonHugePages: 1685504 kBHugePages_Total: 64HugePages_Free: 60HugePages_Rsvd: 0HugePages_Surp: 0Hugepagesize: 1048576 kB 在系统运行时修改大页数量： 123456789101112131415161718192021[root@centos7 ~]# cat /sys/devices/system/node/node0/hugepages/hugepages-1048576kB/nr_hugepages16[root@centos7 ~]# echo 32 &gt; /sys/devices/system/node/node0/hugepages/hugepages-1048576kB/nr_hugepages[root@centos7 ~]# echo 32 &gt; /sys/devices/system/node/node1/hugepages/hugepages-1048576kB/nr_hugepages[root@centos7 ~]#[root@centos7 ~]# numastat -cm | egrep &#x27;Node|Huge&#x27;` `Node 0 Node 1 TotalAnonHugePages 10962 1528 12490HugePages_Total 32768 32768 65536HugePages_Free 32768 32768 65536HugePages_Surp 0 0 0 检查透明大页参数，在 CentOS7 上缺省开启；开启了透明大页，不影响静态大页的使用。 123cat /sys/kernel/mm/transparent_hugepage/enabled[always] madvise never （4）vCPU 钉选设置 CPU Affinity 的好处是提高 CPU 缓存效率，避免进程在多个 CPU 核之间跳跃，切换 CPU 核均会导致缓存中的数据无效，缓存命中率大幅降低，导致数据获取的开销居高不下，损失性能。 virsh vcpuinfo csr1kv-1 可以查看 vCPU 的分配。 （5）编辑 CSR 1000v 的 XML 调优参数virsh edit csr1kv-1 可以编辑 XML 的参数，如下： 123456789101112131415161718192021222324&lt;memoryBacking&gt; &lt;hugepages&gt; &lt;page size=&#x27;1048576&#x27; unit=&#x27;KiB&#x27;/&gt; &lt;/hugepages&gt; &lt;locked/&gt; &lt;nosharepages/&gt;&lt;/memoryBacking&gt;&lt;vcpu placement=&#x27;static&#x27;&gt;8&lt;/vcpu&gt;&lt;cputune&gt; &lt;vcpupin vcpu=&#x27;0&#x27; cpuset=&#x27;1&#x27;/&gt; &lt;vcpupin vcpu=&#x27;1&#x27; cpuset=&#x27;2&#x27;/&gt; &lt;vcpupin vcpu=&#x27;2&#x27; cpuset=&#x27;3&#x27;/&gt; &lt;vcpupin vcpu=&#x27;3&#x27; cpuset=&#x27;4&#x27;/&gt; &lt;vcpupin vcpu=&#x27;4&#x27; cpuset=&#x27;5&#x27;/&gt; &lt;vcpupin vcpu=&#x27;5&#x27; cpuset=&#x27;6&#x27;/&gt; &lt;vcpupin vcpu=&#x27;6&#x27; cpuset=&#x27;7&#x27;/&gt; &lt;vcpupin vcpu=&#x27;7&#x27; cpuset=&#x27;8&#x27;/&gt; &lt;emulatorpin cpuset=&#x27;45-52&#x27;/&gt; &lt;!-- If Hyper-threading were enabled --&gt;&lt;/cputune&gt;&lt;numatune&gt; &lt;memory mode=&#x27;strict&#x27; nodeset=&#x27;0&#x27;/&gt;&lt;/numatune&gt;&lt;cpu mode=&#x27;host-passthrough&#x27; check=&#x27;none&#x27;/&gt; &lt;memballoon model=&#x27;none&#x27;/&gt; 注：**** 参数，仅当 Hyper-Threating 开启时使用；有一些平台并未关闭超线程，例如 Cisco 专门的 NFV 平台 CSP。通过 virsh chapabilities 查看 siblings=’1,37’，当 core 1 设置为 vcpupin 时，core 37 应设置到 emulatorpin cpuset 中。 以下关于 CPU 和内存的参数设定建议来自于https://libvirt.org/formatdomain.html 和 https://libvirt.org/kbase/kvm-realtime.html 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129&lt;domain type=&#x27;kvm&#x27;&gt; &lt;name&gt;csr1kv-1&lt;/name&gt; &lt;uuid&gt;59581018-6387-49df-ab09-2bcf40fc12ba&lt;/uuid&gt; &lt;memory unit=&#x27;KiB&#x27;&gt;8388608&lt;/memory&gt; &lt;currentMemory unit=&#x27;KiB&#x27;&gt;8388608&lt;/currentMemory&gt; &lt;memoryBacking&gt; &lt;hugepages&gt; &lt;page size=&#x27;1048576&#x27; unit=&#x27;KiB&#x27;/&gt;&lt;/hugepages&gt;&lt;locked/&gt; &lt;nosharepages/&gt; &lt;/memoryBacking&gt; &lt;vcpu placement=&#x27;static&#x27;&gt;8&lt;/vcpu&gt; &lt;cputune&gt; &lt;vcpupin vcpu=&#x27;0&#x27; cpuset=&#x27;1&#x27;/&gt; &lt;vcpupin vcpu=&#x27;1&#x27; cpuset=&#x27;2&#x27;/&gt; &lt;vcpupin vcpu=&#x27;2&#x27; cpuset=&#x27;3&#x27;/&gt; &lt;vcpupin vcpu=&#x27;3&#x27; cpuset=&#x27;4&#x27;/&gt; &lt;vcpupin vcpu=&#x27;4&#x27; cpuset=&#x27;5&#x27;/&gt; &lt;vcpupin vcpu=&#x27;5&#x27; cpuset=&#x27;6&#x27;/&gt; &lt;vcpupin vcpu=&#x27;6&#x27; cpuset=&#x27;7&#x27;/&gt; &lt;vcpupin vcpu=&#x27;7&#x27; cpuset=&#x27;8&#x27;/&gt; &lt;emulatorpin cpuset=&#x27;37-44&#x27;/&gt; &lt;/cputune&gt; &lt;numatune&gt; &lt;memory mode=&#x27;strict&#x27; nodeset=&#x27;0&#x27;/&gt; &lt;/numatune&gt; &lt;os&gt; &lt;type arch=&#x27;x86_64&#x27; machine=&#x27;pc-i440fx-rhel7.0.0&#x27;&gt;hvm&lt;/type&gt; &lt;boot dev=&#x27;hd&#x27;/&gt; &lt;/os&gt; &lt;features&gt; &lt;acpi/&gt; &lt;apic/&gt; &lt;/features&gt; &lt;cpu mode=&#x27;host-passthrough&#x27; check=&#x27;none&#x27;/&gt; &lt;clock offset=&#x27;utc&#x27;&gt; &lt;timer name=&#x27;rtc&#x27; tickpolicy=&#x27;catchup&#x27;/&gt; &lt;timer name=&#x27;pit&#x27; tickpolicy=&#x27;delay&#x27;/&gt; &lt;timer name=&#x27;hpet&#x27; present=&#x27;no&#x27;/&gt; &lt;/clock&gt; &lt;on_poweroff&gt;destroy&lt;/on_poweroff&gt; &lt;on_reboot&gt;restart&lt;/on_reboot&gt; &lt;on_crash&gt;destroy&lt;/on_crash&gt; &lt;pm&gt; &lt;suspend-to-mem enabled=&#x27;no&#x27;/&gt; &lt;suspend-to-disk enabled=&#x27;no&#x27;/&gt; &lt;/pm&gt; &lt;devices&gt; &lt;emulator&gt;/usr/libexec/qemu-kvm&lt;/emulator&gt; &lt;disk type=&#x27;file&#x27; device=&#x27;disk&#x27;&gt; &lt;driver name=&#x27;qemu&#x27; type=&#x27;qcow2&#x27;/&gt; &lt;source file=&#x27;/home/root/images/csr1000v-universalk9.17.02.01v.qcow2&#x27;/&gt; &lt;target dev=&#x27;vda&#x27; bus=&#x27;virtio&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x07&#x27; function=&#x27;0x0&#x27;/&gt; &lt;/disk&gt; &lt;controller type=&#x27;usb&#x27; index=&#x27;0&#x27; model=&#x27;ich9-ehci1&#x27;&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x05&#x27; function=&#x27;0x7&#x27;/&gt; &lt;/controller&gt; &lt;controller type=&#x27;usb&#x27; index=&#x27;0&#x27; model=&#x27;ich9-uhci1&#x27;&gt; &lt;master startport=&#x27;0&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x05&#x27; function=&#x27;0x0&#x27; multifunction=&#x27;on&#x27;/&gt; &lt;/controller&gt; &lt;controller type=&#x27;usb&#x27; index=&#x27;0&#x27; model=&#x27;ich9-uhci2&#x27;&gt; &lt;master startport=&#x27;2&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x05&#x27; function=&#x27;0x1&#x27;/&gt; &lt;/controller&gt; &lt;controller type=&#x27;usb&#x27; index=&#x27;0&#x27; model=&#x27;ich9-uhci3&#x27;&gt; &lt;master startport=&#x27;4&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x05&#x27; function=&#x27;0x2&#x27;/&gt; &lt;/controller&gt; &lt;controller type=&#x27;pci&#x27; index=&#x27;0&#x27; model=&#x27;pci-root&#x27;/&gt; &lt;controller type=&#x27;virtio-serial&#x27; index=&#x27;0&#x27;&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x06&#x27; function=&#x27;0x0&#x27;/&gt; &lt;/controller&gt; &lt;interface type=&#x27;direct&#x27;&gt; &lt;mac address=&#x27;52:54:00:36:95:f0&#x27;/&gt; &lt;source dev=&#x27;eno1&#x27; mode=&#x27;bridge&#x27;/&gt; &lt;model type=&#x27;virtio&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x03&#x27; function=&#x27;0x0&#x27;/&gt; &lt;/interface&gt; &lt;interface type=&#x27;network&#x27;&gt; &lt;mac address=&#x27;52:54:00:2d:87:99&#x27;/&gt; &lt;source network=&#x27;eno2_sriov_pool&#x27;/&gt; &lt;model type=&#x27;virtio&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x04&#x27; function=&#x27;0x0&#x27;/&gt; &lt;/interface&gt; &lt;serial type=&#x27;pty&#x27;&gt; &lt;target type=&#x27;isa-serial&#x27; port=&#x27;0&#x27;&gt; &lt;model name=&#x27;isa-serial&#x27;/&gt; &lt;/target&gt; &lt;/serial&gt; &lt;console type=&#x27;pty&#x27;&gt; &lt;target type=&#x27;serial&#x27; port=&#x27;0&#x27;/&gt; &lt;/console&gt; &lt;channel type=&#x27;unix&#x27;&gt; &lt;target type=&#x27;virtio&#x27; name=&#x27;org.qemu.guest_agent.0&#x27;/&gt; &lt;address type=&#x27;virtio-serial&#x27; controller=&#x27;0&#x27; bus=&#x27;0&#x27; port=&#x27;1&#x27;/&gt; &lt;/channel&gt; &lt;channel type=&#x27;spicevmc&#x27;&gt; &lt;target type=&#x27;virtio&#x27; name=&#x27;com.redhat.spice.0&#x27;/&gt; &lt;address type=&#x27;virtio-serial&#x27; controller=&#x27;0&#x27; bus=&#x27;0&#x27; port=&#x27;2&#x27;/&gt; &lt;/channel&gt; &lt;input type=&#x27;tablet&#x27; bus=&#x27;usb&#x27;&gt; &lt;address type=&#x27;usb&#x27; bus=&#x27;0&#x27; port=&#x27;1&#x27;/&gt; &lt;/input&gt; &lt;input type=&#x27;mouse&#x27; bus=&#x27;ps2&#x27;/&gt; &lt;input type=&#x27;keyboard&#x27; bus=&#x27;ps2&#x27;/&gt; &lt;graphics type=&#x27;spice&#x27; autoport=&#x27;yes&#x27;&gt; &lt;listen type=&#x27;address&#x27;/&gt; &lt;image compression=&#x27;off&#x27;/&gt; &lt;/graphics&gt; &lt;video&gt; &lt;model type=&#x27;qxl&#x27; ram=&#x27;65536&#x27; vram=&#x27;65536&#x27; vgamem=&#x27;16384&#x27; heads=&#x27;1&#x27; primary=&#x27;yes&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x02&#x27; function=&#x27;0x0&#x27;/&gt; &lt;/video&gt; &lt;redirdev bus=&#x27;usb&#x27; type=&#x27;spicevmc&#x27;&gt; &lt;address type=&#x27;usb&#x27; bus=&#x27;0&#x27; port=&#x27;2&#x27;/&gt; &lt;/redirdev&gt; &lt;redirdev bus=&#x27;usb&#x27; type=&#x27;spicevmc&#x27;&gt; &lt;address type=&#x27;usb&#x27; bus=&#x27;0&#x27; port=&#x27;3&#x27;/&gt; &lt;/redirdev&gt; &lt;memballoon model=&#x27;none&#x27;/&gt; &lt;rng model=&#x27;virtio&#x27;&gt; &lt;backend model=&#x27;random&#x27;&gt;/dev/urandom&lt;/backend&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x09&#x27; function=&#x27;0x0&#x27;/&gt; &lt;/rng&gt; &lt;/devices&gt;&lt;/domain&gt; 上述参数整理自《Cisco CSR 1000v and Cisco ISRv Software Configuration Guide》 完成上述 XML 文件的编辑后，执行 12345cd /etc/libvirtd/qemuvirsh define csr1kv-1.xmlvirsh start csr1kv-1 （6）在 KVM 主机上访问 CSR1000v 的 Console在 virt-manager 创建 CSR1000v 虚拟机的时候，缺省会添加一个 Serial Device。 12345[root@centos7 qemu]# virsh console 20Connected to domain CSR1000v-1Escape character is ^] CSR 1000v 暂时不能通过 Console 配置，需要通过 virt-manager 的图形化界面进行初始化配置。 vCloud 的 Console 访问正常。 （7）检验 CSR1000v 的调优配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105[root@centos7 ~]# virsh listId Name State---------------------------------------------------- 6 csr1kv-1 running[root@centos7 ~]# virsh vcpuinfo 6VCPU: 0CPU: 1State: runningCPU time: 34.5sCPU Affinity: -y----------------------------------------------------------------------VCPU: 1CPU: 2State: runningCPU time: 32.0sCPU Affinity: --y---------------------------------------------------------------------VCPU: 2CPU: 3State: runningCPU time: 23.8sCPU Affinity: ---y--------------------------------------------------------------------VCPU: 3CPU: 4State: runningCPU time: 19.8sCPU Affinity: ----y-------------------------------------------------------------------VCPU: 4CPU: 5State: runningCPU time: 23.0sCPU Affinity: -----y------------------------------------------------------------------VCPU: 5CPU: 6State: runningCPU time: 23.4sCPU Affinity: ------y-----------------------------------------------------------------VCPU: 6CPU: 7State: runningCPU time: 28.5sCPU Affinity: -------y----------------------------------------------------------------VCPU: 7CPU: 8State: runningCPU time: 28.2sCPU Affinity: --------y--------------------------------------------------------------- 以上显示 CSR1000v 虚拟机的 CPU 亲和性在 1-8 核上。 123456789101112131415161718192021[root@centos7 ~]# numastat -c qemu-kvmPer-node process memory usage (in MBs) for PID 46895 (qemu-kvm) Node 0 Node 1 Total ------ ------ -----Huge 8192 0 8192Heap 118 0 118Stack 0 0 0Private 144 7 151------- ------ ------ -----Total 8455 7 8461 以上显示，内存主要使用 Node 0。 123456789[root@centos7 ~]# numastat -vm -p 13428 | grep HugePageAnonHugePages 956.00 494.00 1450.00HugePages_Total 16384.00 16384.00 32768.00HugePages_Free 8192.00 16384.00 24576.00HugePages_Surp 0.00 0.00 0.00 （8）在 CSR1KV 上检查虚拟网卡1234567891011csr1kv-1#show platform software vnic-if interface-mapping------------------------------------------------------------- Interface Name Driver Name Mac Addr------------------------------------------------------------- GigabitEthernet2 net_ixgbe_vf 5254.002d.8799 GigabitEthernet1 net_virtio 5254.0036.95f0 上述驱动名显示为 net_ixgbe_vf 表明该虚拟网卡是一个 SR-IOV 池中分配的 VF 设备。 Linux 上抓取虚拟网卡的报文（参考）查找虚拟机的网卡列表 1234567891011[root@centos7 ~]# virsh domiflist 10Interface Type Source Model MAC-------------------------------------------------------vnet0 network default virtio 52:54:00:38:71:4amacvtap0 direct eno1 virtio 52:54:00:b2:70:90vnet5 bridge net-br1 virtio 52:54:00:69:93:23 抓取 vnet5 的报文 123456789[root@centos7 ~]# tcpdump -i vnet5 -w ping.pcaptcpdump: listening on vnet5, link-type EN10MB (Ethernet), capture size 262144 bytes^C49 packets captured51 packets received by filter0 packets dropped by kernel 注：VF 如果被分配给虚拟机，那么在 Linux 主机里，通过 ip link 则查看不到该设备，无法通过上述办法抓包。 CSR 1000v 的初始化配置及 Smart License 注册（1）CSR 1000v 初始化配置示例部分节略，其他为缺省配置。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081CSR1000v-1#show sdwan running-configsystem system-ip 1.1.10.1 site-id 101 sp-organization-name CiscoBJ organization-name CiscoBJ vbond 10.75.58.51 port 12346!hostname CSR1000v-1username admin privilege 15 secret 9 $9$4&#x2F;QL2V2K4&#x2F;6H1k$XUmRNf.T7t3KDOj&#x2F;FmoNexpEypCxr482dExXHDnohSIip name-server 64.104.123.245ip route 0.0.0.0 0.0.0.0 10.75.59.1interface GigabitEthernet1 no shutdown arp timeout 1200 ip address 10.75.59.35 255.255.255.0 no ip redirects ip mtu 1500 mtu 1500 negotiation autoexitinterface Tunnel1 no shutdown ip unnumbered GigabitEthernet1 no ip redirects ipv6 unnumbered GigabitEthernet1 no ipv6 redirects tunnel source GigabitEthernet1 tunnel mode sdwanexitclock timezone CST 8 0ntp server 10.75.58.1 version 4sdwan interface GigabitEthernet1 tunnel-interface encapsulation ipsec allow-service sshd exit （2）常用命令 show sdwan control local-properties show sdwan control connections show sdwan control connection-history show sdwan running-config show sdwan bfd sessions show sdwan omp peers show sdwan omp routes （3）CSR 1000v Smart License 注册CSR 1000v 默认限速为 250Mbps，需要注册 Smart License 才可解开限速。 123CSR1000v-2#show platform hardware throughput levelThe current throughput level is 250000 kb&#x2F;s 注册 Smart License 需要满足以下条件： 1、 CSR 1000v 已经注册到 vManage，控制面连接正常； 2、 配置 ip http client source-interface GigabitEthernet2 3、 sdwan interface GigabitEthernet2 tunnel-interface allow-service all —针对 16.12.x 版本；在新版本中仅需要 allow https 4、 CSR 1000v 可以访问 URL：https://tools.cisco.com/its/service/oddce/services/DDCEService 允许访问 114.114.114.114，CSR 1000v 可解析域名 tools.cisco.com 替代办法，增加一条命令 ip host tools.cisco.com 72.163.4.38 执行 license smart register idtoken xxxxxx 进行注册 show license status 查看注册结果 注册完成后，系统解开限速： 123CSR1000v-1#show platform hardware throughput levelThe current throughput level is 200000000 kb&#x2F;s 性能和相关限制（1）SRIOV 的性能在上述 CSR1KV-1 安装好后，使用测试仪进行性能测试，测试条件中，设置丢包率为 0%，其性能如下： Packet Site SDWAN Performance (Mbps) CEF Performance (Mbps) 128Byte 800 2843.75 256Byte 1431.26 6500.00 512Byte 2581.26 10578.13 1024Byte 3731.26 15500.00 1400Byte 4306.26 18171.88 注：上述测试结果非官方结果，不同服务器和网卡可能测试结果有区别，上述性能数据仅供参考。 （2）SRIOV 的限制SRIOV 的主要限制是每一个 VF 设备支持的 VLAN 数，ixgbevf 所支持的最大 VLAN 数为 64；因此，在 CSR1KV 中对应的虚拟接口配置的活跃子接口数最大为 64。 配置指南中有关于 SRIOV 子接口限制的说明： Cisco CSR 1000v and Cisco ISRv Software Configuration Guide: SR-IOV (ixgbevf) Maximum VLANs: The maximum number of VLANs supported on PF is 64. Together, all VFs can have a total of 64 VLANs. (Intel limitation.) SR-IOV (i40evf) Maximum VLANs: The maximum number of VLANs supported on PF is 512. Together, all VFs can have a total of 512 VLANs. (Intel limitation.) Per-VF resources are managed by the PF (host) device driver. 附录（1）Virt-manager 设置虚机的 CPU 模式说明Libvirt 主要支持三种 CPU mode： host-passthrough: libvirt 令 KVM 把宿主机的 CPU 指令集全部透传给虚拟机。因此虚拟机能够最大限度的使用宿主机 CPU 指令集，故性能是最好的。但是在热迁移时，它要求目的节点的 CPU 和源节点的一致。 host-model: libvirt 根据当前宿主机 CPU 指令集从配置文件 /usr/share/libvirt/cpu_map.xml 选择一种最相配的 CPU 型号。在这种 mode 下，虚拟机的指令集往往比宿主机少，性能相对 host-passthrough 要差一点，但是热迁移时，它允许目的节点 CPU 和源节点的存在一定的差异。 custom: 这种模式下虚拟机 CPU 指令集数最少，故性能相对最差，但是它在热迁移时跨不同型号 CPU 的能力最强。此外，custom 模式下支持用户添加额外的指令集。 三种 mode 的性能排序是：host-passthrough &gt; host-model &gt; custom 实际性能差异不大：100%&gt; 95.84%&gt;94.73% 引自：http://wsfdl.com/openstack/2018/01/02/libvirt_cpu_mode.html （2）有关网卡模式的说明使用 virt-manager 创建虚拟机，在添加网卡时，有 3 中选择，分别是 e1000, rtl8139, virtio。 “rtl8139”这个网卡模式是 qemu-kvm 默认的模拟网卡类型，RTL8139 是 Realtek 半导体公司的一个 10/100M 网卡系列，是曾经非常流行（当然现在看来有点古老）且兼容性好的网卡，几乎所有的现代操作系统都对 RTL8139 网卡驱动的提供支持。 “e1000”系列提供 Intel e1000 系列的网卡模拟，纯的 QEMU（非 qemu-kvm）默认就是提供 Intel e1000 系列的虚拟网卡。 “virtio” 类型是 qemu-kvm 对半虚拟化 IO（virtio）驱动的支持。 这三个网卡的最大区别(此处指最需要关注的地方)是速度： rtl8139 10/100Mb/s e1000 1Gb/s virtio 10Gb/s 注意 virtio 是唯一可以达到 10Gb/s 的。 virtio 是一种 I/O 半虚拟化解决方案，是一套通用 I/O 设备虚拟化的程序，是对半虚拟化 Hypervisor 中的一组通用 I/O 设备的抽象。提供了一套上层应用与各 Hypervisor 虚拟化设备（KVM，Xen，VMware 等）之间的通信框架和编程接口，减少跨平台所带来的兼容性问题，大大提高驱动程序开发效率。 （3）有关 MACVTAP以下内容来自：https://www.ibm.com/developerworks/cn/linux/1312_xiawc_linuxvirtnet/index.html MACVTAP 的实现基于传统的 MACVLAN。和 TAP 设备一样，每一个 MACVTAP 设备拥有一个对应的 Linux 字符设备，并拥有和 TAP 设备一样的 IOCTL 接口，因此能直接被 KVM/Qemu 使用，方便地完成网络数据交换工作。引入 MACVTAP 设备的目标是：简化虚拟化环境中的交换网络，代替传统的 Linux TAP 设备加 Bridge 设备组合，同时支持新的虚拟化网络技术，如 802.1 Qbg。 MACVTAP 设备和 VLAN 设备类似，是以一对多的母子关系出现的。在一个母设备上可以创建多个 MACVTAP 子设备，一个 MACVTAP 设备只有一个母设备，MACVTAP 子设备可以做为母设备，再一次嵌套的创建 MACVTAP 子设备。母子设备之间被隐含的桥接起来，母设备相当于现实世界中的交换机 TRUNK 口。实际上当 MACVTAP 设备被创建并且模式不为 Passthrough 时，内核隐含的创建了 MACVLAN 网络，完成转发功能。MACVTAP 设备有四种工作模式：Bridge、VEPA、Private，Passthrough。 Bridge 模式下，它完成与 Bridge 设备类似功能，数据可以在属于同一个母设备的子设备间交换转发，虚拟机相当于简单接入了一个交换机。当前的 Linux 实现有一个缺陷，此模式下 MACVTAP 子设备无法和 Linux Host 通讯，即虚拟机无法和 Host 通讯。—-经验证，属实。 Passthrough 模式下，内核的 MACVLAN 数据处理逻辑被跳过，硬件决定数据如何处理，从而释放了 Host CPU 资源。 MACVTAP Passthrough 概念与 PCI Passthrough 概念不同，上图详细解释了两种情况的区别。 PCI Passthrough 针对的是任意 PCI 设备，不一定是网络设备，目的是让 Guest OS 直接使用 Host 上的 PCI 硬件以提高效率。以 X86 平台为例，数据将通过需要硬件支持的 VT-D 技术从 Guest OS 直接传递到 Host 硬件上。这样做固然效率很高，但因为模拟器失去了对虚拟硬件的控制，难以同步不同 Host 上的硬件状态，因此当前在使用 PCI Passthrough 的情况下难以做动态迁移。 MACVTAP Passthrough 仅仅针对 MACVTAP 网络设备，目的是绕过内核里 MACVTAP 的部分软件处理过程，转而交给硬件处理。在虚拟化条件下，数据还是会先到达模拟器 I/O 层，再转发到硬件上。这样做效率有损失，但模拟器仍然控制虚拟硬件的状态及数据的走向，可以做动态迁移。 （4）SR-IOV 介绍如果网卡支持 SRIOV，请使用 SRIOV PCI Passthrough。 软件模拟是通过 Hypervisor 层模拟虚拟网卡，实现与物理设备完全一样的接口，虚拟机操作系统无须修改就能直接驱动虚拟网卡，其最大的缺点是性能相对较差； 网卡直通支持虚拟机绕过 Hypervisor 层，直接访问物理 I/O 设备，具有最高的性能，但是在同一时刻物理 I/O 设备只能被一个虚拟机独享； SR-IOV 是 Intel 在 2007 年提出的解决虚拟化网络 I/O 的硬件技术方案，该技术不仅能够继承网卡直通的高性能优势，而且同时支持物理 I/O 设备的跨虚拟机共享，具有较好的应用前景。 原文链接：https://blog.csdn.net/lsz137105/article/details/100752930 SR-IOV（Single Root I/O Virtualization）是一个将 PCIe 设备（如网卡）共享给虚拟机的标准，通过为虚拟机提供独立的内存空间、中断、DMA 流，来绕过 VMM 实现数据访问。 SR-IOV 引入了两种 PCIe functions： PF（Physical Function）：包含完整的 PCIe 功能，包括 SR-IOV 的扩张能力，该功能用于 SR-IOV 的配置和管理。 VF（Virtual Function）：包含轻量级的 PCIe 功能。每一个 VF 有它自己独享的 PCI 配置区域，并且可能与其他 VF 共享着同一个物理资源。 SR-IOV 网卡通过将 SR-IOV 功能集成到物理网卡上，将单一的物理网卡虚拟成多个 VF 接口，每个 VF 接口都有单独的虚拟 PCIe 通道，这些虚拟的 PCIe 通道共用物理网卡的 PCIe 通道。每个虚拟机可占用一个或多个 VF 接口，这样虚拟机就可以直接访问自己的 VF 接口，而不需要 Hypervisor 的协调干预，从而大幅提升网络吞吐性能。 （5）探索虚拟机进程每一个客户机就是宿主机中的一个 QEMU 进程，而一个客户机的多个 vCPU 就是一个 QEMU 进程中的多个线程。 123[root@centos7 ~]# ps -ef | grep qemuqemu 52595 1 99 10:37 ? 01:24:12 /usr/libexec/qemu-kvm -name csr1kv-1 -S -machine pc-i440fx-rhel7.0.0,accel=kvm,usb=off,dump-guest-core=off,mem-merge=off -cpu host -m 8192 -mem-prealloc -mem-path /dev/hugepages/libvirt/qemu/7-csr1kv-1 -realtime mlock=on -smp 8,sockets=8,cores=1,threads=1 -uuid 59581018-6387-49df-ab09-2bcf40fc12ba -no-user-config -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/domain-7-csr1kv-1/monitor.sock,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc,driftfix=slew -global kvm-pit.lost_tick_policy=delay -no-hpet -no-shutdown -global PIIX4_PM.disable_s3=1 -global PIIX4_PM.disable_s4=1 -boot strict=on -device piix3-usb-uhci,id=usb,bus=pci.0,addr=0x1.0x2 -device virtio-serial-pci,id=virtio-serial0,bus=pci.0,addr=0x6 -drive file=/home/root/images/csr1000v-universalk9.17.02.01v.qcow2,format=qcow2,if=none,id=drive-virtio-disk0 -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x7,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=1 -netdev tap,fd=26,id=hostnet0,vhost=on,vhostfd=28 -device virtio-net-pci,netdev=hostnet0,id=net0,mac=52:54:00:36:95:f0,bus=pci.0,addr=0x3 -chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0 -chardev socket,id=charchannel0,path=/var/lib/libvirt/qemu/channel/target/domain-7-csr1kv-1/org.qemu.guest_agent.0,server,nowait -device virtserialport,bus=virtio-serial0.0,nr=1,chardev=charchannel0,id=channel0,name=org.qemu.guest_agent.0 -chardev spicevmc,id=charchannel1,name=vdagent -device virtserialport,bus=virtio-serial0.0,nr=2,chardev=charchannel1,id=channel1,name=com.redhat.spice.0 -spice port=5900,addr=127.0.0.1,disable-ticketing,image-compression=off,seamless-migration=on -vga qxl -global qxl-vga.ram_size=67108864 -global qxl-vga.vram_size=67108864 -global qxl-vga.vgamem_mb=16 -global qxl-vga.max_outputs=1 -device vfio-pci,host=1d:00.0,id=hostdev1,bus=pci.0,addr=0x8 -device vfio-pci,host=3b:10.1,id=hostdev0,bus=pci.0,addr=0x4 -msg timestamp=on 使用 virsh 命令或 virt-manager 开启虚拟机，是通过调用/usr/libexec/qemu-kvm 并附带虚拟配置的参数，来开启 qemu-kvm 的进程。可以看到上述的参数是非常复杂的，libvirt 提供 XML 参数进行简化。 ps -efL | grep qemu 可以列出所有的线程，但是输出篇幅很长，不在此列出；使用 pstree 可列出其父进程、线程关系，如下： virt-top 可查看虚机运行状态和资源利用率： [root@centos7 ~]# virt-top -1 参考资料连接 https://cloud.tencent.com/developer/article/1703094 https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html-single/performance_tuning_guide/index https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/7/html-single/virtualization_tuning_and_optimization_guide/index https://computingforgeeks.com/how-to-create-and-configure-bridge-networking-for-kvm-in-linux/ https://software.intel.com/content/www/us/en/develop/articles/configure-sr-iov-network-virtual-functions-in-linux-kvm.html","categories":[],"tags":[{"name":"CSR1000v","slug":"CSR1000v","permalink":"https://weiborao.link/tags/CSR1000v/"},{"name":"KVM","slug":"KVM","permalink":"https://weiborao.link/tags/KVM/"},{"name":"SRIOV","slug":"SRIOV","permalink":"https://weiborao.link/tags/SRIOV/"},{"name":"NetworkManager","slug":"NetworkManager","permalink":"https://weiborao.link/tags/NetworkManager/"},{"name":"Cisco","slug":"Cisco","permalink":"https://weiborao.link/tags/Cisco/"}]}],"categories":[],"tags":[{"name":"chatGPT","slug":"chatGPT","permalink":"https://weiborao.link/tags/chatGPT/"},{"name":"AMI","slug":"AMI","permalink":"https://weiborao.link/tags/AMI/"},{"name":"AppDynamics","slug":"AppDynamics","permalink":"https://weiborao.link/tags/AppDynamics/"},{"name":"cloud-init","slug":"cloud-init","permalink":"https://weiborao.link/tags/cloud-init/"},{"name":"traceroute","slug":"traceroute","permalink":"https://weiborao.link/tags/traceroute/"},{"name":"icmp","slug":"icmp","permalink":"https://weiborao.link/tags/icmp/"},{"name":"anycast","slug":"anycast","permalink":"https://weiborao.link/tags/anycast/"},{"name":"docker","slug":"docker","permalink":"https://weiborao.link/tags/docker/"},{"name":"tcpdump","slug":"tcpdump","permalink":"https://weiborao.link/tags/tcpdump/"},{"name":"wireshark","slug":"wireshark","permalink":"https://weiborao.link/tags/wireshark/"},{"name":"ThousandEyes","slug":"ThousandEyes","permalink":"https://weiborao.link/tags/ThousandEyes/"},{"name":"AWS","slug":"AWS","permalink":"https://weiborao.link/tags/AWS/"},{"name":"kvm","slug":"kvm","permalink":"https://weiborao.link/tags/kvm/"},{"name":"ztp","slug":"ztp","permalink":"https://weiborao.link/tags/ztp/"},{"name":"nfvis","slug":"nfvis","permalink":"https://weiborao.link/tags/nfvis/"},{"name":"sriov","slug":"sriov","permalink":"https://weiborao.link/tags/sriov/"},{"name":"csr1000v","slug":"csr1000v","permalink":"https://weiborao.link/tags/csr1000v/"},{"name":"hexo","slug":"hexo","permalink":"https://weiborao.link/tags/hexo/"},{"name":"CSR1000v","slug":"CSR1000v","permalink":"https://weiborao.link/tags/CSR1000v/"},{"name":"KVM","slug":"KVM","permalink":"https://weiborao.link/tags/KVM/"},{"name":"SRIOV","slug":"SRIOV","permalink":"https://weiborao.link/tags/SRIOV/"},{"name":"NetworkManager","slug":"NetworkManager","permalink":"https://weiborao.link/tags/NetworkManager/"},{"name":"Cisco","slug":"Cisco","permalink":"https://weiborao.link/tags/Cisco/"}]}