{"meta":{"title":"Rao Weibo的博客","subtitle":"","description":"记录一些工作、学习相关的笔记","author":"Rao Weibo","url":"https://weiborao.github.io","root":"/"},"pages":[],"posts":[{"title":"Cisco CSR1000v KVM SRIOV Setting Guide on Ubuntu 20.04 With NetworkManager","slug":"csr1kv-kvm-Ubuntu20-04-md","date":"2021-02-03T14:37:33.000Z","updated":"2021-02-05T15:06:32.660Z","comments":true,"path":"csr1kv-kvm-Ubuntu20-04-md.html","link":"","permalink":"https://weiborao.github.io/csr1kv-kvm-Ubuntu20-04-md.html","excerpt":"","text":"Last Update: February 3, 2021 Author: Rao Weibo Version: 1.0 This guide provides the steps on how to install CSR1000v/Catalyst 8000v on KVM, the setting of SRIOV and KVM tuning. Ubuntu 20.04 Installation and Settings(1) BIOS settings Configuration Recommended Setting Intel Hyper-Threading Technology Disabled Number of Enable Cores ALL Execute Disable Enabled Intel VT Enabled Intel VT-D Enabled Intel VT-D coherency support Enabled Intel VT-D ATS support Enabled CPU Performance High throughput Hardware Perfetcher Disabled Adjacent Cache Line Prefetcher Disabled DCU Streamer Prefetch Disable Power Technology Custom Enhanced Intel Speedstep Technology Disabled Intel Turbo Boost Technology Enabled Processor Power State C6 Disabled Processor Power State C1 Enhanced Disabled Frequency Poor Override Enabled P-State Coordination HW_ALL Energy Performance Performance The above settings are from the Installation guide from Cisco CSR 1000v and Cisco ISRv Software Configuration Guide.Note: some platform such as Cisco NFVIS, does not disable the Hyper-Threading. (2) Ubuntu 20.04 installation tipsIf you need the Gnome GUI, you can install the Ubuntu 20.04 Desktop.After the system bootup, ‘apparmor’ is recommended to be disabled, otherwise, you may meet permission denied when you assign SRIOV devices to the CSR1kV. You need to reboot to take effect. 12sudo systemctl disable apparmoregrep -o &#x27;(vmx|svm)&#x27; /proc/cpuinfo | sort | uniq You can find the vmx on Intel platform, or svm on AMD platform. 12sudo systemctl stop ufwsystemctl disable ufw cat /etc/sysctl.conf 1234net.ipv4.ip_forward = 1net.bridge.bridge-nf-call-ip6tables = 0net.bridge.bridge-nf-call-iptables = 0net.bridge.bridge-nf-call-arptables = 0 1$ sudo apt install qemu-kvm libvirt-clients libvirt-daemon-system bridge-utils virt-manager numactl virt-top After installation, you can check the version 123456789101112ubuntu@ubuntu-kvm:~$ libvirtd -Vlibvirtd (libvirt) 6.0.0ubuntu@ubuntu-kvm:~$ qemu-system-x86_64 --versionQEMU emulator version 4.2.1 (Debian 1:4.2-3ubuntu6.11)Copyright (c) 2003-2019 Fabrice Bellard and the QEMU Project developersubuntu@ubuntu-kvm:~$ virt-manager --version2.2.1ubuntu@ubuntu-kvm:~$ modinfo kvm-intelfilename: /lib/modules/5.4.0-62-generic/kernel/arch/x86/kvm/kvm-intel.koubuntu@ubuntu-kvm:~$ modinfo i40evffilename: /lib/modules/5.4.0-62-generic/kernel/drivers/net/ethernet/intel/iavf/iavf.koversion: 3.2.3-k (3) NIC SettingIn Ubuntu 20.04, we use NetworkManager to config the NIC, you can use nmtui on the terminal, or nmcli.First, check the netplan. cat /etc/netplan/ 00-installer-config.yaml Edit the yaml file: 1234# Let NetworkManager manage all devices on this systemnetwork: version: 2 renderer: NetworkManager 1sudo netplan apply Then you can set the network config with nmcli or nmtui. 123456789101112131415161718192021222324252627282930313233ubuntu@ubuntu-kvm:~$ sudo nmcli connection modify netplan-eno1 ipv4.method manual ipv4.addresses 10.75.59.50/24 ipv4.gateway 10.75.59.1 ipv4.dns 64.104.123.245ubuntu@ubuntu-kvm:~$ sudo nmcli connection modify netplan-eno1 connection.id eno1ubuntu@ubuntu-kvm:~$ sudo nmcli connection up eno1# Change the connections name to the devices name.sudo nmcli connectionubuntu@ubuntu-kvm:~$ sudo nmcli connectionNAME UUID TYPE DEVICEeno1 10838d80-caeb-349e-ba73-08ed16d4d666 ethernet eno1enp216s0f0 6556c191-c253-3a5e-b440-c5b071ec29a4 ethernet enp216s0f0enp216s0f1 8080672c-5784-375f-8eb9-a6ef57cbd4f7 ethernet enp216s0f1ens1f0 58fb5b1f-c10a-3e7a-9ab9-a8c449840ce6 ethernet ens1f0ens1f1 2a06a9d9-b761-3bdf-aa0b-3d44fff2158f ethernet ens1f1virbr0 ca7d1e11-a82f-429c-9d91-fc985776232c bridge virbr0#Set the MTU and disable ipv4 for the NIC to enable SRIOV. sudo nmcli connection modify ens1f0 ethernet.mtu 9216sudo nmcli connection modify ens1f0 ipv4.method disabledsudo nmcli connection up ens1f0sudo ip link show ens1f02: ens1f0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9216 qdisc mq state UP mode DEFAULT group default qlen 1000link/ether 40:a6:b7:0f:a7:74 brd ff:ff:ff:ff:ff:ff# Note: This value 9216 of MTU is derived from Cisco NFVIS.CSP5228-1# show pnic-detail mtuName MTU=============================eth0-1 9216eth0-2 9216eth1-1 9216eth1-2 9216 SR-IOV Configuration(1) Check the NIC to support SR-IOVCheck the NIC hardware infor. 1234567891011sudo lshw -c network -businfoBus info Device Class Description========================================================pci@0000:3b:00.0 eno1 network Ethernet Controller 10G X550Tpci@0000:3b:00.1 eno2 network Ethernet Controller 10G X550Tpci@0000:5e:00.0 ens1f0 network Ethernet Controller XXV710 for 25GbE SFP28pci@0000:5e:00.1 ens1f1 network Ethernet Controller XXV710 for 25GbE SFP28pci@0000:d8:00.0 enp216s0f0 network Ethernet Controller XXV710 for 25GbE SFP28pci@0000:d8:00.1 enp216s0f1 network Ethernet Controller XXV710 for 25GbE SFP28 Check the capabilities of NIC 1234567sudo lspci -vv -s 5e:00.0 | grep -A 5 -i SR-IOV Capabilities: [160 v1] Single Root I/O Virtualization (SR-IOV) IOVCap: Migration-, Interrupt Message Number: 000 IOVCtl: Enable+ Migration- Interrupt- MSE+ ARIHierarchy+ IOVSta: Migration- Initial VFs: 64, Total VFs: 64, Number of VFs: 2, Function Dependency Link: 00 VF offset: 16, stride: 1, Device ID: 154c (2) Change the GRUB parameters12sudo vi /etc/default/grub GRUB_CMDLINE_LINUX_DEFAULT=&quot;quiet hugepagesz=1G hugepages=64 default_hugepagesz=1G intel_iommu=on iommu=pt isolcpus=1-8,45-52&quot; Note：The hugepages should be no more than the physical memory, otherwise, it will fail to bootup. default_hugepagesz=1G hugepagesz=1G hugepages=64 will allocate 64 1GB huge pages at boot, which are static huge pages. CSR 1000v will use these static huge pages for best performance. isolcpus=1-8,45-52 will isolate the CPUs cores reserved for CSR 1000v, prevent other processes running on these cores, this can reduce latency for CSR 1000v. You can refer to [Check the capability of the platform][1] to get the CPU information. 1sudo update-grub After reboot, check the /proc/cmdline. 123456789cat /proc/cmdline |grep intel_iommu=ondmesg |grep -e DMAR -e IOMMUdmesg | grep -e DMAR -e IOMMU -e AMD-Viubuntu@ubuntu-kvm:~$ cat /proc/cmdline |grep intel_iommu=onBOOT_IMAGE=/vmlinuz-5.4.0-62-generic root=/dev/mapper/ubuntu--vg-ubuntu--lv ro quiet hugepagesz=1G hugepages=64 default_hugepagesz=1G intel_iommu=on iommu=pt isolcpus=1-8,45-52ubuntu@ubuntu-kvm:~ $ dmesg |grep -e DMAR -e IOMMU[ 0.020313] ACPI: DMAR 0x000000005DA8EB80 000270 (v01 Cisco0 CiscoUCS 00000001 INTL 20091013)[ 1.954441] DMAR: IOMMU enabled Check the CPU info, all the CPU cores and isolated CPU cores. 1234ubuntu@ubuntu-kvm:~$ cat /sys/devices/system/cpu/isolated1-8,45-52ubuntu@ubuntu-kvm:~$ cat /sys/devices/system/cpu/present0-87 (3) VFs PersistenceNetworkManager way: 123456789# nmcli can set the SRIOV, as follow:sudo nmcli connection modify ens1f0 sriov.total-vfs 2sudo nmcli connection modify ens1f1 sriov.total-vfs 2# We can set the MAC address and set the trust on:sudo nmcli connection modify ens1f0 sriov.vfs &#x27;0 mac=b6:4f:02:37:5a:d8 trust=true, 1 mac=26:04:1d:1f:3d:a9 trust=true&#x27;sudo nmcli connection modify ens1f1 sriov.vfs &#x27;0 mac=76:6c:4e:16:7f:e2 trust=true, 1 mac=6a:f2:bd:97:71:65 trust=true&#x27;sudo nmcli connection up ens1f0sudo nmcli connection up ens1f1 After reboot, check the dmesg. 1234567891011121314151617sudo dmesg | grep -i vf[ 6.599304] i40e 0000:5e:00.0: Allocating 2 VFs.[ 6.680111] i40e 0000:5e:00.1: Allocating 2 VFs.[ 6.730167] iavf: Intel(R) Ethernet Adaptive Virtual Function Network Driver - version 3.2.3-k&lt;&lt; snip &gt;&gt;[ 17.931493] i40e 0000:5e:00.0: Setting MAC b6:4f:02:37:5a:d8 on VF 0[ 18.111781] i40e 0000:5e:00.0: VF 0 is now trusted[ 18.112559] i40e 0000:5e:00.0: Setting MAC 26:04:1d:1f:3d:a9 on VF 1[ 18.291464] i40e 0000:5e:00.0: VF 1 is now trusted[ 18.292231] i40e 0000:5e:00.1: Setting MAC 76:6c:4e:16:7f:e2 on VF 0[ 18.475259] i40e 0000:5e:00.1: VF 0 is now trusted[ 18.475929] i40e 0000:5e:00.1: Setting MAC 6a:f2:bd:97:71:65 on VF 1[ 18.659465] i40e 0000:5e:00.1: VF 1 is now trusted[ 18.728124] iavf 0000:5e:02.0 ens1f0v0: NIC Link is Up Speed is 25 Gbps Full Duplex[ 18.752275] iavf 0000:5e:02.1 ens1f0v1: NIC Link is Up Speed is 25 Gbps Full Duplex[ 18.776329] iavf 0000:5e:0a.0 ens1f1v0: NIC Link is Up Speed is 25 Gbps Full Duplex[ 18.800569] iavf 0000:5e:0a.1 ens1f1v1: NIC Link is Up Speed is 25 Gbps Full Duplex (4) Check the VFsYou can check the VFs from lspci or ip link commands. 12ubuntu@ubuntu-kvm:~$ sudo lspci | grep -i Virtualubuntu@ubuntu-kvm:~$ sudo ip link show | grep -B2 vf Good news is that the VFs names are well related to the physical NICs. For example, ens1f0v1 is one of the VFs of the physical NIC ens1f0 . 123456789ubuntu@ubuntu-kvm:~$ ip link show | grep -E ens1f[0,1]v[0,1] -A 19: ens1f0v1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000 link/ether 26:04:1d:1f:3d:a9 brd ff:ff:ff:ff:ff:ff10: ens1f1v1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000 link/ether 6a:f2:bd:97:71:65 brd ff:ff:ff:ff:ff:ff15: ens1f0v0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000 link/ether b6:4f:02:37:5a:d8 brd ff:ff:ff:ff:ff:ff16: ens1f1v0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000link/ether 76:6c:4e:16:7f:e2 brd ff:ff:ff:ff:ff:ff We can change the MTU to 9216 (or you may not need 9216, 1504 is enough). 12345678sudo nmcli connection modify ens1f0v0 ethernet.mtu 9216 ipv4.method disabledsudo nmcli connection modify ens1f0v1 ethernet.mtu 9216 ipv4.method disabledsudo nmcli connection modify ens1f1v0 ethernet.mtu 9216 ipv4.method disabledsudo nmcli connection modify ens1f1v1 ethernet.mtu 9216 ipv4.method disabledsudo nmcli connection up ens1f0v0 sudo nmcli connection up ens1f0v1sudo nmcli connection up ens1f1v0sudo nmcli connection up ens1f1v1 The above configs will survive after reboot. The above configs will survive after reboot. Create SR-IOV Virtual Network Adapter PoolUsing this method, KVM creates a pool of network devices that can be inserted into VMs, and the size of that pool is determined by how many VFs were created on the physical function when they were initialized. (1) Create an xml fileubuntu@ubuntu-kvm:~/kvm$ cat ens1f0_sriov_pool.xml 123456&lt;network&gt; &lt;name&gt;ens1f0_sriov_pool&lt;/name&gt; &lt;!-- This is the name of the file you created --&gt; &lt;forward mode=&#x27;hostdev&#x27; managed=&#x27;yes&#x27;&gt; &lt;pf dev=&#x27;ens1f0&#x27;/&gt; &lt;!-- Use the netdev name of your SR-IOV devices PF here --&gt; &lt;/forward&gt;&lt;/network&gt; (2) Define a network and set to auto start.1234virsh net-define ens1f0_sriov_pool.xmlSleep 1virsh net-start ens1f0_sriov_poolvirsh net-autostart ens1f0_sriov_pool ubuntu@ubuntu-kvm:~/kvm$ virsh net-dumpxml ens1f0_sriov_pool 123456789&lt;network connections=&#x27;1&#x27;&gt; &lt;name&gt;ens1f0_sriov_pool&lt;/name&gt; &lt;uuid&gt;9125a600-6620-4ab6-9a47-8844a61b0918&lt;/uuid&gt; &lt;forward mode=&#x27;hostdev&#x27; managed=&#x27;yes&#x27;&gt; &lt;pf dev=&#x27;ens1f0&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x5e&#x27; slot=&#x27;0x02&#x27; function=&#x27;0x0&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x5e&#x27; slot=&#x27;0x02&#x27; function=&#x27;0x1&#x27;/&gt; &lt;/forward&gt;&lt;/network&gt; (3) Select the virtual adapter from the poolIt is simple to add an SR-IOV NIC, as follow: This is equivalent to the following XML: 123456&lt;interface type=&#x27;network&#x27;&gt; &lt;mac address=&#x27;52:54:00:0b:a5:2e&#x27;/&gt; &lt;source network=&#x27;ens1f0_sriov_pool&#x27;/&gt; &lt;model type=&#x27;virtio&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x02&#x27; slot=&#x27;0x00&#x27; function=&#x27;0x0&#x27;/&gt;&lt;/interface&gt; After the VM powered on, the network info is as follow:virsh dumpxml CSR1KV-1 12345678910&lt;interface type=&#x27;hostdev&#x27; managed=&#x27;yes&#x27;&gt; &lt;mac address=&#x27;52:54:00:0b:a5:2e&#x27;/&gt; &lt;driver name=&#x27;vfio&#x27;/&gt; &lt;source&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x5e&#x27; slot=&#x27;0x02&#x27; function=&#x27;0x0&#x27;/&gt; &lt;/source&gt; &lt;model type=&#x27;virtio&#x27;/&gt; &lt;alias name=&#x27;hostdev0&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x02&#x27; slot=&#x27;0x00&#x27; function=&#x27;0x0&#x27;/&gt;&lt;/interface&gt; Create the CSR1KV VM from virt-managerFrom the virt-manager, create a CSR1KV virtual machine step by step, and choose the virtual network interface from the SR-IOV pool. Note: The first interface of csr1kv-1 is configured to macvtap Bridge mode, so you do not need to create a Linux bridge. However, csr1kv-1 can not communicate to the Linux host through this interface, but it can go out of the Linux host through the eno1. This is a known issue with macvtap. After select the Virtual Network Interface, click the Begin Installation, and you can shut down the virtual machine.The actions above will create the csr1kv-1.xml file under the directory: /etc/libvirtd/qemu/ KVM Performance TunningKVM performance tuning are related to NUMA, Memory Hugepage and vCPU pinning. The main reference is Redhat Linux 7 PERFORMANCE TUNING GUIDE We will use virsh edit csr1kv-1 to do the performance tuning. (1) Check the capability of the platform123456789ubuntu@ubuntu-kvm:~$ virsh nodeinfoCPU model: x86_64CPU(s): 88CPU frequency: 1000 MHzCPU socket(s): 1Core(s) per socket: 22Thread(s) per core: 2NUMA cell(s): 2Memory size: 394929928 KiB [1]:ubuntu@ubuntu-kvm:~$ virsh capabilities 123456789101112&lt;capabilities&gt; &lt;host&gt; &lt;uuid&gt;a24f9760-48f1-f34e-a001-a848f08df7bb&lt;/uuid&gt; &lt;cpu&gt; &lt;arch&gt;x86_64&lt;/arch&gt; &lt;model&gt;Cascadelake-Server-noTSX&lt;/model&gt; &lt;vendor&gt;Intel&lt;/vendor&gt; &lt;microcode version=&#x27;83898371&#x27;/&gt; &lt;counter name=&#x27;tsc&#x27; frequency=&#x27;2095078000&#x27; scaling=&#x27;no&#x27;/&gt; &lt;topology sockets=&#x27;1&#x27; cores=&#x27;22&#x27; threads=&#x27;2&#x27;/&gt;... From the outputs, we can get the cores of CPU, the NUMA info and the hugepages. (2) NUMA Info123456789101112ubuntu@ubuntu-kvm:~$ numactl --hardwareavailable: 2 nodes (0-1)node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65node 0 size: 192172 MBnode 0 free: 152956 MBnode 1 cpus: 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87node 1 size: 193500 MBnode 1 free: 158037 MBnode distances:node 0 1 0: 10 21 1: 21 10 Two CPUs, each has 192GB memory, the NUMA node are node 0 and node 1. You can also check the NUMA info of the NIC, as follow: 12345678910111213ubuntu@ubuntu-kvm:~$ sudo lspci -vv | grep Ethernet -A 65e:00.0 Ethernet controller: Intel Corporation Ethernet Controller XXV710 for 25GbE SFP28 (rev 02) Subsystem: Cisco Systems Inc Ethernet Network Adapter XXV710 Control: I/O- Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr+ Stepping- SERR+ FastB2B- DisINTx+ Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast &gt;TAbort- &lt;TAbort- &lt;MAbort- &gt;SERR- &lt;PERR- INTx- Latency: 0, Cache Line Size: 32 bytes Interrupt: pin A routed to IRQ 137 NUMA node: 0#Or you can find the numa infor from udevadm.ubuntu@ubuntu-kvm:~$ sudo udevadm info -ap /sys/class/net/ens1f0 | grep numa ATTRS&#123;numa_node&#125;==&quot;0&quot; ATTRS&#123;numa_node&#125;==&quot;0&quot; If the memory and NICs are all on numa_node 0, that would be more efficient. Please check the server’s PCI-E slots mapping with the CPU slots. (3) HugePage and Transparent HugepageTo check the memory info 12345678910ubuntu@ubuntu-kvm:~$ cat /proc/meminfo | grep HugeAnonHugePages: 133120 kBShmemHugePages: 0 kBFileHugePages: 0 kBHugePages_Total: 64HugePages_Free: 56HugePages_Rsvd: 0HugePages_Surp: 0Hugepagesize: 1048576 kBHugetlb: 67108864 kB You can change the hugepages on the run time： 12345ubuntu@ubuntu-kvm:~$ cat /sys/devices/system/node/node0/hugepages/hugepages-1048576kB/nr_hugepages32sudo su - // switch to root user.root@ubuntu-kvm:~# echo 64 &gt; /sys/devices/system/node/node0/hugepages/hugepages-1048576kB/nr_hugepagesroot@ubuntu-kvm:~# echo 64 &gt; /sys/devices/system/node/node1/hugepages/hugepages-1048576kB/nr_hugepages Check and configure the transparent_hugepage: 123root@ubuntu-kvm:~# echo always &gt; /sys/kernel/mm/transparent_hugepage/enabledcat /sys/kernel/mm/transparent_hugepage/enabled[always] madvise never (4) vCPU PinningvCPU pinning can improve the cache meet rate and improve the performance.You can check the vcpu pinning.virsh vcpuinfo csr1kv-1 (5) Edit the XML file of the CSR1KVRun virsh edit csr1kv-1 to edit the parameters.Please pay attention to the following texts, other parameters might be different. 123456789101112131415161718192021222324&lt;memoryBacking&gt; &lt;hugepages&gt; &lt;page size=&#x27;1048576&#x27; unit=&#x27;KiB&#x27;/&gt; &lt;/hugepages&gt; &lt;locked/&gt; &lt;nosharepages/&gt;&lt;/memoryBacking&gt;&lt;vcpu placement=&#x27;static&#x27;&gt;8&lt;/vcpu&gt;&lt;cputune&gt; &lt;vcpupin vcpu=&#x27;0&#x27; cpuset=&#x27;1&#x27;/&gt; &lt;vcpupin vcpu=&#x27;1&#x27; cpuset=&#x27;2&#x27;/&gt; &lt;vcpupin vcpu=&#x27;2&#x27; cpuset=&#x27;3&#x27;/&gt; &lt;vcpupin vcpu=&#x27;3&#x27; cpuset=&#x27;4&#x27;/&gt; &lt;vcpupin vcpu=&#x27;4&#x27; cpuset=&#x27;5&#x27;/&gt; &lt;vcpupin vcpu=&#x27;5&#x27; cpuset=&#x27;6&#x27;/&gt; &lt;vcpupin vcpu=&#x27;6&#x27; cpuset=&#x27;7&#x27;/&gt; &lt;vcpupin vcpu=&#x27;7&#x27; cpuset=&#x27;8&#x27;/&gt; &lt;emulatorpin cpuset=&#x27;45-52&#x27;/&gt; &lt;!-- If Hyper-threading were enabled --&gt;&lt;/cputune&gt;&lt;numatune&gt; &lt;memory mode=&#x27;strict&#x27; nodeset=&#x27;0&#x27;/&gt;&lt;/numatune&gt;&lt;cpu mode=&#x27;host-passthrough&#x27; check=&#x27;none&#x27;/&gt; &lt;memballoon model=&#x27;none&#x27;/&gt; Please note that, if hyper-threading is enabled in BIOS setting, the parameter “emulatorpin” should be set, the cpuset are from the “virsh capabilities”, for example, siblings=’1,45’. When the core 1 is pinned, core 45 should be set in emulatorpin. From the guide https://libvirt.org/formatdomain.html and https://libvirt.org/kbase/kvm-realtime.html we set the CPU and memory tuning parameters as the highlighted text. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165&lt;domain type=&#x27;kvm&#x27;&gt; &lt;name&gt;csr1kv-1&lt;/name&gt; &lt;uuid&gt;83f90c1c-f0ea-4298-bcdf-3d676de2aeb4&lt;/uuid&gt; &lt;metadata&gt; &lt;libosinfo:libosinfo xmlns:libosinfo=&quot;http://libosinfo.org/xmlns/libvirt/domain/1.0&quot;&gt; &lt;libosinfo:os id=&quot;http://centos.org/centos/7.0&quot;/&gt; &lt;/libosinfo:libosinfo&gt; &lt;/metadata&gt; &lt;memory unit=&#x27;KiB&#x27;&gt;8388608&lt;/memory&gt; &lt;currentMemory unit=&#x27;KiB&#x27;&gt;8388608&lt;/currentMemory&gt; &lt;memoryBacking&gt; &lt;hugepages&gt; &lt;page size=&#x27;1048576&#x27; unit=&#x27;KiB&#x27;/&gt; &lt;/hugepages&gt; &lt;locked/&gt; &lt;nosharepages/&gt; &lt;/memoryBacking&gt; &lt;vcpu placement=&#x27;static&#x27;&gt;8&lt;/vcpu&gt; &lt;cputune&gt; &lt;vcpupin vcpu=&#x27;0&#x27; cpuset=&#x27;1&#x27;/&gt; &lt;vcpupin vcpu=&#x27;1&#x27; cpuset=&#x27;2&#x27;/&gt; &lt;vcpupin vcpu=&#x27;2&#x27; cpuset=&#x27;3&#x27;/&gt; &lt;vcpupin vcpu=&#x27;3&#x27; cpuset=&#x27;4&#x27;/&gt; &lt;vcpupin vcpu=&#x27;4&#x27; cpuset=&#x27;5&#x27;/&gt; &lt;vcpupin vcpu=&#x27;5&#x27; cpuset=&#x27;6&#x27;/&gt; &lt;vcpupin vcpu=&#x27;6&#x27; cpuset=&#x27;7&#x27;/&gt; &lt;vcpupin vcpu=&#x27;7&#x27; cpuset=&#x27;8&#x27;/&gt; &lt;emulatorpin cpuset=&#x27;45-52&#x27;/&gt; &lt;!-- If Hyper-threading were enabled --&gt; &lt;/cputune&gt; &lt;numatune&gt; &lt;memory mode=&#x27;strict&#x27; nodeset=&#x27;0&#x27;/&gt; &lt;/numatune&gt; &lt;os&gt; &lt;type arch=&#x27;x86_64&#x27; machine=&#x27;pc-q35-4.2&#x27;&gt;hvm&lt;/type&gt; &lt;boot dev=&#x27;hd&#x27;/&gt; &lt;/os&gt; &lt;features&gt; &lt;acpi/&gt; &lt;apic/&gt; &lt;vmport state=&#x27;off&#x27;/&gt; &lt;/features&gt; &lt;cpu mode=&#x27;host-passthrough&#x27; check=&#x27;none&#x27;/&gt; &lt;clock offset=&#x27;utc&#x27;&gt; &lt;timer name=&#x27;rtc&#x27; tickpolicy=&#x27;catchup&#x27;/&gt; &lt;timer name=&#x27;pit&#x27; tickpolicy=&#x27;delay&#x27;/&gt; &lt;timer name=&#x27;hpet&#x27; present=&#x27;no&#x27;/&gt; &lt;/clock&gt; &lt;on_poweroff&gt;destroy&lt;/on_poweroff&gt; &lt;on_reboot&gt;restart&lt;/on_reboot&gt; &lt;on_crash&gt;destroy&lt;/on_crash&gt; &lt;pm&gt; &lt;suspend-to-mem enabled=&#x27;no&#x27;/&gt; &lt;suspend-to-disk enabled=&#x27;no&#x27;/&gt; &lt;/pm&gt; &lt;devices&gt; &lt;emulator&gt;/usr/bin/qemu-system-x86_64&lt;/emulator&gt; &lt;disk type=&#x27;file&#x27; device=&#x27;disk&#x27;&gt; &lt;driver name=&#x27;qemu&#x27; type=&#x27;qcow2&#x27;/&gt; &lt;source file=&#x27;/var/lib/libvirt/images/csr1000v-universalk9.17.02.01v.qcow2&#x27;/&gt; &lt;target dev=&#x27;vda&#x27; bus=&#x27;virtio&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x05&#x27; slot=&#x27;0x00&#x27; function=&#x27;0x0&#x27;/&gt; &lt;/disk&gt; &lt;controller type=&#x27;usb&#x27; index=&#x27;0&#x27; model=&#x27;qemu-xhci&#x27; ports=&#x27;15&#x27;&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x03&#x27; slot=&#x27;0x00&#x27; function=&#x27;0x0&#x27;/&gt; &lt;/controller&gt; &lt;controller type=&#x27;sata&#x27; index=&#x27;0&#x27;&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x1f&#x27; function=&#x27;0x2&#x27;/&gt; &lt;/controller&gt; &lt;controller type=&#x27;pci&#x27; index=&#x27;0&#x27; model=&#x27;pcie-root&#x27;/&gt; &lt;controller type=&#x27;pci&#x27; index=&#x27;1&#x27; model=&#x27;pcie-root-port&#x27;&gt; &lt;model name=&#x27;pcie-root-port&#x27;/&gt; &lt;target chassis=&#x27;1&#x27; port=&#x27;0x10&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x02&#x27; function=&#x27;0x0&#x27; multifunction=&#x27;on&#x27;/&gt; &lt;/controller&gt; &lt;controller type=&#x27;pci&#x27; index=&#x27;2&#x27; model=&#x27;pcie-root-port&#x27;&gt; &lt;model name=&#x27;pcie-root-port&#x27;/&gt; &lt;target chassis=&#x27;2&#x27; port=&#x27;0x11&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x02&#x27; function=&#x27;0x1&#x27;/&gt; &lt;/controller&gt; &lt;controller type=&#x27;pci&#x27; index=&#x27;3&#x27; model=&#x27;pcie-root-port&#x27;&gt; &lt;model name=&#x27;pcie-root-port&#x27;/&gt; &lt;target chassis=&#x27;3&#x27; port=&#x27;0x12&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x02&#x27; function=&#x27;0x2&#x27;/&gt; &lt;/controller&gt; &lt;controller type=&#x27;pci&#x27; index=&#x27;4&#x27; model=&#x27;pcie-root-port&#x27;&gt; &lt;model name=&#x27;pcie-root-port&#x27;/&gt; &lt;target chassis=&#x27;4&#x27; port=&#x27;0x13&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x02&#x27; function=&#x27;0x3&#x27;/&gt; &lt;/controller&gt; &lt;controller type=&#x27;pci&#x27; index=&#x27;5&#x27; model=&#x27;pcie-root-port&#x27;&gt; &lt;model name=&#x27;pcie-root-port&#x27;/&gt; &lt;target chassis=&#x27;5&#x27; port=&#x27;0x14&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x02&#x27; function=&#x27;0x4&#x27;/&gt; &lt;/controller&gt; &lt;controller type=&#x27;pci&#x27; index=&#x27;6&#x27; model=&#x27;pcie-root-port&#x27;&gt; &lt;model name=&#x27;pcie-root-port&#x27;/&gt; &lt;target chassis=&#x27;6&#x27; port=&#x27;0x15&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x02&#x27; function=&#x27;0x5&#x27;/&gt; &lt;/controller&gt; &lt;controller type=&#x27;pci&#x27; index=&#x27;7&#x27; model=&#x27;pcie-root-port&#x27;&gt; &lt;model name=&#x27;pcie-root-port&#x27;/&gt; &lt;target chassis=&#x27;7&#x27; port=&#x27;0x16&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x02&#x27; function=&#x27;0x6&#x27;/&gt; &lt;/controller&gt; &lt;controller type=&#x27;pci&#x27; index=&#x27;8&#x27; model=&#x27;pcie-root-port&#x27;&gt; &lt;model name=&#x27;pcie-root-port&#x27;/&gt; &lt;target chassis=&#x27;8&#x27; port=&#x27;0x17&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x02&#x27; function=&#x27;0x7&#x27;/&gt; &lt;/controller&gt; &lt;controller type=&#x27;virtio-serial&#x27; index=&#x27;0&#x27;&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x04&#x27; slot=&#x27;0x00&#x27; function=&#x27;0x0&#x27;/&gt; &lt;/controller&gt; &lt;interface type=&#x27;direct&#x27;&gt; &lt;mac address=&#x27;52:54:00:69:ec:73&#x27;/&gt; &lt;source dev=&#x27;eno1&#x27; mode=&#x27;bridge&#x27;/&gt; &lt;model type=&#x27;virtio&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x01&#x27; slot=&#x27;0x00&#x27; function=&#x27;0x0&#x27;/&gt; &lt;/interface&gt; &lt;interface type=&#x27;network&#x27;&gt; &lt;mac address=&#x27;52:54:00:0b:a5:2e&#x27;/&gt; &lt;source network=&#x27;ens1f0_sriov_pool&#x27;/&gt; &lt;model type=&#x27;virtio&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x02&#x27; slot=&#x27;0x00&#x27; function=&#x27;0x0&#x27;/&gt; &lt;/interface&gt; &lt;interface type=&#x27;network&#x27;&gt; &lt;mac address=&#x27;52:54:00:bd:9f:54&#x27;/&gt; &lt;source network=&#x27;ens1f1_sriov_pool&#x27;/&gt; &lt;model type=&#x27;virtio&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x08&#x27; slot=&#x27;0x00&#x27; function=&#x27;0x0&#x27;/&gt; &lt;/interface&gt; &lt;serial type=&#x27;pty&#x27;&gt; &lt;target type=&#x27;isa-serial&#x27; port=&#x27;0&#x27;&gt; &lt;model name=&#x27;isa-serial&#x27;/&gt; &lt;/target&gt; &lt;/serial&gt; &lt;console type=&#x27;pty&#x27;&gt; &lt;target type=&#x27;serial&#x27; port=&#x27;0&#x27;/&gt; &lt;/console&gt; &lt;channel type=&#x27;unix&#x27;&gt; &lt;target type=&#x27;virtio&#x27; name=&#x27;org.qemu.guest_agent.0&#x27;/&gt; &lt;address type=&#x27;virtio-serial&#x27; controller=&#x27;0&#x27; bus=&#x27;0&#x27; port=&#x27;1&#x27;/&gt; &lt;/channel&gt; &lt;channel type=&#x27;spicevmc&#x27;&gt; &lt;target type=&#x27;virtio&#x27; name=&#x27;com.redhat.spice.0&#x27;/&gt; &lt;address type=&#x27;virtio-serial&#x27; controller=&#x27;0&#x27; bus=&#x27;0&#x27; port=&#x27;2&#x27;/&gt; &lt;/channel&gt; &lt;input type=&#x27;mouse&#x27; bus=&#x27;ps2&#x27;/&gt; &lt;input type=&#x27;keyboard&#x27; bus=&#x27;ps2&#x27;/&gt; &lt;graphics type=&#x27;spice&#x27; autoport=&#x27;yes&#x27;&gt; &lt;listen type=&#x27;address&#x27;/&gt; &lt;image compression=&#x27;off&#x27;/&gt; &lt;/graphics&gt; &lt;video&gt; &lt;model type=&#x27;qxl&#x27; ram=&#x27;65536&#x27; vram=&#x27;65536&#x27; vgamem=&#x27;16384&#x27; heads=&#x27;1&#x27; primary=&#x27;yes&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x01&#x27; function=&#x27;0x0&#x27;/&gt; &lt;/video&gt; &lt;redirdev bus=&#x27;usb&#x27; type=&#x27;spicevmc&#x27;&gt; &lt;address type=&#x27;usb&#x27; bus=&#x27;0&#x27; port=&#x27;2&#x27;/&gt; &lt;/redirdev&gt; &lt;redirdev bus=&#x27;usb&#x27; type=&#x27;spicevmc&#x27;&gt; &lt;address type=&#x27;usb&#x27; bus=&#x27;0&#x27; port=&#x27;3&#x27;/&gt; &lt;/redirdev&gt; &lt;memballoon model=&#x27;none&#x27;/&gt; &lt;/devices&gt;&lt;/domain&gt; The parameters are from Cisco CSR 1000v and Cisco ISRv Software Configuration GuideAfter you install the CSR1000v, and edit the XML file, you can run virsh to create the csr1kv. 123cd /etc/libvirtd/qemuvirsh define csr1kv-1.xmlvirsh start csr1kv-1 (6) Check CSR1000v’s tunningubuntu@ubuntu-kvm:/etc/libvirt/qemu$ virsh list 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253 Id Name State-------------------------- 1 csr1kv-1 runningubuntu@ubuntu-kvm:/etc/libvirt/qemu$ virsh vcpuinfo 1VCPU: 0CPU: 1State: runningCPU time: 167.0sCPU Affinity: -y--------------------------------------------------------------------------------------VCPU: 1CPU: 2State: runningCPU time: 193.3sCPU Affinity: --y-------------------------------------------------------------------------------------VCPU: 2CPU: 3State: runningCPU time: 152.5sCPU Affinity: ---y------------------------------------------------------------------------------------VCPU: 3CPU: 4State: runningCPU time: 174.3sCPU Affinity: ----y-----------------------------------------------------------------------------------VCPU: 4CPU: 5State: runningCPU time: 260.6sCPU Affinity: -----y----------------------------------------------------------------------------------VCPU: 5CPU: 6State: runningCPU time: 386.0sCPU Affinity: ------y---------------------------------------------------------------------------------VCPU: 6CPU: 7State: runningCPU time: 2189.9sCPU Affinity: -------y--------------------------------------------------------------------------------VCPU: 7CPU: 8State: runningCPU time: 2192.2sCPU Affinity: --------y------------------------------------------------------------------------------- From the above outputs, we can find that the vCPUa are pinned to NO. 1 to 8 CPU cores. 12345678910111213141516171819202122232425ubuntu@ubuntu-kvm:~$ sudo numastat -c qemuPer-node process memory usage (in MBs)PID Node 0 Node 1 Total--------------- ------ ------ -----4391 (qemu-syste 8373 0 83735891 (sudo) 4 1 5--------------- ------ ------ -----Total 8377 1 8377ubuntu@ubuntu-kvm:~$ sudo numastat -p 4391Per-node process memory usage (in MBs) for PID 4391 (qemu-system-x86) Node 0 Node 1 Total --------------- --------------- ---------------Huge 8192.00 0.00 8192.00Heap 10.00 0.00 10.00Stack 0.03 0.00 0.03Private 170.70 0.15 170.85---------------- --------------- --------------- ---------------Total 8372.73 0.16 8372.88 From the above outputs, the csr1kv-1 are using the memories from node 0. (7) Check the vNIC in CSR1KV123456789csr1kv-1#show platform software vnic-if interface-mapping------------------------------------------------------------- Interface Name Driver Name Mac Addr------------------------------------------------------------- GigabitEthernet3 net_i40e_vf 5254.00bd.9f54 GigabitEthernet2 net_i40e_vf 5254.000b.a52e GigabitEthernet1 net_virtio 5254.0069.ec73 The Driver Name net_i40e_vf indicates that the vNIC is a VF from the SR-IOV pool. CSR 1000v initial configuration and Smart License registration(1) CSR 1000v initial config examplePart of the configuration: 123456789101112131415161718192021222324252627282930313233343536373839404142CSR1000v-1#show sdwan running-configsystem system-ip 1.1.10.1 site-id 101 sp-organization-name CiscoBJ organization-name CiscoBJ vbond 10.75.58.51 port 12346!hostname CSR1000v-1username admin privilege 15 secret 9 $9$4&#x2F;QL2V2K4&#x2F;6H1k$XUmRNf.T7t3KDOj&#x2F;FmoNexpEypCxr482dExXHDnohSIip name-server 64.104.123.245ip route 0.0.0.0 0.0.0.0 10.75.59.1interface GigabitEthernet1 no shutdown arp timeout 1200 ip address 10.75.59.35 255.255.255.0 no ip redirects ip mtu 1500 mtu 1500 negotiation autoexitinterface Tunnel1 no shutdown ip unnumbered GigabitEthernet1 no ip redirects ipv6 unnumbered GigabitEthernet1 no ipv6 redirects tunnel source GigabitEthernet1 tunnel mode sdwanexitclock timezone CST 8 0ntp server x.x.x.x version 4sdwan interface GigabitEthernet1 tunnel-interface encapsulation ipsec allow-service sshd exit (2) Commands to check the status1234567show sdwan control local-propertiesshow sdwan control connectionsshow sdwan control connection-historyshow sdwan running-configshow sdwan bfd sessionsshow sdwan omp peersshow sdwan omp routes (3) CSR 1000v Smart License RegistrationBefore Smart License registration, you need ： CSR 1000v’s control connections are up； Configure ip http client source-interface GigabitEthernet2 If the version is 16.12.x and bellow, you need to allow service allsdwan interface GigabitEthernet2 tunnel-interface allow-service all In the 17.2.x and above versions, there is an allow-service https CSR 1000v can access URL：https://tools.cisco.com/its/service/oddce/services/DDCEServiceThe command license smart register idtoken xxxxxx will do the registration.You can find the idtoken from your Smart Account smart license inventory.show license status to check the status. 12CSR1000v-1#show platform hardware throughput levelThe current throughput level is 200000000 kb&#x2F;s Performances and limitations(1) The performance of the SR-IOVA performance test was done after the SR-IOV setup, the CSR1KV was configured as 8vCPU and 8G Memory, the packet drop rate was 0%. Packet Site SDWAN Performance (Mbps) CEF Performance (Mbps) 128Bytes 800 2843.75 256Bytes 1431.26 6500.00 512Bytes 2581.26 10578.13 1024Bytes 3731.26 15500.00 1400Bytes 4306.26 18171.88 Note: These test results are not to represent the official performance data. Different servers and network cards may have different test results. The above data is for demo only. (2) The limitation of the SR-IOVThe main limitation of the SR-IOV is the number of VLANs on each VF, the maximum VLAN of an VF is limited to 63 in ixgbevf.So, the active number of sub-interfaces on an interface of the CSR1KV that uses the SRIOV VFs is limited to 63.There is some notes in the Cisco CSR 1000v and Cisco ISRv Software Configuration Guide: SR-IOV (ixgbevf)Maximum VLANs: The maximum number of VLANs supported on PF is 64. Together, all VFs can have a total of 64 VLANs. (Intel limitation.) SR-IOV (i40evf)Maximum VLANs: The maximum number of VLANs supported on PF is 512. Together, all VFs can have a total of 512 VLANs. (Intel limitation.) Per-VF resources are managed by the PF (host) device driver. Referenceshttps://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html-single/performance_tuning_guide/index https://computingforgeeks.com/how-to-create-and-configure-bridge-networking-for-kvm-in-linux/ https://software.intel.com/content/www/us/en/develop/articles/configure-sr-iov-network-virtual-functions-in-linux-kvm.html https://linuxconfig.org/install-and-set-up-kvm-on-ubuntu-20-04-focal-fossa-linux https://kifarunix.com/how-to-fix-qemu-kvm-not-connected-error-on-ubuntu-20-04/ https://linuxconfig.org/how-to-disable-apparmor-on-ubuntu-20-04-focal-fossa-linux CSR1000v SD-WAN Installation on KVM by Jean-Marc Barozet Scripts and XML Example Files Ubuntu and KVM installation and setting NIC Settings with nmcli Check NIC SRIOV Capabilities GRUB Settings SRIOV Settings Check VFs Create SRIOV Adapter Pool KVM Tuning Check CSR1000v Tuning ens1f0_sriov_pool.xml csr1kv-1.xml","categories":[],"tags":[{"name":"CSR1000v","slug":"CSR1000v","permalink":"https://weiborao.github.io/tags/CSR1000v/"},{"name":"KVM","slug":"KVM","permalink":"https://weiborao.github.io/tags/KVM/"},{"name":"SRIOV","slug":"SRIOV","permalink":"https://weiborao.github.io/tags/SRIOV/"},{"name":"NetworkManager","slug":"NetworkManager","permalink":"https://weiborao.github.io/tags/NetworkManager/"}]},{"title":"CSR 1000v KVM SRIOV CentOS 7安装设置指南","slug":"csr1kv-kvm-CentOS7","date":"2021-02-03T05:58:32.000Z","updated":"2021-02-05T05:57:29.431Z","comments":true,"path":"csr1kv-kvm-CentOS7.html","link":"","permalink":"https://weiborao.github.io/csr1kv-kvm-CentOS7.html","excerpt":"","text":"作者：Rao Weibo 版本：1.0 更新日期：20210203 本文提供在 CentOS 7 版本上安装 KVM，并安装和设置 CSR 1000v/Catalyst 8000v 的指南，内容包括 SRIOV，KVM 调优以及 CSR1KV 初始化配置等内容。 CentOS7 安装及设置（1）BIOS 设置建议 Configuration Recommended Setting Intel Hyper-Threading Technology Disabled Number of Enable Cores ALL Execute Disable Enabled Intel VT Enabled Intel VT-D Enabled Intel VT-D coherency support Enabled Intel VT-D ATS support Enabled CPU Performance High throughput Hardware Perfetcher Disabled Adjacent Cache Line Prefetcher Disabled DCU Streamer Prefetch Disable Power Technology Custom Enhanced Intel Speedstep Technology Disabled Intel Turbo Boost Technology Enabled Processor Power State C6 Disabled Processor Power State C1 Enhanced Disabled Frequency Poor Override Enabled P-State Coordination HW_ALL Energy Performance Performance 以上建议来自 CSR 1000v 的安装指南。 （2）CentOS 7 的安装在安装的时候选择 Server with GUI Virtualization Client Virtualization Hypervisor Virtualization Tools 启动完毕以后关闭 selinux，重启生效。 123sed -i &#x27;s/SELINUX=enforcing/SELINUX=disabled/g&#x27; /etc/selinux/configgetenforce //结果为：Enforcing（开启状态）disabled(关闭状态) 123456789101112131415# 安装完后，SSH登录可能显示中文，可修改 .bash_profileLANG=&quot;en_US.UTF-8&quot;export LANGsource .bash_profileegrep -o &#x27;(vmx|svm)&#x27; /proc/cpuinfo | sort | uniq# 注：在生产环境中，需要在服务器连接的交换机以及出口防火墙上做好安全策略。systemctl stop firewalldsystemctl disable firewalld 本文档采用 NetworkManager 配置，故在此并不停用 NetworkManager。 123456789cat /etc/sysctl.confnet.ipv4.ip_forward = 1net.bridge.bridge-nf-call-ip6tables = 0net.bridge.bridge-nf-call-iptables = 0net.bridge.bridge-nf-call-arptables = 0 检查 KVM 组件版本： 123456789101112131415161718192021[root@centos7 ~]# libvirtd -Vlibvirtd (libvirt) 4.5.0[root@centos7 ~]# /usr/libexec/qemu-kvm --versionQEMU emulator version 1.5.3 (qemu-kvm-1.5.3-173.el7_8.3), Copyright (c) 2003-2008 Fabrice Bellard[root@centos7 ~]# virt-manager --version1.5.0[root@centos7 ~]# modinfo kvm-intelfilename: /lib/modules/3.10.0-1127.18.2.el7.x86_64/kernel/arch/x86/kvm/kvm-intel.ko.xz[root@centos7 ~]# modinfo ixgbevffilename: /lib/modules/3.10.0-1127.18.2.el7.x86_64/kernel/drivers/net/ethernet/intel/ixgbevf/ixgbevf.ko.xzversion: 4.1.0-k-rh7.7 （3）创建本地 Yum 源（可选）12345678910111213141516171819202122232425262728293031323334353637383940414243# 1、备份本地/etc/yum.repos.d 目录下的yum源cd /etc/yum.repos.d/mkdir bakmv C* bak/# 2、上传CentOS-7-x86_64-Everything-2009.iso镜像到/opt# 3、挂载镜像mkdir -p /media/cdrommount -t iso9660 -o loop /opt/CentOS-7-x86_64-Everything-2009.iso /media/cdrom/mount //查看挂载信息df -hvi /etc/fstab/opt/CentOS-7-x86_64-Everything-2009.iso /media/cdrom iso9660 loop 0 0tail -1 /etc/fstab //查看是否写入/etc/fstab# 4、配置本地yum源cd /etc/yum.repos.d/vi local.repo[local]name=localbaseurl=file:///media/cdrom //前面的file://是协议,后面的/media/cdrom是光盘挂载点gpgcheck=0 //1使用公钥验证rpm包的正确性,0不验证enabled=1 //1启用yum源,0禁用yum源yum install -y numactl telnet 运行 virt-manager 启动图形化界面。 如果对 virsh CLI 命令熟悉，可以使用 virsh 命令创建虚拟机。 （4）服务器网卡配置—NetworkManager 配置在终端界面，可以通过 nmtui 打开图形化界面进行设置；以下使用 nmcli 进行设置。 123456789nmcli connection add con-name eno1 type ethernet autoconnect yes ifname eno1nmcli connection modify eno1 ipv4.method manual ipv4.addresses 10.75.58.43/24 ipv4.gateway 10.75.58.1 ipv4.dns 64.104.123.245nmcli connection up eno1nmcli connection show eno1ping 10.75.58.1 上述命令完成后，在/etc/sysconfig/network-scripts 中会生成网卡的 ifcfg 配置文件。 12345678910111213141516171819202122232425262728293031323334353637383940414243cat /etc/sysconfig/network-scripts/ifcfg-eno1HWADDR=70:7D:B9:59:5B:AETYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=noneIPADDR=10.75.58.43PREFIX=24GATEWAY=10.75.58.1DNS1=64.104.123.245DEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=eno1UUID=2a1c8b39-7f44-321b-a65f-a93e70ab0616ONBOOT=yesAUTOCONNECT_PRIORITY=-999DEVICE=eno1 此时，可以将 network.service 停止和关闭。 123systemctl stop networksystemctl disable network 注意，如果 NetworkManager 未设置妥当，执行 systemctl stop network 后，会导致服务器无法管理。 准备开启 SRIOV 的网卡设置，以 eno2 为例： 123456789nmcli connection add con-name eno2 type ethernet autoconnect yes ifname eno2nmcli connection modify eno2 ethernet.mtu 9216 ipv4.method disablednmcli connection up eno2nmcli connection show eno2ip link show dev eno2 注：上述 MTU 值设置为 9216 是借鉴自 Cisco NFVIS 平台，如下： 12345678910111213CSP5228-1# show pnic-detail mtuName MTU&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;eth0-1 9216eth0-2 9216eth1-1 9216eth1-2 9216 （5）配置 Linux 网桥 （可选）网桥 br1 配置示例： 12345nmcli connection add con-name br1 type bridge autoconnect yes ipv4.method disabled ethernet.mtu 9216 ifname br1nmcli connection up br1ip link show dev br1 网桥 br1 的物理网卡配置 1234567nmcli connection add con-name eno5 type ethernet autoconnect yes ifname eno5nmcli connection modify eno5 ethernet.mtu 9216 ipv4.method disabled master br1nmcli connection up eno5ip link show dev eno5 创建 net-br1 网络 [root@centos7 ~]# cat net-br1.xml 123456789&lt;network&gt; &lt;name&gt;net-br1&lt;/name&gt; &lt;forward mode=&quot;bridge&quot;/&gt; &lt;bridge name=&quot;br1&quot;/&gt;&lt;/network&gt; 1234567891011[root@centos7 ~]# virsh net-define net-br1.xmlNetwork net-br1 defined from net-br1.xml[root@centos7 ~]# virsh net-start net-br1Network net-br1 started[root@centos7 ~]# virsh net-autostart net-br1Network net-br1 marked as autostarted 配置 SR-IOV（1）检查网卡对 SR-IOV 的支持，并配置网卡可使用 lshw 和 lspci 检查网卡对 SR-IOV 的支持 lshw -c network -businfo 123456789101112131415Bus info Device Class Description========================================================pci@0000:1d:00.0 eno5 network VIC Ethernet NICpci@0000:1d:00.1 eno6 network VIC Ethernet NICpci@0000:1d:00.2 eno7 network VIC Ethernet NICpci@0000:1d:00.3 eno8 network VIC Ethernet NICpci@0000:3b:00.0 eno1 network Ethernet Controller 10G X550Tpci@0000:3b:00.1 eno2 network Ethernet Controller 10G X550T lspci -vv -s 3b:00.1 | grep -A 5 -i SR-IOV 1234567891011Capabilities: [160 v1] Single Root I/O Virtualization (SR-IOV) IOVCap: Migration-, Interrupt Message Number: 000 IOVCtl: Enable+ Migration- Interrupt- MSE+ ARIHierarchy- IOVSta: Migration- Initial VFs: 64, Total VFs: 64, Number of VFs: 8, Function Dependency Link: 01 VF offset: 128, stride: 2, Device ID: 1565 （2）设置启动参数1234567891011vi /etc/default/grubGRUB_CMDLINE_LINUX=&quot;crashkernel=auto spectre_v2=retpoline rd.lvm.lv=centos/root rd.lvm.lv=centos/swap rhgb quiet hugepagesz=1G hugepages=32 default_hugepagesz=1G intel_iommu=on iommu=pt isolcpus=1-8,37-44&quot;注：页面数字不要过大，不然启动失败，如果后续不够，可以在运行时添加。grub2-mkconfig -o /boot/grub2/grub.cfggrub2-mkconfig -o /boot/efi/EFI/centos/grub.cfgiommu=pt 参数是将SRIOV设备支持PCI Passthrough 重启后验证 12345cat /proc/cmdline |grep intel_iommu=ondmesg |grep -e DMAR -e IOMMUdmesg | grep -e DMAR -e IOMMU -e AMD-Vi default_hugepagesz=1G hugepagesz=1G hugepages=32 参数设置主机在启动时分配 32 个 1GB 的内存大页，这些是静态内存大页。 CSR 1000v 虚拟机将试用这些静态大页以获得最优性能。 isolcpus=1-8,37-44 参数设置的作用是隔离 1-8，37-44 的 CPU 核，使其独立于内核的平衡调度算法，也就是内核本身不会将进程分配到被隔离的 CPU。之后我们可将指定的进程 CSR 1000v 虚拟机绑定到被隔离的 CPU 上运行，让进程独占 CPU，使其实时性可得到一定程度的提高。 可参考 这个章节获取主机 CPU 核的相关信息。 123456789root@centos7 ~]# cat /proc/cmdline |grep intel_iommu=onBOOT_IMAGE=/vmlinuz-3.10.0-1127.el7.x86_64 root=/dev/mapper/centos-root ro crashkernel=auto spectre_v2=retpoline rd.lvm.lv=centos/root rd.lvm.lv=centos/swap rhgb quiet hugepagesz=1G hugepages=32 default_hugepagesz=1G intel_iommu=on iommu=pt LANG=en_US.UTF-8[root@centos7 ~]# dmesg |grep -e DMAR -e IOMMU[ 0.000000] ACPI: DMAR 000000005d6f5d70 00250 (v01 Cisco0 CiscoUCS 00000001 INTL 20091013)[ 0.000000] DMAR: IOMMU enabled 查看隔离的 CPU 核以及所有的 CPU 核。 1234567[root@centos7 ~]# cat /sys/devices/system/cpu/isolated1-8,37-44[root@centos7 ~]# cat /sys/devices/system/cpu/present0-71 （3）通过 nmcli 持久化 VFs 配置nmcli 可以设置网卡的 sriov 参数，如下： 1nmcli connection modify eno2 sriov.total-vfs 4 还可以设置每一个 VF 设备的 MAC 地址，便于管理： 1nmcli connection modify eno2 sriov.vfs &#x27;0 mac=8E:DF:08:C1:D1:DE trust=true, 1 mac=5A:B9:2F:99:A6:CE trust=true, 2 mac=46:78:69:E3:71:3D trust=true, 3 mac=7E:A7:DB:3B:1B:B3 trust=true&#x27; 执行上述命令后： cat /etc/sysconfig/network-scripts/ifcfg-eno2 1234567891011121314151617181920212223242526272829TYPE=EthernetNAME=eno2UUID=64ffa204-0158-40c8-ba86-2b7aebf27619DEVICE=eno2ONBOOT=yesMTU=9216HWADDR=70:7D:B9:59:5B:AFPROXY_METHOD=noneBROWSER_ONLY=noIPV6INIT=noSRIOV_TOTAL_VFS=4SRIOV_VF0=&quot;mac=8E:DF:08:C1:D1:DE trust=true&quot;SRIOV_VF1=&quot;mac=5A:B9:2F:99:A6:CE trust=true&quot;SRIOV_VF2=&quot;mac=46:78:69:E3:71:3D trust=true&quot;SRIOV_VF3=&quot;mac=7E:A7:DB:3B:1B:B3 trust=true&quot; 重启后，检查 dmesg： dmesg | grep -i vf | grep -i eno2 12345678910111213141516171819202122232425[ 11.953333] ixgbe 0000:3b:00.1 eno2: SR-IOV enabled with 4 VFs[ 12.541801] ixgbe 0000:3b:00.1: setting MAC 8e:df:08:c1:d1:de on VF 0[ 12.541805] ixgbe 0000:3b:00.1: Reload the VF driver to make this change effective.[ 12.541841] ixgbe 0000:3b:00.1 eno2: VF 0 is trusted[ 12.541846] ixgbe 0000:3b:00.1: setting MAC 5a:b9:2f:99:a6:ce on VF 1[ 12.541850] ixgbe 0000:3b:00.1: Reload the VF driver to make this change effective.[ 12.541883] ixgbe 0000:3b:00.1 eno2: VF 1 is trusted[ 12.541887] ixgbe 0000:3b:00.1: setting MAC 46:78:69:e3:71:3d on VF 2[ 12.541891] ixgbe 0000:3b:00.1: Reload the VF driver to make this change effective.[ 12.541923] ixgbe 0000:3b:00.1 eno2: VF 2 is trusted[ 12.541928] ixgbe 0000:3b:00.1: setting MAC 7e:a7:db:3b:1b:b3 on VF 3[ 12.541932] ixgbe 0000:3b:00.1: Reload the VF driver to make this change effective.[ 12.541965] ixgbe 0000:3b:00.1 eno2: VF 3 is trusted （4）检查 VF可通过 lspci 和 ip link 检查 VF，如下： 123[root@centos7 ~]# lspci | grep -i Virtual[root@centos7 ~]# ip link show | grep -B2 vf 寻找 Physical Function 和 Virtual Function 之间的对应关系： 1[root@centos7 ~]# ls -l /sys/class/net/eno1/device/ | grep virtfn VF 被创建后，NetworkManager 自动给新的设备创建 Connection，可以修改名称，如下： nmcli connection 12345678910111213NAME UUID TYPE DEVICEeno1 2a1c8b39-7f44-321b-a65f-a93e70ab0616 ethernet eno1eno2 64ffa204-0158-40c8-ba86-2b7aebf27619 ethernet eno2enp59s16f1 19c28a93-aa36-38e6-a556-55a922a0a332 ethernet enp59s16f1enp59s16f3 428d9707-1515-3475-b356-7eb229c3f937 ethernet enp59s16f3enp59s16f5 21a8dd5f-239f-37b2-9b09-24ce0e7413bc ethernet enp59s16f5enp59s16f7 0ca0da65-64c4-314d-89a4-4213f9e4f478 ethernet enp59s16f7 修改名称： 1nmcli connection modify uuid 19c28a93-aa36-38e6-a556-55a922a0a332 connection.id enp59s16f1 修改 MTU 值，并禁用 IPv4 和 IPv6，网卡启动更快 12345nmcli connection modify enp59s16f1 ifname enp59s16f1 ipv4.method disabled ipv6.method ignore ethernet.mtu 9216 ethernet.mac-address &quot;&quot;nmcli connection up enp59s16f1nmcli connection show enp59s16f1 上述命令生成的 ifcfg 配置文件如下： [root@centos7 ~]# cat /etc/sysconfig/network-scripts/ifcfg-enp59s16f1 12345678910111213141516171819202122232425262728293031TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=noIPV6_AUTOCONF=noIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=enp59s16f1UUID=19c28a93-aa36-38e6-a556-55a922a0a332ONBOOT=yesAUTOCONNECT_PRIORITY=-999DEVICE=enp59s16f1MTU=9216 这样，即便系统重启，上述配置依然能生效。 使用 KVM 的虚拟网络适配器池主机上创建一个 VF 网络设备的资源池，资源池内的设备可以自动地分配给虚拟机使用。 （1）创建一个 xml 文件。[root@centos7 ~]# cat eno2_sriov_pool.xml 1234567891011&lt;network&gt; &lt;name&gt;eno2_sriov_pool&lt;/name&gt; &lt;!-- This is the name of the file you created --&gt; &lt;forward mode=&#x27;hostdev&#x27; managed=&#x27;yes&#x27;&gt; &lt;pf dev=&#x27;eno2&#x27;/&gt; &lt;!-- Use the netdev name of your SR-IOV devices PF here --&gt; &lt;/forward&gt;&lt;/network&gt; （2）根据 xml 定义一个网络，并设置为自动重启12345virsh net-define eno2_sriov_pool.xmlvirsh net-start eno2_sriov_poolvirsh net-autostart eno2_sriov_pool [root@centos7 ~]# virsh net-dumpxml eno2_sriov_pool 123456789101112131415161718192021&lt;network connections=&#x27;1&#x27;&gt; &lt;name&gt;eno2_sriov_pool&lt;/name&gt; &lt;uuid&gt;e0842451-0137-4255-8783-305ca27f082d&lt;/uuid&gt; &lt;forward mode=&#x27;hostdev&#x27; managed=&#x27;yes&#x27;&gt; &lt;pf dev=&#x27;eno2&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x3b&#x27; slot=&#x27;0x10&#x27; function=&#x27;0x1&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x3b&#x27; slot=&#x27;0x10&#x27; function=&#x27;0x3&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x3b&#x27; slot=&#x27;0x10&#x27; function=&#x27;0x5&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x3b&#x27; slot=&#x27;0x10&#x27; function=&#x27;0x7&#x27;/&gt; &lt;/forward&gt;&lt;/network&gt; （3）从网络适配器池中分配网卡给虚拟机用这种方法添加 SRIOV 网卡比较简单： 按照如上方法添加网卡，等同于以下 xml 配置： 1234567891011&lt;interface type=&#x27;network&#x27;&gt; &lt;mac address=&#x27;52:54:00:2d:87:99&#x27;/&gt; &lt;source network=&#x27;eno2_sriov_pool&#x27;/&gt; &lt;model type=&#x27;virtio&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x04&#x27; function=&#x27;0x0&#x27;/&gt;&lt;/interface&gt; 开机后 dumpxml 如下： 12345678910111213141516171819 &lt;interface type=&#x27;hostdev&#x27; managed=&#x27;yes&#x27;&gt; &lt;mac address=&#x27;52:54:00:2d:87:99&#x27;/&gt; &lt;driver name=&#x27;vfio&#x27;/&gt; &lt;source&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x3b&#x27; slot=&#x27;0x10&#x27; function=&#x27;0x1&#x27;/&gt; &lt;/source&gt; &lt;model type=&#x27;virtio&#x27;/&gt; &lt;alias name=&#x27;hostdev0&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x0f&#x27; function=&#x27;0x0&#x27;/&gt;&lt;/interface&gt; 使用 Virt-manager 安装 CSR1000v在 CentOS 图形界面中，打开 Terminal，运行 virt-manager，按照以下步骤创建 CSR1000v；添加网卡，并选择 en2_sriov_pool。 注：csr1kv-1 的第一个网口选择macvtap Bridge模式，这样就无需创建一个 Linux 网桥。但是，csr1kv-1 启动以后不能通过该接口与 Linux 主机进行通信，仅能通过该接口访问 Linux 主机外的网络。 添加完网卡后，点击开始安装，然后就可以关闭虚拟机了。上述操作完成后，virt-manager 会在**/etc/libvirtd/qemu/目录下创建csr1kv-1.xml**。 KVM 调优配置KVM 的调优比较复杂，主要是 NUMA、内存大页、vCPU PIN 等，参考资料为 Redhat Linux 7 PERFORMANCE TUNING GUIDE。 （1）检查平台的能力1234567891011121314151617[root@centos7 ~]# virsh nodeinfoCPU model: x86_64CPU(s): 72CPU frequency: 999 MHzCPU socket(s): 1Core(s) per socket: 18Thread(s) per core: 2NUMA cell(s): 2Memory size: 263665612 KiB 1234567891011121314151617181920212223[root@centos7 ~]# virsh capabilities&lt;capabilities&gt; &lt;host&gt; &lt;uuid&gt;4e53df1f-5b36-6842-99ee-1369d7c68730&lt;/uuid&gt; &lt;cpu&gt; &lt;arch&gt;x86_64&lt;/arch&gt; &lt;model&gt;Skylake-Server-IBRS&lt;/model&gt; &lt;vendor&gt;Intel&lt;/vendor&gt; &lt;microcode version=&#x27;33581318&#x27;/&gt; &lt;counter name=&#x27;tsc&#x27; frequency=&#x27;2294597000&#x27; scaling=&#x27;yes&#x27;/&gt; &lt;topology sockets=&#x27;1&#x27; cores=&#x27;18&#x27; threads=&#x27;2&#x27;/&gt;…… 可检查平台的 CPU 核数、分布，内存的 NUMA 分布等。 （2）NUMA 调优1234567891011121314151617181920212223[root@centos7 ~]# numactl --hardwareavailable: 2 nodes (0-1)node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53node 0 size: 128491 MBnode 0 free: 112157 MBnode 1 cpus: 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71node 1 size: 128994 MBnode 1 free: 124120 MBnode distances:node 0 1 0: 10 21 1: 21 10 两颗 CPU，每颗 CPU 各有 128GB 内存，分别是 node 0 和 node 1。 （3）内存大页 HugePage 以及透明大页 cat /proc/meminfo | grep HugePages 查看当前系统有多少个大页： 12345678910111213[root@centos7 ~]# cat /proc/meminfo | grep HugeAnonHugePages: 1685504 kBHugePages_Total: 64HugePages_Free: 60HugePages_Rsvd: 0HugePages_Surp: 0Hugepagesize: 1048576 kB 在系统运行时修改大页数量： 123456789101112131415161718192021[root@centos7 ~]# cat /sys/devices/system/node/node0/hugepages/hugepages-1048576kB/nr_hugepages16[root@centos7 ~]# echo 32 &gt; /sys/devices/system/node/node0/hugepages/hugepages-1048576kB/nr_hugepages[root@centos7 ~]# echo 32 &gt; /sys/devices/system/node/node1/hugepages/hugepages-1048576kB/nr_hugepages[root@centos7 ~]#[root@centos7 ~]# numastat -cm | egrep &#x27;Node|Huge&#x27;` `Node 0 Node 1 TotalAnonHugePages 10962 1528 12490HugePages_Total 32768 32768 65536HugePages_Free 32768 32768 65536HugePages_Surp 0 0 0 检查透明大页参数，在 CentOS7 上缺省开启；开启了透明大页，不影响静态大页的使用。 123cat /sys/kernel/mm/transparent_hugepage/enabled[always] madvise never （4）vCPU 钉选设置 CPU Affinity 的好处是提高 CPU 缓存效率，避免进程在多个 CPU 核之间跳跃，切换 CPU 核均会导致缓存中的数据无效，缓存命中率大幅降低，导致数据获取的开销居高不下，损失性能。 virsh vcpuinfo csr1kv-1 可以查看 vCPU 的分配。 （5）编辑 CSR 1000v 的 XML 调优参数virsh edit csr1kv-1 可以编辑 XML 的参数，如下： 123456789101112131415161718192021222324&lt;memoryBacking&gt; &lt;hugepages&gt; &lt;page size=&#x27;1048576&#x27; unit=&#x27;KiB&#x27;/&gt; &lt;/hugepages&gt; &lt;locked/&gt; &lt;nosharepages/&gt;&lt;/memoryBacking&gt;&lt;vcpu placement=&#x27;static&#x27;&gt;8&lt;/vcpu&gt;&lt;cputune&gt; &lt;vcpupin vcpu=&#x27;0&#x27; cpuset=&#x27;1&#x27;/&gt; &lt;vcpupin vcpu=&#x27;1&#x27; cpuset=&#x27;2&#x27;/&gt; &lt;vcpupin vcpu=&#x27;2&#x27; cpuset=&#x27;3&#x27;/&gt; &lt;vcpupin vcpu=&#x27;3&#x27; cpuset=&#x27;4&#x27;/&gt; &lt;vcpupin vcpu=&#x27;4&#x27; cpuset=&#x27;5&#x27;/&gt; &lt;vcpupin vcpu=&#x27;5&#x27; cpuset=&#x27;6&#x27;/&gt; &lt;vcpupin vcpu=&#x27;6&#x27; cpuset=&#x27;7&#x27;/&gt; &lt;vcpupin vcpu=&#x27;7&#x27; cpuset=&#x27;8&#x27;/&gt; &lt;emulatorpin cpuset=&#x27;45-52&#x27;/&gt; &lt;!-- If Hyper-threading were enabled --&gt;&lt;/cputune&gt;&lt;numatune&gt; &lt;memory mode=&#x27;strict&#x27; nodeset=&#x27;0&#x27;/&gt;&lt;/numatune&gt;&lt;cpu mode=&#x27;host-passthrough&#x27; check=&#x27;none&#x27;/&gt; &lt;memballoon model=&#x27;none&#x27;/&gt; 注：**** 参数，仅当 Hyper-Threating 开启时使用；有一些平台并未关闭超线程，例如 Cisco 专门的 NFV 平台 CSP。通过 virsh chapabilities 查看 siblings=’1,37’，当 core 1 设置为 vcpupin 时，core 37 应设置到 emulatorpin cpuset 中。 以下关于 CPU 和内存的参数设定建议来自于https://libvirt.org/formatdomain.html 和 https://libvirt.org/kbase/kvm-realtime.html 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129&lt;domain type=&#x27;kvm&#x27;&gt; &lt;name&gt;csr1kv-1&lt;/name&gt; &lt;uuid&gt;59581018-6387-49df-ab09-2bcf40fc12ba&lt;/uuid&gt; &lt;memory unit=&#x27;KiB&#x27;&gt;8388608&lt;/memory&gt; &lt;currentMemory unit=&#x27;KiB&#x27;&gt;8388608&lt;/currentMemory&gt; &lt;memoryBacking&gt; &lt;hugepages&gt; &lt;page size=&#x27;1048576&#x27; unit=&#x27;KiB&#x27;/&gt;&lt;/hugepages&gt;&lt;locked/&gt; &lt;nosharepages/&gt; &lt;/memoryBacking&gt; &lt;vcpu placement=&#x27;static&#x27;&gt;8&lt;/vcpu&gt; &lt;cputune&gt; &lt;vcpupin vcpu=&#x27;0&#x27; cpuset=&#x27;1&#x27;/&gt; &lt;vcpupin vcpu=&#x27;1&#x27; cpuset=&#x27;2&#x27;/&gt; &lt;vcpupin vcpu=&#x27;2&#x27; cpuset=&#x27;3&#x27;/&gt; &lt;vcpupin vcpu=&#x27;3&#x27; cpuset=&#x27;4&#x27;/&gt; &lt;vcpupin vcpu=&#x27;4&#x27; cpuset=&#x27;5&#x27;/&gt; &lt;vcpupin vcpu=&#x27;5&#x27; cpuset=&#x27;6&#x27;/&gt; &lt;vcpupin vcpu=&#x27;6&#x27; cpuset=&#x27;7&#x27;/&gt; &lt;vcpupin vcpu=&#x27;7&#x27; cpuset=&#x27;8&#x27;/&gt; &lt;emulatorpin cpuset=&#x27;37-44&#x27;/&gt; &lt;/cputune&gt; &lt;numatune&gt; &lt;memory mode=&#x27;strict&#x27; nodeset=&#x27;0&#x27;/&gt; &lt;/numatune&gt; &lt;os&gt; &lt;type arch=&#x27;x86_64&#x27; machine=&#x27;pc-i440fx-rhel7.0.0&#x27;&gt;hvm&lt;/type&gt; &lt;boot dev=&#x27;hd&#x27;/&gt; &lt;/os&gt; &lt;features&gt; &lt;acpi/&gt; &lt;apic/&gt; &lt;/features&gt; &lt;cpu mode=&#x27;host-passthrough&#x27; check=&#x27;none&#x27;/&gt; &lt;clock offset=&#x27;utc&#x27;&gt; &lt;timer name=&#x27;rtc&#x27; tickpolicy=&#x27;catchup&#x27;/&gt; &lt;timer name=&#x27;pit&#x27; tickpolicy=&#x27;delay&#x27;/&gt; &lt;timer name=&#x27;hpet&#x27; present=&#x27;no&#x27;/&gt; &lt;/clock&gt; &lt;on_poweroff&gt;destroy&lt;/on_poweroff&gt; &lt;on_reboot&gt;restart&lt;/on_reboot&gt; &lt;on_crash&gt;destroy&lt;/on_crash&gt; &lt;pm&gt; &lt;suspend-to-mem enabled=&#x27;no&#x27;/&gt; &lt;suspend-to-disk enabled=&#x27;no&#x27;/&gt; &lt;/pm&gt; &lt;devices&gt; &lt;emulator&gt;/usr/libexec/qemu-kvm&lt;/emulator&gt; &lt;disk type=&#x27;file&#x27; device=&#x27;disk&#x27;&gt; &lt;driver name=&#x27;qemu&#x27; type=&#x27;qcow2&#x27;/&gt; &lt;source file=&#x27;/home/root/images/csr1000v-universalk9.17.02.01v.qcow2&#x27;/&gt; &lt;target dev=&#x27;vda&#x27; bus=&#x27;virtio&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x07&#x27; function=&#x27;0x0&#x27;/&gt; &lt;/disk&gt; &lt;controller type=&#x27;usb&#x27; index=&#x27;0&#x27; model=&#x27;ich9-ehci1&#x27;&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x05&#x27; function=&#x27;0x7&#x27;/&gt; &lt;/controller&gt; &lt;controller type=&#x27;usb&#x27; index=&#x27;0&#x27; model=&#x27;ich9-uhci1&#x27;&gt; &lt;master startport=&#x27;0&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x05&#x27; function=&#x27;0x0&#x27; multifunction=&#x27;on&#x27;/&gt; &lt;/controller&gt; &lt;controller type=&#x27;usb&#x27; index=&#x27;0&#x27; model=&#x27;ich9-uhci2&#x27;&gt; &lt;master startport=&#x27;2&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x05&#x27; function=&#x27;0x1&#x27;/&gt; &lt;/controller&gt; &lt;controller type=&#x27;usb&#x27; index=&#x27;0&#x27; model=&#x27;ich9-uhci3&#x27;&gt; &lt;master startport=&#x27;4&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x05&#x27; function=&#x27;0x2&#x27;/&gt; &lt;/controller&gt; &lt;controller type=&#x27;pci&#x27; index=&#x27;0&#x27; model=&#x27;pci-root&#x27;/&gt; &lt;controller type=&#x27;virtio-serial&#x27; index=&#x27;0&#x27;&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x06&#x27; function=&#x27;0x0&#x27;/&gt; &lt;/controller&gt; &lt;interface type=&#x27;direct&#x27;&gt; &lt;mac address=&#x27;52:54:00:36:95:f0&#x27;/&gt; &lt;source dev=&#x27;eno1&#x27; mode=&#x27;bridge&#x27;/&gt; &lt;model type=&#x27;virtio&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x03&#x27; function=&#x27;0x0&#x27;/&gt; &lt;/interface&gt; &lt;interface type=&#x27;network&#x27;&gt; &lt;mac address=&#x27;52:54:00:2d:87:99&#x27;/&gt; &lt;source network=&#x27;eno2_sriov_pool&#x27;/&gt; &lt;model type=&#x27;virtio&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x04&#x27; function=&#x27;0x0&#x27;/&gt; &lt;/interface&gt; &lt;serial type=&#x27;pty&#x27;&gt; &lt;target type=&#x27;isa-serial&#x27; port=&#x27;0&#x27;&gt; &lt;model name=&#x27;isa-serial&#x27;/&gt; &lt;/target&gt; &lt;/serial&gt; &lt;console type=&#x27;pty&#x27;&gt; &lt;target type=&#x27;serial&#x27; port=&#x27;0&#x27;/&gt; &lt;/console&gt; &lt;channel type=&#x27;unix&#x27;&gt; &lt;target type=&#x27;virtio&#x27; name=&#x27;org.qemu.guest_agent.0&#x27;/&gt; &lt;address type=&#x27;virtio-serial&#x27; controller=&#x27;0&#x27; bus=&#x27;0&#x27; port=&#x27;1&#x27;/&gt; &lt;/channel&gt; &lt;channel type=&#x27;spicevmc&#x27;&gt; &lt;target type=&#x27;virtio&#x27; name=&#x27;com.redhat.spice.0&#x27;/&gt; &lt;address type=&#x27;virtio-serial&#x27; controller=&#x27;0&#x27; bus=&#x27;0&#x27; port=&#x27;2&#x27;/&gt; &lt;/channel&gt; &lt;input type=&#x27;tablet&#x27; bus=&#x27;usb&#x27;&gt; &lt;address type=&#x27;usb&#x27; bus=&#x27;0&#x27; port=&#x27;1&#x27;/&gt; &lt;/input&gt; &lt;input type=&#x27;mouse&#x27; bus=&#x27;ps2&#x27;/&gt; &lt;input type=&#x27;keyboard&#x27; bus=&#x27;ps2&#x27;/&gt; &lt;graphics type=&#x27;spice&#x27; autoport=&#x27;yes&#x27;&gt; &lt;listen type=&#x27;address&#x27;/&gt; &lt;image compression=&#x27;off&#x27;/&gt; &lt;/graphics&gt; &lt;video&gt; &lt;model type=&#x27;qxl&#x27; ram=&#x27;65536&#x27; vram=&#x27;65536&#x27; vgamem=&#x27;16384&#x27; heads=&#x27;1&#x27; primary=&#x27;yes&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x02&#x27; function=&#x27;0x0&#x27;/&gt; &lt;/video&gt; &lt;redirdev bus=&#x27;usb&#x27; type=&#x27;spicevmc&#x27;&gt; &lt;address type=&#x27;usb&#x27; bus=&#x27;0&#x27; port=&#x27;2&#x27;/&gt; &lt;/redirdev&gt; &lt;redirdev bus=&#x27;usb&#x27; type=&#x27;spicevmc&#x27;&gt; &lt;address type=&#x27;usb&#x27; bus=&#x27;0&#x27; port=&#x27;3&#x27;/&gt; &lt;/redirdev&gt; &lt;memballoon model=&#x27;none&#x27;/&gt; &lt;rng model=&#x27;virtio&#x27;&gt; &lt;backend model=&#x27;random&#x27;&gt;/dev/urandom&lt;/backend&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x09&#x27; function=&#x27;0x0&#x27;/&gt; &lt;/rng&gt; &lt;/devices&gt;&lt;/domain&gt; 上述参数整理自《Cisco CSR 1000v and Cisco ISRv Software Configuration Guide》 完成上述 XML 文件的编辑后，执行 12345cd /etc/libvirtd/qemuvirsh define csr1kv-1.xmlvirsh start csr1kv-1 （6）在 KVM 主机上访问 CSR1000v 的 Console在 virt-manager 创建 CSR1000v 虚拟机的时候，缺省会添加一个 Serial Device。 12345[root@centos7 qemu]# virsh console 20Connected to domain CSR1000v-1Escape character is ^] CSR 1000v 暂时不能通过 Console 配置，需要通过 virt-manager 的图形化界面进行初始化配置。 vCloud 的 Console 访问正常。 （7）检验 CSR1000v 的调优配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105[root@centos7 ~]# virsh listId Name State---------------------------------------------------- 6 csr1kv-1 running[root@centos7 ~]# virsh vcpuinfo 6VCPU: 0CPU: 1State: runningCPU time: 34.5sCPU Affinity: -y----------------------------------------------------------------------VCPU: 1CPU: 2State: runningCPU time: 32.0sCPU Affinity: --y---------------------------------------------------------------------VCPU: 2CPU: 3State: runningCPU time: 23.8sCPU Affinity: ---y--------------------------------------------------------------------VCPU: 3CPU: 4State: runningCPU time: 19.8sCPU Affinity: ----y-------------------------------------------------------------------VCPU: 4CPU: 5State: runningCPU time: 23.0sCPU Affinity: -----y------------------------------------------------------------------VCPU: 5CPU: 6State: runningCPU time: 23.4sCPU Affinity: ------y-----------------------------------------------------------------VCPU: 6CPU: 7State: runningCPU time: 28.5sCPU Affinity: -------y----------------------------------------------------------------VCPU: 7CPU: 8State: runningCPU time: 28.2sCPU Affinity: --------y--------------------------------------------------------------- 以上显示 CSR1000v 虚拟机的 CPU 亲和性在 1-8 核上。 123456789101112131415161718192021[root@centos7 ~]# numastat -c qemu-kvmPer-node process memory usage (in MBs) for PID 46895 (qemu-kvm) Node 0 Node 1 Total ------ ------ -----Huge 8192 0 8192Heap 118 0 118Stack 0 0 0Private 144 7 151------- ------ ------ -----Total 8455 7 8461 以上显示，内存主要使用 Node 0。 123456789[root@centos7 ~]# numastat -vm -p 13428 | grep HugePageAnonHugePages 956.00 494.00 1450.00HugePages_Total 16384.00 16384.00 32768.00HugePages_Free 8192.00 16384.00 24576.00HugePages_Surp 0.00 0.00 0.00 （8）在 CSR1KV 上检查虚拟网卡1234567891011csr1kv-1#show platform software vnic-if interface-mapping------------------------------------------------------------- Interface Name Driver Name Mac Addr------------------------------------------------------------- GigabitEthernet2 net_ixgbe_vf 5254.002d.8799 GigabitEthernet1 net_virtio 5254.0036.95f0 上述驱动名显示为 net_ixgbe_vf 表明该虚拟网卡是一个 SR-IOV 池中分配的 VF 设备。 Linux 上抓取虚拟网卡的报文（参考）查找虚拟机的网卡列表 1234567891011[root@centos7 ~]# virsh domiflist 10Interface Type Source Model MAC-------------------------------------------------------vnet0 network default virtio 52:54:00:38:71:4amacvtap0 direct eno1 virtio 52:54:00:b2:70:90vnet5 bridge net-br1 virtio 52:54:00:69:93:23 抓取 vnet5 的报文 123456789[root@centos7 ~]# tcpdump -i vnet5 -w ping.pcaptcpdump: listening on vnet5, link-type EN10MB (Ethernet), capture size 262144 bytes^C49 packets captured51 packets received by filter0 packets dropped by kernel 注：VF 如果被分配给虚拟机，那么在 Linux 主机里，通过 ip link 则查看不到该设备，无法通过上述办法抓包。 CSR 1000v 的初始化配置及 Smart License 注册（1）CSR 1000v 初始化配置示例部分节略，其他为缺省配置。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081CSR1000v-1#show sdwan running-configsystem system-ip 1.1.10.1 site-id 101 sp-organization-name CiscoBJ organization-name CiscoBJ vbond 10.75.58.51 port 12346!hostname CSR1000v-1username admin privilege 15 secret 9 $9$4&#x2F;QL2V2K4&#x2F;6H1k$XUmRNf.T7t3KDOj&#x2F;FmoNexpEypCxr482dExXHDnohSIip name-server 64.104.123.245ip route 0.0.0.0 0.0.0.0 10.75.59.1interface GigabitEthernet1 no shutdown arp timeout 1200 ip address 10.75.59.35 255.255.255.0 no ip redirects ip mtu 1500 mtu 1500 negotiation autoexitinterface Tunnel1 no shutdown ip unnumbered GigabitEthernet1 no ip redirects ipv6 unnumbered GigabitEthernet1 no ipv6 redirects tunnel source GigabitEthernet1 tunnel mode sdwanexitclock timezone CST 8 0ntp server 10.75.58.1 version 4sdwan interface GigabitEthernet1 tunnel-interface encapsulation ipsec allow-service sshd exit （2）常用命令 show sdwan control local-properties show sdwan control connections show sdwan control connection-history show sdwan running-config show sdwan bfd sessions show sdwan omp peers show sdwan omp routes （3）CSR 1000v Smart License 注册CSR 1000v 默认限速为 250Mbps，需要注册 Smart License 才可解开限速。 123CSR1000v-2#show platform hardware throughput levelThe current throughput level is 250000 kb&#x2F;s 注册 Smart License 需要满足以下条件： 1、 CSR 1000v 已经注册到 vManage，控制面连接正常； 2、 配置 ip http client source-interface GigabitEthernet2 3、 sdwan interface GigabitEthernet2 tunnel-interface allow-service all —针对 16.12.x 版本；在新版本中仅需要 allow https 4、 CSR 1000v 可以访问 URL：https://tools.cisco.com/its/service/oddce/services/DDCEService 允许访问 114.114.114.114，CSR 1000v 可解析域名 tools.cisco.com 替代办法，增加一条命令 ip host tools.cisco.com 72.163.4.38 执行 license smart register idtoken xxxxxx 进行注册 show license status 查看注册结果 注册完成后，系统解开限速： 123CSR1000v-1#show platform hardware throughput levelThe current throughput level is 200000000 kb&#x2F;s 性能和相关限制（1）SRIOV 的性能在上述 CSR1KV-1 安装好后，使用测试仪进行性能测试，测试条件中，设置丢包率为 0%，其性能如下： Packet Site SDWAN Performance (Mbps) CEF Performance (Mbps) 128Byte 800 2843.75 256Byte 1431.26 6500.00 512Byte 2581.26 10578.13 1024Byte 3731.26 15500.00 1400Byte 4306.26 18171.88 注：上述测试结果非官方结果，不同服务器和网卡可能测试结果有区别，上述性能数据仅供参考。 （2）SRIOV 的限制SRIOV 的主要限制是每一个 VF 设备支持的 VLAN 数，ixgbevf 所支持的最大 VLAN 数为 64；因此，在 CSR1KV 中对应的虚拟接口配置的活跃子接口数最大为 64。 配置指南中有关于 SRIOV 子接口限制的说明： Cisco CSR 1000v and Cisco ISRv Software Configuration Guide: SR-IOV (ixgbevf) Maximum VLANs: The maximum number of VLANs supported on PF is 64. Together, all VFs can have a total of 64 VLANs. (Intel limitation.) SR-IOV (i40evf) Maximum VLANs: The maximum number of VLANs supported on PF is 512. Together, all VFs can have a total of 512 VLANs. (Intel limitation.) Per-VF resources are managed by the PF (host) device driver. 附录（1）Virt-manager 设置虚机的 CPU 模式说明Libvirt 主要支持三种 CPU mode： host-passthrough: libvirt 令 KVM 把宿主机的 CPU 指令集全部透传给虚拟机。因此虚拟机能够最大限度的使用宿主机 CPU 指令集，故性能是最好的。但是在热迁移时，它要求目的节点的 CPU 和源节点的一致。 host-model: libvirt 根据当前宿主机 CPU 指令集从配置文件 /usr/share/libvirt/cpu_map.xml 选择一种最相配的 CPU 型号。在这种 mode 下，虚拟机的指令集往往比宿主机少，性能相对 host-passthrough 要差一点，但是热迁移时，它允许目的节点 CPU 和源节点的存在一定的差异。 custom: 这种模式下虚拟机 CPU 指令集数最少，故性能相对最差，但是它在热迁移时跨不同型号 CPU 的能力最强。此外，custom 模式下支持用户添加额外的指令集。 三种 mode 的性能排序是：host-passthrough &gt; host-model &gt; custom 实际性能差异不大：100%&gt; 95.84%&gt;94.73% 引自：http://wsfdl.com/openstack/2018/01/02/libvirt_cpu_mode.html （2）有关网卡模式的说明使用 virt-manager 创建虚拟机，在添加网卡时，有 3 中选择，分别是 e1000, rtl8139, virtio。 “rtl8139”这个网卡模式是 qemu-kvm 默认的模拟网卡类型，RTL8139 是 Realtek 半导体公司的一个 10/100M 网卡系列，是曾经非常流行（当然现在看来有点古老）且兼容性好的网卡，几乎所有的现代操作系统都对 RTL8139 网卡驱动的提供支持。 “e1000”系列提供 Intel e1000 系列的网卡模拟，纯的 QEMU（非 qemu-kvm）默认就是提供 Intel e1000 系列的虚拟网卡。 “virtio” 类型是 qemu-kvm 对半虚拟化 IO（virtio）驱动的支持。 这三个网卡的最大区别(此处指最需要关注的地方)是速度： rtl8139 10/100Mb/s e1000 1Gb/s virtio 10Gb/s 注意 virtio 是唯一可以达到 10Gb/s 的。 virtio 是一种 I/O 半虚拟化解决方案，是一套通用 I/O 设备虚拟化的程序，是对半虚拟化 Hypervisor 中的一组通用 I/O 设备的抽象。提供了一套上层应用与各 Hypervisor 虚拟化设备（KVM，Xen，VMware 等）之间的通信框架和编程接口，减少跨平台所带来的兼容性问题，大大提高驱动程序开发效率。 （3）有关 MACVTAP以下内容来自：https://www.ibm.com/developerworks/cn/linux/1312_xiawc_linuxvirtnet/index.html MACVTAP 的实现基于传统的 MACVLAN。和 TAP 设备一样，每一个 MACVTAP 设备拥有一个对应的 Linux 字符设备，并拥有和 TAP 设备一样的 IOCTL 接口，因此能直接被 KVM/Qemu 使用，方便地完成网络数据交换工作。引入 MACVTAP 设备的目标是：简化虚拟化环境中的交换网络，代替传统的 Linux TAP 设备加 Bridge 设备组合，同时支持新的虚拟化网络技术，如 802.1 Qbg。 MACVTAP 设备和 VLAN 设备类似，是以一对多的母子关系出现的。在一个母设备上可以创建多个 MACVTAP 子设备，一个 MACVTAP 设备只有一个母设备，MACVTAP 子设备可以做为母设备，再一次嵌套的创建 MACVTAP 子设备。母子设备之间被隐含的桥接起来，母设备相当于现实世界中的交换机 TRUNK 口。实际上当 MACVTAP 设备被创建并且模式不为 Passthrough 时，内核隐含的创建了 MACVLAN 网络，完成转发功能。MACVTAP 设备有四种工作模式：Bridge、VEPA、Private，Passthrough。 Bridge 模式下，它完成与 Bridge 设备类似功能，数据可以在属于同一个母设备的子设备间交换转发，虚拟机相当于简单接入了一个交换机。当前的 Linux 实现有一个缺陷，此模式下 MACVTAP 子设备无法和 Linux Host 通讯，即虚拟机无法和 Host 通讯。—-经验证，属实。 Passthrough 模式下，内核的 MACVLAN 数据处理逻辑被跳过，硬件决定数据如何处理，从而释放了 Host CPU 资源。 MACVTAP Passthrough 概念与 PCI Passthrough 概念不同，上图详细解释了两种情况的区别。 PCI Passthrough 针对的是任意 PCI 设备，不一定是网络设备，目的是让 Guest OS 直接使用 Host 上的 PCI 硬件以提高效率。以 X86 平台为例，数据将通过需要硬件支持的 VT-D 技术从 Guest OS 直接传递到 Host 硬件上。这样做固然效率很高，但因为模拟器失去了对虚拟硬件的控制，难以同步不同 Host 上的硬件状态，因此当前在使用 PCI Passthrough 的情况下难以做动态迁移。 MACVTAP Passthrough 仅仅针对 MACVTAP 网络设备，目的是绕过内核里 MACVTAP 的部分软件处理过程，转而交给硬件处理。在虚拟化条件下，数据还是会先到达模拟器 I/O 层，再转发到硬件上。这样做效率有损失，但模拟器仍然控制虚拟硬件的状态及数据的走向，可以做动态迁移。 （4）SR-IOV 介绍如果网卡支持 SRIOV，请使用 SRIOV PCI Passthrough。 软件模拟是通过 Hypervisor 层模拟虚拟网卡，实现与物理设备完全一样的接口，虚拟机操作系统无须修改就能直接驱动虚拟网卡，其最大的缺点是性能相对较差； 网卡直通支持虚拟机绕过 Hypervisor 层，直接访问物理 I/O 设备，具有最高的性能，但是在同一时刻物理 I/O 设备只能被一个虚拟机独享； SR-IOV 是 Intel 在 2007 年提出的解决虚拟化网络 I/O 的硬件技术方案，该技术不仅能够继承网卡直通的高性能优势，而且同时支持物理 I/O 设备的跨虚拟机共享，具有较好的应用前景。 原文链接：https://blog.csdn.net/lsz137105/article/details/100752930 SR-IOV（Single Root I/O Virtualization）是一个将 PCIe 设备（如网卡）共享给虚拟机的标准，通过为虚拟机提供独立的内存空间、中断、DMA 流，来绕过 VMM 实现数据访问。 SR-IOV 引入了两种 PCIe functions： PF（Physical Function）：包含完整的 PCIe 功能，包括 SR-IOV 的扩张能力，该功能用于 SR-IOV 的配置和管理。 VF（Virtual Function）：包含轻量级的 PCIe 功能。每一个 VF 有它自己独享的 PCI 配置区域，并且可能与其他 VF 共享着同一个物理资源。 SR-IOV 网卡通过将 SR-IOV 功能集成到物理网卡上，将单一的物理网卡虚拟成多个 VF 接口，每个 VF 接口都有单独的虚拟 PCIe 通道，这些虚拟的 PCIe 通道共用物理网卡的 PCIe 通道。每个虚拟机可占用一个或多个 VF 接口，这样虚拟机就可以直接访问自己的 VF 接口，而不需要 Hypervisor 的协调干预，从而大幅提升网络吞吐性能。 （5）探索虚拟机进程每一个客户机就是宿主机中的一个 QEMU 进程，而一个客户机的多个 vCPU 就是一个 QEMU 进程中的多个线程。 123[root@centos7 ~]# ps -ef | grep qemuqemu 52595 1 99 10:37 ? 01:24:12 /usr/libexec/qemu-kvm -name csr1kv-1 -S -machine pc-i440fx-rhel7.0.0,accel=kvm,usb=off,dump-guest-core=off,mem-merge=off -cpu host -m 8192 -mem-prealloc -mem-path /dev/hugepages/libvirt/qemu/7-csr1kv-1 -realtime mlock=on -smp 8,sockets=8,cores=1,threads=1 -uuid 59581018-6387-49df-ab09-2bcf40fc12ba -no-user-config -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/domain-7-csr1kv-1/monitor.sock,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc,driftfix=slew -global kvm-pit.lost_tick_policy=delay -no-hpet -no-shutdown -global PIIX4_PM.disable_s3=1 -global PIIX4_PM.disable_s4=1 -boot strict=on -device piix3-usb-uhci,id=usb,bus=pci.0,addr=0x1.0x2 -device virtio-serial-pci,id=virtio-serial0,bus=pci.0,addr=0x6 -drive file=/home/root/images/csr1000v-universalk9.17.02.01v.qcow2,format=qcow2,if=none,id=drive-virtio-disk0 -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x7,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=1 -netdev tap,fd=26,id=hostnet0,vhost=on,vhostfd=28 -device virtio-net-pci,netdev=hostnet0,id=net0,mac=52:54:00:36:95:f0,bus=pci.0,addr=0x3 -chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0 -chardev socket,id=charchannel0,path=/var/lib/libvirt/qemu/channel/target/domain-7-csr1kv-1/org.qemu.guest_agent.0,server,nowait -device virtserialport,bus=virtio-serial0.0,nr=1,chardev=charchannel0,id=channel0,name=org.qemu.guest_agent.0 -chardev spicevmc,id=charchannel1,name=vdagent -device virtserialport,bus=virtio-serial0.0,nr=2,chardev=charchannel1,id=channel1,name=com.redhat.spice.0 -spice port=5900,addr=127.0.0.1,disable-ticketing,image-compression=off,seamless-migration=on -vga qxl -global qxl-vga.ram_size=67108864 -global qxl-vga.vram_size=67108864 -global qxl-vga.vgamem_mb=16 -global qxl-vga.max_outputs=1 -device vfio-pci,host=1d:00.0,id=hostdev1,bus=pci.0,addr=0x8 -device vfio-pci,host=3b:10.1,id=hostdev0,bus=pci.0,addr=0x4 -msg timestamp=on 使用 virsh 命令或 virt-manager 开启虚拟机，是通过调用/usr/libexec/qemu-kvm 并附带虚拟配置的参数，来开启 qemu-kvm 的进程。可以看到上述的参数是非常复杂的，libvirt 提供 XML 参数进行简化。 ps -efL | grep qemu 可以列出所有的线程，但是输出篇幅很长，不在此列出；使用 pstree 可列出其父进程、线程关系，如下： virt-top 可查看虚机运行状态和资源利用率： [root@centos7 ~]# virt-top -1 参考资料连接 https://cloud.tencent.com/developer/article/1703094 https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html-single/performance_tuning_guide/index https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/7/html-single/virtualization_tuning_and_optimization_guide/index https://computingforgeeks.com/how-to-create-and-configure-bridge-networking-for-kvm-in-linux/ https://software.intel.com/content/www/us/en/develop/articles/configure-sr-iov-network-virtual-functions-in-linux-kvm.html","categories":[],"tags":[{"name":"CSR1000v","slug":"CSR1000v","permalink":"https://weiborao.github.io/tags/CSR1000v/"},{"name":"KVM","slug":"KVM","permalink":"https://weiborao.github.io/tags/KVM/"},{"name":"SRIOV","slug":"SRIOV","permalink":"https://weiborao.github.io/tags/SRIOV/"},{"name":"NetworkManager","slug":"NetworkManager","permalink":"https://weiborao.github.io/tags/NetworkManager/"},{"name":"Cisco","slug":"Cisco","permalink":"https://weiborao.github.io/tags/Cisco/"}]}],"categories":[],"tags":[{"name":"CSR1000v","slug":"CSR1000v","permalink":"https://weiborao.github.io/tags/CSR1000v/"},{"name":"KVM","slug":"KVM","permalink":"https://weiborao.github.io/tags/KVM/"},{"name":"SRIOV","slug":"SRIOV","permalink":"https://weiborao.github.io/tags/SRIOV/"},{"name":"NetworkManager","slug":"NetworkManager","permalink":"https://weiborao.github.io/tags/NetworkManager/"},{"name":"Cisco","slug":"Cisco","permalink":"https://weiborao.github.io/tags/Cisco/"}]}